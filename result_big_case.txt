 
(112, 3, 1, 32, 16, 8)
Params #:  784
MACs:  9834496
 
[15:21:00] (INFO) Building library...
[15:21:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7966441761363636, 'median': 1.75341796875, 'mins': 1.60888671875}
 
(112, 3, 1, 32, 16, 16)
Params #:  912
MACs:  11440128
 
[15:22:04] (INFO) Building library...
[15:22:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7916459517045455, 'median': 1.7530517578125, 'mins': 1.6241455078125}
 
(112, 3, 1, 32, 32, 8)
Params #:  1568
MACs:  19668992
 
[15:22:25] (INFO) Building library...
[15:22:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3830610795454545, 'median': 1.362060546875, 'mins': 1.24853515625}
 
(112, 3, 1, 32, 32, 16)
Params #:  1824
MACs:  22880256
 
[15:22:42] (INFO) Building library...
[15:22:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.04700927734375, 'median': 1.0328369140625, 'mins': 0.9412841796875}
 
(112, 3, 2, 16, 32, 12)
Params #:  1184
MACs:  8529920
 
[15:23:00] (INFO) Building library...
[15:23:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9621726296164772, 'median': 1.9453125, 'mins': 1.826904296875}
 
(112, 3, 2, 16, 32, 24)
Params #:  1568
MACs:  9734144
 
[15:23:19] (INFO) Building library...
[15:23:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0179953835227273, 'median': 2.0067138671875, 'mins': 1.8519287109375}
 
(112, 3, 2, 16, 48, 12)
Params #:  1776
MACs:  12794880
 
[15:23:37] (INFO) Building library...
[15:23:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8896018288352272, 'median': 1.8720703125, 'mins': 1.767578125}
 
(112, 3, 2, 16, 48, 24)
Params #:  2352
MACs:  14601216
 
[15:23:56] (INFO) Building library...
[15:23:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.194968483664773, 'median': 2.1033935546875, 'mins': 1.994140625}
 
(112, 3, 2, 16, 64, 12)
Params #:  2368
MACs:  17059840
 
[15:24:14] (INFO) Building library...
[15:24:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.494888583096591, 'median': 2.4483642578125, 'mins': 2.304931640625}
 
(112, 3, 2, 16, 64, 24)
Params #:  3136
MACs:  19468288
 
[15:24:33] (INFO) Building library...
[15:24:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5250954367897727, 'median': 2.469482421875, 'mins': 2.325439453125}
 
(112, 3, 2, 16, 80, 12)
Params #:  2960
MACs:  21324800
 
[15:24:52] (INFO) Building library...
[15:24:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5521506569602272, 'median': 2.5262451171875, 'mins': 2.4305419921875}
 
(112, 3, 2, 16, 80, 24)
Params #:  3920
MACs:  24335360
 
[15:25:11] (INFO) Building library...
[15:25:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6274369673295452, 'median': 2.5782470703125, 'mins': 2.4173583984375}
 
(112, 3, 2, 16, 96, 12)
Params #:  3552
MACs:  25589760
 
[15:25:30] (INFO) Building library...
[15:25:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.202455832741477, 'median': 2.151611328125, 'mins': 2.04443359375}
 
(112, 3, 2, 16, 96, 24)
Params #:  4704
MACs:  29202432
 
[15:25:49] (INFO) Building library...
[15:25:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7917658025568182, 'median': 1.77587890625, 'mins': 1.6680908203125}
 
(112, 3, 2, 16, 112, 12)
Params #:  4144
MACs:  29854720
 
[15:26:08] (INFO) Building library...
[15:26:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7289073597301137, 'median': 2.66748046875, 'mins': 2.576416015625}
 
(112, 3, 2, 16, 112, 24)
Params #:  5488
MACs:  34069504
 
[15:26:27] (INFO) Building library...
[15:26:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8509776722301137, 'median': 2.7843017578125, 'mins': 2.67236328125}
 
(112, 3, 2, 16, 128, 12)
Params #:  4736
MACs:  34119680
 
[15:26:46] (INFO) Building library...
[15:26:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8092029918323864, 'median': 2.7462158203125, 'mins': 2.6317138671875}
 
(112, 3, 2, 16, 128, 24)
Params #:  6272
MACs:  38936576
 
[15:27:06] (INFO) Building library...
[15:27:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.783709161931818, 'median': 2.720703125, 'mins': 2.6243896484375}
 
(56, 3, 1, 24, 16, 12)
Params #:  720
MACs:  2257920
 
[15:27:25] (INFO) Building library...
[15:27:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.749932306463068, 'median': 1.723876953125, 'mins': 1.619873046875}
 
(56, 3, 1, 24, 16, 24)
Params #:  912
MACs:  2860032
 
[15:27:42] (INFO) Building library...
[15:27:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8045188210227274, 'median': 2.77685546875, 'mins': 2.65576171875}
 
(56, 3, 1, 24, 32, 12)
Params #:  1440
MACs:  4515840
 
[15:28:02] (INFO) Building library...
[15:28:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7820623224431817, 'median': 1.7572021484375, 'mins': 1.6524658203125}
 
(56, 3, 1, 24, 32, 24)
Params #:  1824
MACs:  5720064
 
[15:28:20] (INFO) Building library...
[15:28:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.54371337890625, 'median': 2.5220947265625, 'mins': 2.401611328125}
 
(56, 3, 1, 24, 48, 12)
Params #:  2160
MACs:  6773760
 
[15:28:39] (INFO) Building library...
[15:28:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7971368963068182, 'median': 1.7955322265625, 'mins': 1.5142822265625}
 
(56, 3, 1, 24, 48, 24)
Params #:  2736
MACs:  8580096
 
[15:28:57] (INFO) Building library...
[15:28:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.82642822265625, 'median': 2.706298828125, 'mins': 2.5899658203125}
 
(56, 3, 1, 24, 64, 12)
Params #:  2880
MACs:  9031680
 
[15:29:17] (INFO) Building library...
[15:29:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.731759366122159, 'median': 1.7108154296875, 'mins': 1.59326171875}
 
(56, 3, 1, 24, 64, 24)
Params #:  3648
MACs:  11440128
 
[15:29:35] (INFO) Building library...
[15:29:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5790460759943183, 'median': 2.5477294921875, 'mins': 2.440185546875}
 
(56, 3, 1, 24, 72, 24)
Params #:  4104
MACs:  12870144
 
[15:29:54] (INFO) Building library...
[15:29:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7283180930397726, 'median': 2.6134033203125, 'mins': 2.4920654296875}
 
(56, 3, 1, 24, 80, 12)
Params #:  3600
MACs:  11289600
 
[15:30:14] (INFO) Building library...
[15:30:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1739002574573862, 'median': 2.0361328125, 'mins': 1.97509765625}
 
(56, 3, 1, 24, 80, 24)
Params #:  4560
MACs:  14300160
 
[15:30:32] (INFO) Building library...
[15:30:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.9351973100142046, 'median': 2.868896484375, 'mins': 2.7369384765625}
 
(56, 3, 1, 24, 96, 12)
Params #:  4320
MACs:  13547520
 
[15:30:52] (INFO) Building library...
[15:30:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1621326793323865, 'median': 2.037841796875, 'mins': 1.9110107421875}
 
(56, 3, 1, 24, 96, 24)
Params #:  5472
MACs:  17160192
 
[15:31:11] (INFO) Building library...
[15:31:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.9291215376420454, 'median': 2.8831787109375, 'mins': 2.7506103515625}
 
(56, 3, 1, 24, 112, 12)
Params #:  5040
MACs:  15805440
 
[15:31:33] (INFO) Building library...
[15:31:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8627141779119318, 'median': 1.7808837890625, 'mins': 1.6630859375}
 
(56, 3, 1, 24, 112, 24)
Params #:  6384
MACs:  20020224
 
[15:31:51] (INFO) Building library...
[15:31:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5496271306818183, 'median': 2.5267333984375, 'mins': 2.4061279296875}
 
(56, 3, 1, 24, 128, 12)
Params #:  5760
MACs:  18063360
 
[15:32:10] (INFO) Building library...
[15:32:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8440263227982954, 'median': 1.806884765625, 'mins': 1.695556640625}
 
(56, 3, 1, 24, 128, 24)
Params #:  7296
MACs:  22880256
 
[15:32:28] (INFO) Building library...
[15:32:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.521929376775568, 'median': 2.4915771484375, 'mins': 2.3936767578125}
 
(56, 3, 1, 24, 144, 12)
Params #:  6480
MACs:  20321280
 
[15:32:47] (INFO) Building library...
[15:32:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6996260209517045, 'median': 1.5826416015625, 'mins': 1.4820556640625}
 
(56, 3, 1, 24, 144, 24)
Params #:  8208
MACs:  25740288
 
[15:33:05] (INFO) Building library...
[15:33:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.210039728338068, 'median': 2.18603515625, 'mins': 2.1103515625}
 
(56, 3, 1, 24, 160, 12)
Params #:  7200
MACs:  22579200
 
[15:33:24] (INFO) Building library...
[15:33:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.907721502130682, 'median': 1.822998046875, 'mins': 1.718017578125}
 
(56, 3, 1, 24, 160, 24)
Params #:  9120
MACs:  28600320
 
[15:33:42] (INFO) Building library...
[15:33:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6815740411931817, 'median': 2.6480712890625, 'mins': 2.5689697265625}
 
(56, 3, 1, 24, 176, 12)
Params #:  7920
MACs:  24837120
 
[15:34:02] (INFO) Building library...
[15:34:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0943181818181817, 'median': 1.9825439453125, 'mins': 1.8714599609375}
 
(56, 3, 1, 24, 176, 24)
Params #:  10032
MACs:  31460352
 
[15:34:20] (INFO) Building library...
[15:34:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.9918656782670454, 'median': 2.9490966796875, 'mins': 2.8060302734375}
 
(56, 3, 1, 24, 192, 12)
Params #:  8640
MACs:  27095040
 
[15:34:40] (INFO) Building library...
[15:34:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9637961647727273, 'median': 1.846923828125, 'mins': 1.73681640625}
 
(56, 3, 1, 24, 192, 24)
Params #:  10944
MACs:  34320384
 
[15:34:58] (INFO) Building library...
[15:34:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.759682395241477, 'median': 2.740478515625, 'mins': 2.5848388671875}
 
(56, 3, 1, 24, 208, 12)
Params #:  9360
MACs:  29352960
 
[15:35:18] (INFO) Building library...
[15:35:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.196939364346591, 'median': 2.086181640625, 'mins': 1.97119140625}
 
(56, 3, 1, 24, 208, 24)
Params #:  11856
MACs:  37180416
 
[15:35:36] (INFO) Building library...
[15:35:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.959058726917614, 'median': 2.8868408203125, 'mins': 2.7288818359375}
 
(56, 3, 2, 24, 16, 16)
Params #:  784
MACs:  1517824
 
[15:35:56] (INFO) Building library...
[15:35:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5785899769176137, 'median': 1.5552978515625, 'mins': 1.4700927734375}
 
(56, 3, 2, 24, 16, 32)
Params #:  1040
MACs:  1718528
 
[15:36:14] (INFO) Building library...
[15:36:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.688790616122159, 'median': 1.66259765625, 'mins': 1.5599365234375}
 
(56, 3, 2, 24, 32, 16)
Params #:  1568
MACs:  3035648
 
[15:36:31] (INFO) Building library...
[15:36:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6837535511363637, 'median': 1.6551513671875, 'mins': 1.5343017578125}
 
(56, 3, 2, 24, 32, 32)
Params #:  2080
MACs:  3437056
 
[15:36:49] (INFO) Building library...
[15:36:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.830569735440341, 'median': 1.8092041015625, 'mins': 1.6968994140625}
 
(56, 3, 2, 24, 48, 16)
Params #:  2352
MACs:  4553472
 
[15:37:06] (INFO) Building library...
[15:37:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.739151278409091, 'median': 1.71923828125, 'mins': 1.613525390625}
 
(56, 3, 2, 24, 48, 32)
Params #:  3120
MACs:  5155584
 
[15:37:24] (INFO) Building library...
[15:37:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.942062100497159, 'median': 1.910888671875, 'mins': 1.7989501953125}
 
(56, 3, 2, 24, 64, 16)
Params #:  3136
MACs:  6071296
 
[15:37:41] (INFO) Building library...
[15:37:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0718439275568183, 'median': 1.988037109375, 'mins': 1.854736328125}
 
(56, 3, 2, 24, 64, 32)
Params #:  4160
MACs:  6874112
 
[15:37:58] (INFO) Building library...
[15:37:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6661155007102273, 'median': 1.6414794921875, 'mins': 1.5377197265625}
 
(56, 3, 2, 24, 80, 16)
Params #:  3920
MACs:  7589120
 
[15:38:15] (INFO) Building library...
[15:38:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.171484375, 'median': 2.1439208984375, 'mins': 2.0234375}
 
(56, 3, 2, 24, 80, 32)
Params #:  5200
MACs:  8592640
 
[15:38:33] (INFO) Building library...
[15:38:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9479924982244319, 'median': 1.9229736328125, 'mins': 1.7901611328125}
 
(56, 3, 2, 24, 96, 16)
Params #:  4704
MACs:  9106944
 
[15:38:51] (INFO) Building library...
[15:38:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.721053799715909, 'median': 1.7095947265625, 'mins': 1.6220703125}
 
(56, 3, 2, 24, 96, 32)
Params #:  6240
MACs:  10311168
 
[15:39:08] (INFO) Building library...
[15:39:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9591375177556818, 'median': 1.9368896484375, 'mins': 1.8133544921875}
 
(56, 3, 2, 24, 112, 16)
Params #:  5488
MACs:  10624768
 
[15:39:26] (INFO) Building library...
[15:39:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7674604936079545, 'median': 1.7288818359375, 'mins': 1.6026611328125}
 
(56, 3, 2, 24, 112, 32)
Params #:  7280
MACs:  12029696
 
[15:39:44] (INFO) Building library...
[15:39:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.82742919921875, 'median': 1.7373046875, 'mins': 1.6190185546875}
 
(56, 3, 2, 24, 128, 16)
Params #:  6272
MACs:  12142592
 
[15:40:02] (INFO) Building library...
[15:40:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7875921075994319, 'median': 1.76318359375, 'mins': 1.6416015625}
 
(56, 3, 2, 24, 128, 32)
Params #:  8320
MACs:  13748224
 
[15:40:19] (INFO) Building library...
[15:40:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8186257102272727, 'median': 1.7000732421875, 'mins': 1.6080322265625}
 
(56, 3, 2, 24, 144, 16)
Params #:  7056
MACs:  13660416
 
[15:40:36] (INFO) Building library...
[15:40:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6576038707386365, 'median': 1.6248779296875, 'mins': 1.4984130859375}
 
(56, 3, 2, 24, 144, 32)
Params #:  9360
MACs:  15466752
 
[15:40:54] (INFO) Building library...
[15:40:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3012007279829545, 'median': 1.2705078125, 'mins': 1.1943359375}
 
(56, 3, 2, 24, 160, 16)
Params #:  7840
MACs:  15178240
 
[15:41:11] (INFO) Building library...
[15:41:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9014393199573865, 'median': 1.805419921875, 'mins': 1.666259765625}
 
(56, 3, 2, 24, 160, 32)
Params #:  10400
MACs:  17185280
 
[15:41:29] (INFO) Building library...
[15:41:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7998890269886363, 'median': 1.785400390625, 'mins': 1.6624755859375}
 
(56, 3, 2, 24, 176, 16)
Params #:  8624
MACs:  16696064
 
[15:41:47] (INFO) Building library...
[15:41:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1662042791193183, 'median': 2.0489501953125, 'mins': 1.928955078125}
 
(56, 3, 2, 24, 176, 32)
Params #:  11440
MACs:  18903808
 
[15:42:05] (INFO) Building library...
[15:42:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1194091796875, 'median': 2.0037841796875, 'mins': 1.88232421875}
 
(56, 3, 2, 24, 192, 16)
Params #:  9408
MACs:  18213888
 
[15:42:23] (INFO) Building library...
[15:42:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8772416548295454, 'median': 1.7510986328125, 'mins': 1.63330078125}
 
(56, 3, 2, 24, 192, 32)
Params #:  12480
MACs:  20622336
 
[15:42:41] (INFO) Building library...
[15:42:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.55799560546875, 'median': 1.5306396484375, 'mins': 1.4276123046875}
 
(56, 3, 2, 24, 208, 16)
Params #:  10192
MACs:  19731712
 
[15:42:59] (INFO) Building library...
[15:42:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.071404474431818, 'median': 1.963134765625, 'mins': 1.819091796875}
 
(56, 3, 2, 24, 208, 32)
Params #:  13520
MACs:  22340864
 
[15:43:17] (INFO) Building library...
[15:43:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9329046075994318, 'median': 1.904052734375, 'mins': 1.784423828125}
 
(28, 3, 1, 32, 16, 16)
Params #:  912
MACs:  715008
 
[15:43:35] (INFO) Building library...
[15:43:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5677135120738637, 'median': 1.543701171875, 'mins': 1.4635009765625}
 
(28, 3, 1, 32, 16, 32)
Params #:  1168
MACs:  915712
 
[15:43:52] (INFO) Building library...
[15:43:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5281949129971593, 'median': 2.406982421875, 'mins': 2.309326171875}
 
(28, 3, 1, 32, 48, 16)
Params #:  2736
MACs:  2145024
 
[15:44:11] (INFO) Building library...
[15:44:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5816439541903409, 'median': 1.5556640625, 'mins': 1.4683837890625}
 
(28, 3, 1, 32, 48, 32)
Params #:  3504
MACs:  2747136
 
[15:44:28] (INFO) Building library...
[15:44:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.313184703480114, 'median': 2.2803955078125, 'mins': 2.1864013671875}
 
(28, 3, 1, 32, 64, 16)
Params #:  3648
MACs:  2860032
 
[15:44:47] (INFO) Building library...
[15:44:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5420976118607954, 'median': 1.5316162109375, 'mins': 1.4053955078125}
 
(28, 3, 1, 32, 64, 32)
Params #:  4672
MACs:  3662848
 
[15:45:05] (INFO) Building library...
[15:45:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.241800204190341, 'median': 2.20654296875, 'mins': 2.1173095703125}
 
(28, 3, 1, 32, 80, 16)
Params #:  4560
MACs:  3575040
 
[15:45:25] (INFO) Building library...
[15:45:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7262861772017046, 'median': 1.7088623046875, 'mins': 1.6046142578125}
 
(28, 3, 1, 32, 80, 32)
Params #:  5840
MACs:  4578560
 
[15:45:42] (INFO) Building library...
[15:45:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.455003773082386, 'median': 2.419189453125, 'mins': 2.3238525390625}
 
(28, 3, 1, 32, 96, 16)
Params #:  5472
MACs:  4290048
 
[15:46:01] (INFO) Building library...
[15:46:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3890724875710228, 'median': 1.3792724609375, 'mins': 1.29443359375}
 
(28, 3, 1, 32, 96, 32)
Params #:  7008
MACs:  5494272
 
[15:46:18] (INFO) Building library...
[15:46:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2856955788352273, 'median': 2.2586669921875, 'mins': 2.156005859375}
 
(28, 3, 1, 32, 112, 16)
Params #:  6384
MACs:  5005056
 
[15:46:37] (INFO) Building library...
[15:46:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7718339399857954, 'median': 1.7581787109375, 'mins': 1.687744140625}
 
(28, 3, 1, 32, 112, 32)
Params #:  8176
MACs:  6409984
 
[15:46:54] (INFO) Building library...
[15:46:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.588753995028409, 'median': 2.5491943359375, 'mins': 2.462158203125}
 
(28, 3, 1, 32, 128, 16)
Params #:  7296
MACs:  5720064
 
[15:47:14] (INFO) Building library...
[15:47:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7804964932528409, 'median': 1.7637939453125, 'mins': 1.65283203125}
 
(28, 3, 1, 32, 128, 32)
Params #:  9344
MACs:  7325696
 
[15:47:31] (INFO) Building library...
[15:47:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.764981356534091, 'median': 2.7119140625, 'mins': 2.60205078125}
 
(28, 3, 1, 32, 144, 16)
Params #:  8208
MACs:  6435072
 
[15:47:50] (INFO) Building library...
[15:47:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7833018909801137, 'median': 1.763916015625, 'mins': 1.673095703125}
 
(28, 3, 1, 32, 144, 32)
Params #:  10512
MACs:  8241408
 
[15:48:07] (INFO) Building library...
[15:48:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2230391068892046, 'median': 2.1981201171875, 'mins': 2.114013671875}
 
(28, 3, 1, 32, 160, 16)
Params #:  9120
MACs:  7150080
 
[15:48:26] (INFO) Building library...
[15:48:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7367786754261363, 'median': 1.7181396484375, 'mins': 1.618896484375}
 
(28, 3, 1, 32, 160, 32)
Params #:  11680
MACs:  9157120
 
[15:48:43] (INFO) Building library...
[15:48:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4991876775568183, 'median': 2.45849609375, 'mins': 2.3560791015625}
 
(28, 3, 1, 32, 176, 16)
Params #:  10032
MACs:  7865088
 
[15:49:02] (INFO) Building library...
[15:49:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.58427734375, 'median': 1.5712890625, 'mins': 1.484619140625}
 
(28, 3, 1, 32, 176, 32)
Params #:  12848
MACs:  10072832
 
[15:49:20] (INFO) Building library...
[15:49:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.667310680042614, 'median': 2.6787109375, 'mins': 2.378662109375}
 
(28, 3, 1, 32, 192, 16)
Params #:  10944
MACs:  8580096
 
[15:49:39] (INFO) Building library...
[15:49:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4203025124289772, 'median': 1.4129638671875, 'mins': 1.3594970703125}
 
(28, 3, 1, 32, 192, 32)
Params #:  14016
MACs:  10988544
 
[15:49:56] (INFO) Building library...
[15:49:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.91029052734375, 'median': 1.8851318359375, 'mins': 1.818359375}
 
(28, 3, 1, 32, 208, 16)
Params #:  11856
MACs:  9295104
 
[15:50:16] (INFO) Building library...
[15:50:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.947314453125, 'median': 1.8392333984375, 'mins': 1.7296142578125}
 
(28, 3, 1, 32, 208, 32)
Params #:  15184
MACs:  11904256
 
[15:50:34] (INFO) Building library...
[15:50:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4834328391335228, 'median': 2.4407958984375, 'mins': 2.3089599609375}
 
(28, 3, 1, 32, 224, 16)
Params #:  12768
MACs:  10010112
 
[15:50:53] (INFO) Building library...
[15:50:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8369395862926137, 'median': 1.7947998046875, 'mins': 1.68994140625}
 
(28, 3, 1, 32, 224, 32)
Params #:  16352
MACs:  12819968
 
[15:51:10] (INFO) Building library...
[15:51:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4883433948863636, 'median': 2.444091796875, 'mins': 2.3529052734375}
 
(28, 3, 1, 32, 240, 16)
Params #:  13680
MACs:  10725120
 
[15:51:30] (INFO) Building library...
[15:51:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8615822531960227, 'median': 1.842041015625, 'mins': 1.73486328125}
 
(28, 3, 1, 32, 240, 32)
Params #:  17520
MACs:  13735680
 
[15:51:47] (INFO) Building library...
[15:51:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.501706764914773, 'median': 2.4757080078125, 'mins': 2.3870849609375}
 
(28, 3, 1, 32, 256, 16)
Params #:  14592
MACs:  11440128
 
[15:52:07] (INFO) Building library...
[15:52:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7896673029119319, 'median': 1.7694091796875, 'mins': 1.6553955078125}
 
(28, 3, 1, 32, 256, 32)
Params #:  18688
MACs:  14651392
 
[15:52:24] (INFO) Building library...
[15:52:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5098066850142047, 'median': 2.466796875, 'mins': 2.3779296875}
 
(28, 3, 1, 32, 272, 16)
Params #:  15504
MACs:  12155136
 
[15:52:43] (INFO) Building library...
[15:52:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8517644708806817, 'median': 1.8060302734375, 'mins': 1.71337890625}
 
(28, 3, 1, 32, 272, 32)
Params #:  19856
MACs:  15567104
 
[15:53:00] (INFO) Building library...
[15:53:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5408613725142044, 'median': 2.4940185546875, 'mins': 2.3927001953125}
 
(28, 3, 2, 32, 16, 32)
Params #:  1168
MACs:  529984
 
[15:53:20] (INFO) Building library...
[15:53:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5135764382102272, 'median': 1.4813232421875, 'mins': 1.39990234375}
 
(28, 3, 2, 32, 16, 64)
Params #:  1680
MACs:  630336
 
[15:53:37] (INFO) Building library...
[15:53:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6828879616477272, 'median': 1.6729736328125, 'mins': 1.598876953125}
 
(28, 3, 2, 32, 48, 32)
Params #:  3504
MACs:  1589952
 
[15:53:55] (INFO) Building library...
[15:53:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.71510009765625, 'median': 1.6141357421875, 'mins': 1.5015869140625}
 
(28, 3, 2, 32, 48, 64)
Params #:  5040
MACs:  1891008
 
[15:54:12] (INFO) Building library...
[15:54:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7095814098011364, 'median': 1.68994140625, 'mins': 1.5936279296875}
 
(28, 3, 2, 32, 64, 32)
Params #:  4672
MACs:  2119936
 
[15:54:29] (INFO) Building library...
[15:54:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3703446821732954, 'median': 1.345947265625, 'mins': 1.2933349609375}
 
(28, 3, 2, 32, 64, 64)
Params #:  6720
MACs:  2521344
 
[15:54:46] (INFO) Building library...
[15:54:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.490909090909091, 'median': 1.482177734375, 'mins': 1.4246826171875}
 
(28, 3, 2, 32, 80, 32)
Params #:  5840
MACs:  2649920
 
[15:55:04] (INFO) Building library...
[15:55:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7639726118607955, 'median': 1.7362060546875, 'mins': 1.6370849609375}
 
(28, 3, 2, 32, 80, 64)
Params #:  8400
MACs:  3151680
 
[15:55:21] (INFO) Building library...
[15:55:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6739235617897728, 'median': 1.6474609375, 'mins': 1.566650390625}
 
(28, 3, 2, 32, 96, 32)
Params #:  7008
MACs:  3179904
 
[15:55:39] (INFO) Building library...
[15:55:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5399225408380681, 'median': 1.5240478515625, 'mins': 1.4326171875}
 
(28, 3, 2, 32, 96, 64)
Params #:  10080
MACs:  3782016
 
[15:55:56] (INFO) Building library...
[15:55:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5335293856534091, 'median': 1.52587890625, 'mins': 1.448974609375}
 
(28, 3, 2, 32, 112, 32)
Params #:  8176
MACs:  3709888
 
[15:56:13] (INFO) Building library...
[15:56:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.815060147372159, 'median': 1.6622314453125, 'mins': 1.527587890625}
 
(28, 3, 2, 32, 112, 64)
Params #:  11760
MACs:  4412352
 
[15:56:31] (INFO) Building library...
[15:56:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7347545276988636, 'median': 1.7164306640625, 'mins': 1.6241455078125}
 
(28, 3, 2, 32, 128, 32)
Params #:  9344
MACs:  4239872
 
[15:56:48] (INFO) Building library...
[15:56:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.571114834872159, 'median': 1.555908203125, 'mins': 1.4708251953125}
 
(28, 3, 2, 32, 128, 64)
Params #:  13440
MACs:  5042688
 
[15:57:06] (INFO) Building library...
[15:57:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.777960759943182, 'median': 1.7637939453125, 'mins': 1.68603515625}
 
(28, 3, 2, 32, 144, 32)
Params #:  10512
MACs:  4769856
 
[15:57:23] (INFO) Building library...
[15:57:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8880482066761364, 'median': 1.85498046875, 'mins': 1.6583251953125}
 
(28, 3, 2, 32, 144, 64)
Params #:  15120
MACs:  5673024
 
[15:57:41] (INFO) Building library...
[15:57:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8325295188210227, 'median': 1.8016357421875, 'mins': 1.698974609375}
 
(28, 3, 2, 32, 160, 32)
Params #:  11680
MACs:  5299840
 
[15:57:58] (INFO) Building library...
[15:57:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8044855291193183, 'median': 1.7645263671875, 'mins': 1.63427734375}
 
(28, 3, 2, 32, 160, 64)
Params #:  16800
MACs:  6303360
 
[15:58:16] (INFO) Building library...
[15:58:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.873123446377841, 'median': 2.7635498046875, 'mins': 2.540283203125}
 
(28, 3, 2, 32, 176, 32)
Params #:  12848
MACs:  5829824
 
[15:58:35] (INFO) Building library...
[15:58:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1220603249289773, 'median': 2.092041015625, 'mins': 1.8787841796875}
 
(28, 3, 2, 32, 176, 64)
Params #:  18480
MACs:  6933696
 
[15:58:54] (INFO) Building library...
[15:58:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9148115678267046, 'median': 1.83984375, 'mins': 1.6820068359375}
 
(28, 3, 2, 32, 192, 32)
Params #:  14016
MACs:  6359808
 
[15:59:12] (INFO) Building library...
[15:59:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.272713955965909, 'median': 1.2491455078125, 'mins': 1.1522216796875}
 
(28, 3, 2, 32, 192, 64)
Params #:  20160
MACs:  7564032
 
[15:59:30] (INFO) Building library...
[15:59:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.218435946377841, 'median': 1.1832275390625, 'mins': 1.1029052734375}
 
(28, 3, 2, 32, 208, 32)
Params #:  15184
MACs:  6889792
 
[15:59:48] (INFO) Building library...
[15:59:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9852283824573864, 'median': 1.8643798828125, 'mins': 1.6439208984375}
 
(28, 3, 2, 32, 208, 64)
Params #:  21840
MACs:  8194368
 
[16:00:06] (INFO) Building library...
[16:00:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.887337979403409, 'median': 1.8446044921875, 'mins': 1.71044921875}
 
(28, 3, 2, 32, 224, 32)
Params #:  16352
MACs:  7419776
 
[16:00:23] (INFO) Building library...
[16:00:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.763726251775568, 'median': 1.715087890625, 'mins': 1.6134033203125}
 
(28, 3, 2, 32, 224, 64)
Params #:  23520
MACs:  8824704
 
[16:00:41] (INFO) Building library...
[16:00:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.76619873046875, 'median': 1.7384033203125, 'mins': 1.6263427734375}
 
(28, 3, 2, 32, 240, 32)
Params #:  17520
MACs:  7949760
 
[16:00:59] (INFO) Building library...
[16:00:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5544322620738635, 'median': 1.5401611328125, 'mins': 1.456787109375}
 
(28, 3, 2, 32, 240, 64)
Params #:  25200
MACs:  9455040
 
[16:01:16] (INFO) Building library...
[16:01:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7979236949573865, 'median': 1.760498046875, 'mins': 1.6624755859375}
 
(28, 3, 2, 32, 256, 32)
Params #:  18688
MACs:  8479744
 
[16:01:33] (INFO) Building library...
[16:01:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.778125, 'median': 1.7515869140625, 'mins': 1.6429443359375}
 
(28, 3, 2, 32, 256, 64)
Params #:  26880
MACs:  10085376
 
[16:01:51] (INFO) Building library...
[16:01:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.578484552556818, 'median': 1.5526123046875, 'mins': 1.4610595703125}
 
(28, 3, 2, 32, 272, 32)
Params #:  19856
MACs:  9009728
 
[16:02:07] (INFO) Building library...
[16:02:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8142922141335227, 'median': 1.79345703125, 'mins': 1.6842041015625}
 
(28, 3, 2, 32, 272, 64)
Params #:  28560
MACs:  10715712
 
[16:02:25] (INFO) Building library...
[16:02:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8335060813210227, 'median': 1.7943115234375, 'mins': 1.6646728515625}
 
(14, 3, 1, 64, 16, 32)
Params #:  1680
MACs:  329280
 
[16:02:43] (INFO) Building library...
[16:02:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.67073974609375, 'median': 1.6407470703125, 'mins': 1.564208984375}
 
(14, 3, 1, 64, 16, 64)
Params #:  2192
MACs:  429632
 
[16:03:00] (INFO) Building library...
[16:03:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3223222212357952, 'median': 2.250244140625, 'mins': 2.12109375}
 
(14, 3, 1, 64, 32, 32)
Params #:  3360
MACs:  658560
 
[16:03:19] (INFO) Building library...
[16:03:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5552601207386363, 'median': 1.5443115234375, 'mins': 1.4615478515625}
 
(14, 3, 1, 64, 32, 64)
Params #:  4384
MACs:  859264
 
[16:03:36] (INFO) Building library...
[16:03:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5556673916903407, 'median': 2.4949951171875, 'mins': 2.389404296875}
 
(14, 3, 1, 64, 48, 32)
Params #:  5040
MACs:  987840
 
[16:03:56] (INFO) Building library...
[16:03:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4861772017045454, 'median': 1.46630859375, 'mins': 1.402099609375}
 
(14, 3, 1, 64, 48, 64)
Params #:  6576
MACs:  1288896
 
[16:04:13] (INFO) Building library...
[16:04:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2538185813210228, 'median': 2.239013671875, 'mins': 2.1580810546875}
 
(14, 3, 1, 64, 80, 32)
Params #:  8400
MACs:  1646400
 
[16:04:32] (INFO) Building library...
[16:04:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7165383078835228, 'median': 1.6719970703125, 'mins': 1.568115234375}
 
(14, 3, 1, 64, 80, 64)
Params #:  10960
MACs:  2148160
 
[16:04:50] (INFO) Building library...
[16:04:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3046309037642048, 'median': 2.2557373046875, 'mins': 2.1558837890625}
 
(14, 3, 1, 64, 96, 32)
Params #:  10080
MACs:  1975680
 
[16:05:09] (INFO) Building library...
[16:05:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5209461558948865, 'median': 1.50732421875, 'mins': 1.436279296875}
 
(14, 3, 1, 64, 96, 64)
Params #:  13152
MACs:  2577792
 
[16:05:26] (INFO) Building library...
[16:05:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.672683993252841, 'median': 2.5059814453125, 'mins': 2.3529052734375}
 
(14, 3, 1, 64, 112, 32)
Params #:  11760
MACs:  2304960
 
[16:05:45] (INFO) Building library...
[16:05:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.54727783203125, 'median': 1.51611328125, 'mins': 1.447998046875}
 
(14, 3, 1, 64, 112, 64)
Params #:  15344
MACs:  3007424
 
[16:06:02] (INFO) Building library...
[16:06:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2337590997869317, 'median': 2.212158203125, 'mins': 2.1265869140625}
 
(14, 3, 1, 64, 128, 32)
Params #:  13440
MACs:  2634240
 
[16:06:21] (INFO) Building library...
[16:06:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4049072265625, 'median': 1.370361328125, 'mins': 1.3048095703125}
 
(14, 3, 1, 64, 128, 64)
Params #:  17536
MACs:  3437056
 
[16:06:41] (INFO) Building library...
[16:06:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.037710848721591, 'median': 2.0133056640625, 'mins': 1.9346923828125}
 
(14, 3, 1, 64, 144, 32)
Params #:  15120
MACs:  2963520
 
[16:07:03] (INFO) Building library...
[16:07:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7916515003551137, 'median': 1.75390625, 'mins': 1.5933837890625}
 
(14, 3, 1, 64, 144, 64)
Params #:  19728
MACs:  3866688
 
[16:07:20] (INFO) Building library...
[16:07:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.640292080965909, 'median': 2.5977783203125, 'mins': 2.5067138671875}
 
(14, 3, 1, 64, 160, 32)
Params #:  16800
MACs:  3292800
 
[16:07:39] (INFO) Building library...
[16:07:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.790886896306818, 'median': 1.7568359375, 'mins': 1.646728515625}
 
(14, 3, 1, 64, 160, 64)
Params #:  21920
MACs:  4296320
 
[16:07:56] (INFO) Building library...
[16:07:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.554549893465909, 'median': 2.4974365234375, 'mins': 2.3525390625}
 
(14, 3, 1, 64, 176, 32)
Params #:  18480
MACs:  3622080
 
[16:08:15] (INFO) Building library...
[16:08:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5147760564630681, 'median': 1.48779296875, 'mins': 1.4090576171875}
 
(14, 3, 1, 64, 176, 64)
Params #:  24112
MACs:  4725952
 
[16:08:33] (INFO) Building library...
[16:08:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.624823552911932, 'median': 2.596435546875, 'mins': 2.422119140625}
 
(14, 3, 1, 64, 192, 32)
Params #:  20160
MACs:  3951360
 
[16:08:52] (INFO) Building library...
[16:08:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5375721324573863, 'median': 1.5186767578125, 'mins': 1.4300537109375}
 
(14, 3, 1, 64, 192, 64)
Params #:  26304
MACs:  5155584
 
[16:09:09] (INFO) Building library...
[16:09:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0800792347301136, 'median': 2.060302734375, 'mins': 1.9822998046875}
 
(14, 3, 1, 64, 208, 32)
Params #:  21840
MACs:  4280640
 
[16:09:28] (INFO) Building library...
[16:09:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.724173251065341, 'median': 1.7071533203125, 'mins': 1.5975341796875}
 
(14, 3, 1, 64, 208, 64)
Params #:  28496
MACs:  5585216
 
[16:09:46] (INFO) Building library...
[16:09:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.62196044921875, 'median': 2.5655517578125, 'mins': 2.4451904296875}
 
(14, 3, 1, 64, 224, 32)
Params #:  23520
MACs:  4609920
 
[16:10:05] (INFO) Building library...
[16:10:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5659046519886364, 'median': 1.5574951171875, 'mins': 1.4676513671875}
 
(14, 3, 1, 64, 224, 64)
Params #:  30688
MACs:  6014848
 
[16:10:22] (INFO) Building library...
[16:10:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.457864657315341, 'median': 2.42333984375, 'mins': 2.300048828125}
 
(14, 3, 1, 64, 240, 32)
Params #:  25200
MACs:  4939200
 
[16:10:42] (INFO) Building library...
[16:10:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.774298650568182, 'median': 1.75048828125, 'mins': 1.6458740234375}
 
(14, 3, 1, 64, 240, 64)
Params #:  32880
MACs:  6444480
 
[16:10:59] (INFO) Building library...
[16:10:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2013316761363635, 'median': 2.1915283203125, 'mins': 2.1146240234375}
 
(14, 3, 1, 64, 256, 32)
Params #:  26880
MACs:  5268480
 
[16:11:19] (INFO) Building library...
[16:11:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5926846590909092, 'median': 1.561767578125, 'mins': 1.4400634765625}
 
(14, 3, 1, 64, 256, 64)
Params #:  35072
MACs:  6874112
 
[16:11:36] (INFO) Building library...
[16:11:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7517045454545452, 'median': 2.6802978515625, 'mins': 2.428955078125}
 
(14, 3, 1, 64, 272, 32)
Params #:  28560
MACs:  5597760
 
[16:11:55] (INFO) Building library...
[16:11:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7794655539772728, 'median': 1.7490234375, 'mins': 1.66357421875}
 
(14, 3, 1, 64, 272, 64)
Params #:  37264
MACs:  7303744
 
[16:12:12] (INFO) Building library...
[16:12:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.38253173828125, 'median': 2.3756103515625, 'mins': 2.275634765625}
 
(14, 3, 1, 64, 288, 32)
Params #:  30240
MACs:  5927040
 
[16:12:32] (INFO) Building library...
[16:12:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7733498313210228, 'median': 1.749755859375, 'mins': 1.6510009765625}
 
(14, 3, 1, 64, 288, 64)
Params #:  39456
MACs:  7733376
 
[16:12:49] (INFO) Building library...
[16:12:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1913441051136364, 'median': 2.1817626953125, 'mins': 2.106201171875}
 
(14, 3, 1, 64, 304, 32)
Params #:  31920
MACs:  6256320
 
[16:13:08] (INFO) Building library...
[16:13:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7380038174715908, 'median': 1.7125244140625, 'mins': 1.6121826171875}
 
(14, 3, 1, 64, 304, 64)
Params #:  41648
MACs:  8163008
 
[16:13:26] (INFO) Building library...
[16:13:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4661376953125, 'median': 2.4200439453125, 'mins': 2.3115234375}
 
(14, 3, 1, 64, 320, 32)
Params #:  33600
MACs:  6585600
 
[16:13:45] (INFO) Building library...
[16:13:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7165416370738635, 'median': 1.6871337890625, 'mins': 1.6014404296875}
 
(14, 3, 1, 64, 320, 64)
Params #:  43840
MACs:  8592640
 
[16:14:03] (INFO) Building library...
[16:14:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8325605912642047, 'median': 2.7435302734375, 'mins': 2.603271484375}
 
(14, 3, 1, 64, 336, 32)
Params #:  35280
MACs:  6914880
 
[16:14:22] (INFO) Building library...
[16:14:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7619895241477272, 'median': 1.73681640625, 'mins': 1.6488037109375}
 
(14, 3, 1, 64, 336, 64)
Params #:  46032
MACs:  9022272
 
[16:14:39] (INFO) Building library...
[16:14:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.603531161221591, 'median': 2.5440673828125, 'mins': 2.4453125}
 
(14, 3, 1, 64, 352, 32)
Params #:  36960
MACs:  7244160
 
[16:14:59] (INFO) Building library...
[16:14:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.611459073153409, 'median': 1.582275390625, 'mins': 1.5081787109375}
 
(14, 3, 1, 64, 352, 64)
Params #:  48224
MACs:  9451904
 
[16:15:16] (INFO) Building library...
[16:15:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.260801003196023, 'median': 2.224609375, 'mins': 2.13525390625}
 
(14, 3, 1, 64, 368, 32)
Params #:  38640
MACs:  7573440
 
[16:15:36] (INFO) Building library...
[16:15:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7572265625, 'median': 1.7371826171875, 'mins': 1.6339111328125}
 
(14, 3, 1, 64, 368, 64)
Params #:  50416
MACs:  9881536
 
[16:15:53] (INFO) Building library...
[16:15:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4430952592329547, 'median': 2.4140625, 'mins': 2.3349609375}
 
(14, 3, 1, 64, 384, 32)
Params #:  40320
MACs:  7902720
 
[16:16:13] (INFO) Building library...
[16:16:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4033391779119317, 'median': 1.3892822265625, 'mins': 1.3319091796875}
 
(14, 3, 1, 64, 384, 64)
Params #:  52608
MACs:  10311168
 
[16:16:31] (INFO) Building library...
[16:16:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8111083984375, 'median': 1.7989501953125, 'mins': 1.7301025390625}
 
(14, 3, 1, 64, 400, 32)
Params #:  42000
MACs:  8232000
 
[16:16:51] (INFO) Building library...
[16:16:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7212291370738637, 'median': 1.7010498046875, 'mins': 1.6002197265625}
 
(14, 3, 1, 64, 400, 64)
Params #:  54800
MACs:  10740800
 
[16:17:08] (INFO) Building library...
[16:17:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2439619584517048, 'median': 2.21728515625, 'mins': 2.129638671875}
 
(14, 3, 1, 64, 416, 32)
Params #:  43680
MACs:  8561280
 
[16:17:28] (INFO) Building library...
[16:17:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5520430131392045, 'median': 1.5380859375, 'mins': 1.4664306640625}
 
(14, 3, 1, 64, 416, 64)
Params #:  56992
MACs:  11170432
 
[16:17:45] (INFO) Building library...
[16:17:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.524009011008523, 'median': 2.4859619140625, 'mins': 2.39453125}
 
(14, 3, 1, 64, 432, 32)
Params #:  45360
MACs:  8890560
 
[16:18:04] (INFO) Building library...
[16:18:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5808460582386363, 'median': 1.55615234375, 'mins': 1.47998046875}
 
(14, 3, 1, 64, 432, 64)
Params #:  59184
MACs:  11600064
 
[16:18:21] (INFO) Building library...
[16:18:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4140880237926137, 'median': 2.40087890625, 'mins': 2.316162109375}
 
(14, 3, 1, 64, 448, 32)
Params #:  47040
MACs:  9219840
 
[16:18:41] (INFO) Building library...
[16:18:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.905524236505682, 'median': 1.86328125, 'mins': 1.662109375}
 
(14, 3, 1, 64, 448, 64)
Params #:  61376
MACs:  12029696
 
[16:18:58] (INFO) Building library...
[16:18:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4487071644176135, 'median': 2.42431640625, 'mins': 2.30859375}
 
(14, 3, 1, 64, 464, 32)
Params #:  48720
MACs:  9549120
 
[16:19:17] (INFO) Building library...
[16:19:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7611272638494317, 'median': 1.7371826171875, 'mins': 1.646728515625}
 
(14, 3, 1, 64, 464, 64)
Params #:  63568
MACs:  12459328
 
[16:19:35] (INFO) Building library...
[16:19:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4430963689630683, 'median': 2.412353515625, 'mins': 2.306884765625}
 
(14, 3, 1, 64, 480, 32)
Params #:  50400
MACs:  9878400
 
[16:19:54] (INFO) Building library...
[16:19:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6309026544744318, 'median': 1.61083984375, 'mins': 1.5218505859375}
 
(14, 3, 1, 64, 480, 64)
Params #:  65760
MACs:  12888960
 
[16:20:11] (INFO) Building library...
[16:20:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.530838290127841, 'median': 2.4552001953125, 'mins': 2.3687744140625}
 
(14, 3, 1, 64, 496, 32)
Params #:  52080
MACs:  10207680
 
[16:20:30] (INFO) Building library...
[16:20:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7469105113636363, 'median': 1.7357177734375, 'mins': 1.6641845703125}
 
(14, 3, 1, 64, 496, 64)
Params #:  67952
MACs:  13318592
 
[16:20:48] (INFO) Building library...
[16:20:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.337313565340909, 'median': 2.3035888671875, 'mins': 2.208984375}
 
(14, 3, 1, 64, 512, 32)
Params #:  53760
MACs:  10536960
 
[16:21:07] (INFO) Building library...
[16:21:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8814297762784091, 'median': 1.8421630859375, 'mins': 1.7515869140625}
 
(14, 3, 1, 64, 512, 64)
Params #:  70144
MACs:  13748224
 
[16:21:24] (INFO) Building library...
[16:21:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6438643022017048, 'median': 2.6083984375, 'mins': 2.50146484375}
 
(14, 3, 1, 64, 528, 32)
Params #:  55440
MACs:  10866240
 
[16:21:43] (INFO) Building library...
[16:21:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6044367009943181, 'median': 1.600830078125, 'mins': 1.5230712890625}
 
(14, 3, 1, 64, 528, 64)
Params #:  72336
MACs:  14177856
 
[16:22:01] (INFO) Building library...
[16:22:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.263694069602273, 'median': 2.2586669921875, 'mins': 2.1793212890625}
 
(14, 3, 1, 64, 544, 32)
Params #:  57120
MACs:  11195520
 
[16:22:20] (INFO) Building library...
[16:22:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.821280184659091, 'median': 1.7974853515625, 'mins': 1.7056884765625}
 
(14, 3, 1, 64, 544, 64)
Params #:  74528
MACs:  14607488
 
[16:22:37] (INFO) Building library...
[16:22:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2789928089488636, 'median': 2.2572021484375, 'mins': 2.1568603515625}
 
(14, 3, 1, 64, 560, 32)
Params #:  58800
MACs:  11524800
 
[16:22:57] (INFO) Building library...
[16:22:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5544544566761365, 'median': 1.547119140625, 'mins': 1.4779052734375}
 
(14, 3, 1, 64, 560, 64)
Params #:  76720
MACs:  15037120
 
[16:23:14] (INFO) Building library...
[16:23:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.506040261008523, 'median': 2.46826171875, 'mins': 2.368896484375}
 
(14, 3, 1, 64, 16, 48)
Params #:  1936
MACs:  379456
 
[16:23:33] (INFO) Building library...
[16:23:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6277798739346592, 'median': 1.4915771484375, 'mins': 1.39404296875}
 
(14, 3, 1, 64, 16, 96)
Params #:  2704
MACs:  529984
 
[16:23:50] (INFO) Building library...
[16:23:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4399513938210227, 'median': 1.42626953125, 'mins': 1.26904296875}
 
(14, 3, 1, 64, 32, 48)
Params #:  3872
MACs:  758912
 
[16:24:08] (INFO) Building library...
[16:24:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5574818004261364, 'median': 1.5382080078125, 'mins': 1.4593505859375}
 
(14, 3, 1, 64, 32, 96)
Params #:  5408
MACs:  1059968
 
[16:24:25] (INFO) Building library...
[16:24:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5012384588068182, 'median': 1.4840087890625, 'mins': 1.4180908203125}
 
(14, 3, 1, 64, 48, 48)
Params #:  5808
MACs:  1138368
 
[16:24:42] (INFO) Building library...
[16:24:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6281871448863636, 'median': 1.60986328125, 'mins': 1.5252685546875}
 
(14, 3, 1, 64, 48, 96)
Params #:  8112
MACs:  1589952
 
[16:24:59] (INFO) Building library...
[16:24:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6783192027698863, 'median': 1.636474609375, 'mins': 1.4674072265625}
 
(14, 3, 1, 64, 80, 48)
Params #:  9680
MACs:  1897280
 
[16:25:16] (INFO) Building library...
[16:25:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6965098987926137, 'median': 1.6793212890625, 'mins': 1.603271484375}
 
(14, 3, 1, 64, 80, 96)
Params #:  13520
MACs:  2649920
 
[16:25:33] (INFO) Building library...
[16:25:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.509453790838068, 'median': 1.4954833984375, 'mins': 1.406494140625}
 
(14, 3, 1, 64, 96, 48)
Params #:  11616
MACs:  2276736
 
[16:25:51] (INFO) Building library...
[16:25:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6905839399857954, 'median': 1.6717529296875, 'mins': 1.57861328125}
 
(14, 3, 1, 64, 96, 96)
Params #:  16224
MACs:  3179904
 
[16:26:07] (INFO) Building library...
[16:26:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4854414506392046, 'median': 1.4764404296875, 'mins': 1.42041015625}
 
(14, 3, 1, 64, 112, 48)
Params #:  13552
MACs:  2656192
 
[16:26:24] (INFO) Building library...
[16:26:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5483209783380683, 'median': 1.53759765625, 'mins': 1.46044921875}
 
(14, 3, 1, 64, 112, 96)
Params #:  18928
MACs:  3709888
 
[16:26:41] (INFO) Building library...
[16:26:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6776167436079545, 'median': 1.6519775390625, 'mins': 1.566650390625}
 
(14, 3, 1, 64, 128, 48)
Params #:  15488
MACs:  3035648
 
[16:26:58] (INFO) Building library...
[16:26:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.352359286221591, 'median': 1.330810546875, 'mins': 1.268798828125}
 
(14, 3, 1, 64, 128, 96)
Params #:  21632
MACs:  4239872
 
[16:27:17] (INFO) Building library...
[16:27:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3725297407670454, 'median': 1.3544921875, 'mins': 1.2896728515625}
 
(14, 3, 1, 64, 144, 48)
Params #:  17424
MACs:  3415104
 
[16:27:37] (INFO) Building library...
[16:27:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3599198774857955, 'median': 1.3475341796875, 'mins': 1.2796630859375}
 
(14, 3, 1, 64, 144, 96)
Params #:  24336
MACs:  4769856
 
[16:27:54] (INFO) Building library...
[16:27:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6576182972301137, 'median': 1.64208984375, 'mins': 1.5498046875}
 
(14, 3, 1, 64, 160, 48)
Params #:  19360
MACs:  3794560
 
[16:28:11] (INFO) Building library...
[16:28:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5785056374289772, 'median': 1.5504150390625, 'mins': 1.4720458984375}
 
(14, 3, 1, 64, 160, 96)
Params #:  27040
MACs:  5299840
 
[16:28:28] (INFO) Building library...
[16:28:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7447587446732955, 'median': 1.7181396484375, 'mins': 1.617431640625}
 
(14, 3, 1, 64, 176, 48)
Params #:  21296
MACs:  4174016
 
[16:28:46] (INFO) Building library...
[16:28:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7286865234375, 'median': 1.7117919921875, 'mins': 1.613037109375}
 
(14, 3, 1, 64, 176, 96)
Params #:  29744
MACs:  5829824
 
[16:29:03] (INFO) Building library...
[16:29:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7339854847301137, 'median': 1.7158203125, 'mins': 1.620849609375}
 
(14, 3, 1, 64, 192, 48)
Params #:  23232
MACs:  4553472
 
[16:29:20] (INFO) Building library...
[16:29:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5198785955255683, 'median': 1.5003662109375, 'mins': 1.4176025390625}
 
(14, 3, 1, 64, 192, 96)
Params #:  32448
MACs:  6359808
 
[16:29:36] (INFO) Building library...
[16:29:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6879838423295455, 'median': 1.66162109375, 'mins': 1.56982421875}
 
(14, 3, 1, 64, 208, 48)
Params #:  25168
MACs:  4932928
 
[16:29:53] (INFO) Building library...
[16:29:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.855721768465909, 'median': 1.711181640625, 'mins': 1.609130859375}
 
(14, 3, 1, 64, 208, 96)
Params #:  35152
MACs:  6889792
 
[16:30:11] (INFO) Building library...
[16:30:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7729225852272728, 'median': 1.75, 'mins': 1.66650390625}
 
(14, 3, 1, 64, 224, 48)
Params #:  27104
MACs:  5312384
 
[16:30:28] (INFO) Building library...
[16:30:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7518077503551137, 'median': 1.727294921875, 'mins': 1.641357421875}
 
(14, 3, 1, 64, 224, 96)
Params #:  37856
MACs:  7419776
 
[16:30:46] (INFO) Building library...
[16:30:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7541792436079546, 'median': 1.7327880859375, 'mins': 1.623779296875}
 
(14, 3, 1, 64, 240, 48)
Params #:  29040
MACs:  5691840
 
[16:31:03] (INFO) Building library...
[16:31:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.715774813565341, 'median': 1.685546875, 'mins': 1.609619140625}
 
(14, 3, 1, 64, 240, 96)
Params #:  40560
MACs:  7949760
 
[16:31:20] (INFO) Building library...
[16:31:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6730035955255682, 'median': 1.669921875, 'mins': 1.583740234375}
 
(14, 3, 1, 64, 256, 48)
Params #:  30976
MACs:  6071296
 
[16:31:37] (INFO) Building library...
[16:31:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6455388849431818, 'median': 1.6302490234375, 'mins': 1.545166015625}
 
(14, 3, 1, 64, 256, 96)
Params #:  43264
MACs:  8479744
 
[16:31:54] (INFO) Building library...
[16:31:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9426058682528409, 'median': 1.9161376953125, 'mins': 1.7996826171875}
 
(14, 3, 1, 64, 272, 48)
Params #:  32912
MACs:  6450752
 
[16:32:12] (INFO) Building library...
[16:32:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5686345880681818, 'median': 1.5482177734375, 'mins': 1.471435546875}
 
(14, 3, 1, 64, 272, 96)
Params #:  45968
MACs:  9009728
 
[16:32:29] (INFO) Building library...
[16:32:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7364834872159092, 'median': 1.693115234375, 'mins': 1.61572265625}
 
(14, 3, 1, 64, 288, 48)
Params #:  34848
MACs:  6830208
 
[16:32:46] (INFO) Building library...
[16:32:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4171275745738636, 'median': 1.3714599609375, 'mins': 1.302490234375}
 
(14, 3, 1, 64, 288, 96)
Params #:  48672
MACs:  9539712
 
[16:33:04] (INFO) Building library...
[16:33:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5503118341619317, 'median': 1.528076171875, 'mins': 1.4576416015625}
 
(14, 3, 1, 64, 304, 48)
Params #:  36784
MACs:  7209664
 
[16:33:22] (INFO) Building library...
[16:33:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5004782936789773, 'median': 1.4793701171875, 'mins': 1.4041748046875}
 
(14, 3, 1, 64, 304, 96)
Params #:  51376
MACs:  10069696
 
[16:33:39] (INFO) Building library...
[16:33:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5911732066761364, 'median': 1.5762939453125, 'mins': 1.488037109375}
 
(14, 3, 1, 64, 320, 48)
Params #:  38720
MACs:  7589120
 
[16:33:56] (INFO) Building library...
[16:33:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5891368519176137, 'median': 1.567138671875, 'mins': 1.4710693359375}
 
(14, 3, 1, 64, 320, 96)
Params #:  54080
MACs:  10599680
 
[16:34:13] (INFO) Building library...
[16:34:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.778335848721591, 'median': 1.7513427734375, 'mins': 1.6671142578125}
 
(14, 3, 1, 64, 336, 48)
Params #:  40656
MACs:  7968576
 
[16:34:30] (INFO) Building library...
[16:34:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7919899680397726, 'median': 1.767822265625, 'mins': 1.68408203125}
 
(14, 3, 1, 64, 336, 96)
Params #:  56784
MACs:  11129664
 
[16:34:47] (INFO) Building library...
[16:34:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7584039861505683, 'median': 1.72509765625, 'mins': 1.6524658203125}
 
(14, 3, 1, 64, 352, 48)
Params #:  42592
MACs:  8348032
 
[16:35:05] (INFO) Building library...
[16:35:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5170465642755682, 'median': 1.5050048828125, 'mins': 1.4267578125}
 
(14, 3, 1, 64, 352, 96)
Params #:  59488
MACs:  11659648
 
[16:35:22] (INFO) Building library...
[16:35:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5791193181818182, 'median': 1.5582275390625, 'mins': 1.4742431640625}
 
(14, 3, 1, 64, 368, 48)
Params #:  44528
MACs:  8727488
 
[16:35:39] (INFO) Building library...
[16:35:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6785511363636363, 'median': 1.658935546875, 'mins': 1.5809326171875}
 
(14, 3, 1, 64, 368, 96)
Params #:  62192
MACs:  12189632
 
[16:35:56] (INFO) Building library...
[16:35:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6522194602272726, 'median': 1.6392822265625, 'mins': 1.5604248046875}
 
(14, 3, 1, 64, 384, 48)
Params #:  46464
MACs:  9106944
 
[16:36:14] (INFO) Building library...
[16:36:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4217584783380681, 'median': 1.4044189453125, 'mins': 1.328369140625}
 
(14, 3, 1, 64, 384, 96)
Params #:  64896
MACs:  12719616
 
[16:36:31] (INFO) Building library...
[16:36:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.1504438920454545, 'median': 1.1435546875, 'mins': 1.092041015625}
 
(14, 3, 1, 64, 400, 48)
Params #:  48400
MACs:  9486400
 
[16:36:48] (INFO) Building library...
[16:36:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7735973011363637, 'median': 1.74560546875, 'mins': 1.6336669921875}
 
(14, 3, 1, 64, 400, 96)
Params #:  67600
MACs:  13249600
 
[16:37:06] (INFO) Building library...
[16:37:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7144586736505683, 'median': 1.694091796875, 'mins': 1.61181640625}
 
(14, 3, 1, 64, 416, 48)
Params #:  50336
MACs:  9865856
 
[16:37:23] (INFO) Building library...
[16:37:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.748681640625, 'median': 1.7108154296875, 'mins': 1.6346435546875}
 
(14, 3, 1, 64, 416, 96)
Params #:  70304
MACs:  13779584
 
[16:37:40] (INFO) Building library...
[16:37:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7222678444602273, 'median': 1.7139892578125, 'mins': 1.6373291015625}
 
(14, 3, 1, 64, 432, 48)
Params #:  52272
MACs:  10245312
 
[16:37:57] (INFO) Building library...
[16:37:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5234685724431818, 'median': 1.5052490234375, 'mins': 1.42578125}
 
(14, 3, 1, 64, 432, 96)
Params #:  73008
MACs:  14309568
 
[16:38:15] (INFO) Building library...
[16:38:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.549789151278409, 'median': 1.5208740234375, 'mins': 1.4468994140625}
 
(14, 3, 1, 64, 448, 48)
Params #:  54208
MACs:  10624768
 
[16:38:32] (INFO) Building library...
[16:38:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7412897283380682, 'median': 1.7239990234375, 'mins': 1.634521484375}
 
(14, 3, 1, 64, 448, 96)
Params #:  75712
MACs:  14839552
 
[16:38:49] (INFO) Building library...
[16:38:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8172629616477274, 'median': 1.809814453125, 'mins': 1.7200927734375}
 
(14, 3, 1, 64, 464, 48)
Params #:  56144
MACs:  11004224
 
[16:39:07] (INFO) Building library...
[16:39:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7735806551846591, 'median': 1.7537841796875, 'mins': 1.6461181640625}
 
(14, 3, 1, 64, 464, 96)
Params #:  78416
MACs:  15369536
 
[16:39:24] (INFO) Building library...
[16:39:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7657137784090908, 'median': 1.733642578125, 'mins': 1.6383056640625}
 
(14, 3, 1, 64, 480, 48)
Params #:  58080
MACs:  11383680
 
[16:39:41] (INFO) Building library...
[16:39:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.792532626065341, 'median': 1.7850341796875, 'mins': 1.718017578125}
 
(14, 3, 1, 64, 480, 96)
Params #:  81120
MACs:  15899520
 
[16:39:58] (INFO) Building library...
[16:39:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5628861860795455, 'median': 1.553466796875, 'mins': 1.480224609375}
 
(14, 3, 1, 64, 496, 48)
Params #:  60016
MACs:  11763136
 
[16:40:16] (INFO) Building library...
[16:40:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6888194691051137, 'median': 1.681640625, 'mins': 1.5693359375}
 
(14, 3, 1, 64, 496, 96)
Params #:  83824
MACs:  16429504
 
[16:40:33] (INFO) Building library...
[16:40:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7861538973721591, 'median': 1.756591796875, 'mins': 1.6590576171875}
 
(14, 3, 1, 64, 512, 48)
Params #:  61952
MACs:  12142592
 
[16:40:50] (INFO) Building library...
[16:40:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7763461026278409, 'median': 1.764404296875, 'mins': 1.6593017578125}
 
(14, 3, 1, 64, 512, 96)
Params #:  86528
MACs:  16959488
 
[16:41:07] (INFO) Building library...
[16:41:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9541448419744318, 'median': 1.917236328125, 'mins': 1.7987060546875}
 
(14, 3, 1, 64, 528, 48)
Params #:  63888
MACs:  12522048
 
[16:41:24] (INFO) Building library...
[16:41:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8002197265625, 'median': 1.6397705078125, 'mins': 1.5379638671875}
 
(14, 3, 1, 64, 528, 96)
Params #:  89232
MACs:  17489472
 
[16:41:41] (INFO) Building library...
[16:41:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.56790771484375, 'median': 1.54931640625, 'mins': 1.4847412109375}
 
(14, 3, 1, 64, 544, 48)
Params #:  65824
MACs:  12901504
 
[16:41:59] (INFO) Building library...
[16:41:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7816450639204546, 'median': 1.7601318359375, 'mins': 1.6551513671875}
 
(14, 3, 1, 64, 544, 96)
Params #:  91936
MACs:  18019456
 
[16:42:16] (INFO) Building library...
[16:42:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8844271573153408, 'median': 1.8634033203125, 'mins': 1.753173828125}
 
(14, 3, 1, 64, 560, 48)
Params #:  67760
MACs:  13280960
 
[16:42:33] (INFO) Building library...
[16:42:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7390869140625, 'median': 1.7266845703125, 'mins': 1.64111328125}
 
(14, 3, 1, 64, 560, 96)
Params #:  94640
MACs:  18549440
 
[16:42:50] (INFO) Building library...
[16:42:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8838700727982955, 'median': 1.8673095703125, 'mins': 1.7705078125}
 
(14, 3, 1, 96, 16, 48)
Params #:  2448
MACs:  479808
 
[16:43:08] (INFO) Building library...
[16:43:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3912464488636365, 'median': 1.375732421875, 'mins': 1.3060302734375}
 
(14, 3, 1, 96, 16, 96)
Params #:  3216
MACs:  630336
 
[16:43:25] (INFO) Building library...
[16:43:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.822372159090909, 'median': 1.8162841796875, 'mins': 1.7322998046875}
 
(14, 3, 1, 96, 32, 48)
Params #:  4896
MACs:  959616
 
[16:43:45] (INFO) Building library...
[16:43:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4151023171164774, 'median': 1.3870849609375, 'mins': 1.3173828125}
 
(14, 3, 1, 96, 32, 96)
Params #:  6432
MACs:  1260672
 
[16:44:02] (INFO) Building library...
[16:44:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3237571022727272, 'median': 2.2802734375, 'mins': 2.190185546875}
 
(14, 3, 1, 96, 48, 48)
Params #:  7344
MACs:  1439424
 
[16:44:21] (INFO) Building library...
[16:44:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5361805308948864, 'median': 1.518798828125, 'mins': 1.4332275390625}
 
(14, 3, 1, 96, 48, 96)
Params #:  9648
MACs:  1891008
 
[16:44:38] (INFO) Building library...
[16:44:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.277200594815341, 'median': 2.2569580078125, 'mins': 2.1407470703125}
 
(14, 3, 1, 96, 64, 48)
Params #:  9792
MACs:  1919232
 
[16:44:57] (INFO) Building library...
[16:44:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5406505237926136, 'median': 1.50927734375, 'mins': 1.42822265625}
 
(14, 3, 1, 96, 64, 96)
Params #:  12864
MACs:  2521344
 
[16:45:14] (INFO) Building library...
[16:45:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3365434126420452, 'median': 2.308837890625, 'mins': 2.2222900390625}
 
(14, 3, 1, 96, 80, 48)
Params #:  12240
MACs:  2399040
 
[16:45:33] (INFO) Building library...
[16:45:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.531795987215909, 'median': 1.518798828125, 'mins': 1.4501953125}
 
(14, 3, 1, 96, 80, 96)
Params #:  16080
MACs:  3151680
 
[16:45:50] (INFO) Building library...
[16:45:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2455366654829545, 'median': 2.2220458984375, 'mins': 2.1243896484375}
 
(14, 3, 1, 96, 112, 48)
Params #:  17136
MACs:  3358656
 
[16:46:09] (INFO) Building library...
[16:46:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7235384854403408, 'median': 1.7109375, 'mins': 1.63525390625}
 
(14, 3, 1, 96, 112, 96)
Params #:  22512
MACs:  4412352
 
[16:46:27] (INFO) Building library...
[16:46:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2501797762784093, 'median': 2.236328125, 'mins': 2.1688232421875}
 
(14, 3, 1, 96, 128, 48)
Params #:  19584
MACs:  3838464
 
[16:46:46] (INFO) Building library...
[16:46:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7024325284090909, 'median': 1.68603515625, 'mins': 1.6031494140625}
 
(14, 3, 1, 96, 128, 96)
Params #:  25728
MACs:  5042688
 
[16:47:03] (INFO) Building library...
[16:47:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7053300337357955, 'median': 2.6966552734375, 'mins': 2.610595703125}
 
(14, 3, 1, 96, 144, 48)
Params #:  22032
MACs:  4318272
 
[16:47:22] (INFO) Building library...
[16:47:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5465953480113637, 'median': 1.5318603515625, 'mins': 1.454345703125}
 
(14, 3, 1, 96, 144, 96)
Params #:  28944
MACs:  5673024
 
[16:47:40] (INFO) Building library...
[16:47:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4287486683238635, 'median': 2.394287109375, 'mins': 2.3074951171875}
 
(14, 3, 1, 96, 160, 48)
Params #:  24480
MACs:  4798080
 
[16:47:59] (INFO) Building library...
[16:47:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5507612748579545, 'median': 1.5379638671875, 'mins': 1.471435546875}
 
(14, 3, 1, 96, 160, 96)
Params #:  32160
MACs:  6303360
 
[16:48:16] (INFO) Building library...
[16:48:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2762828480113635, 'median': 2.264892578125, 'mins': 2.1741943359375}
 
(14, 3, 1, 96, 176, 48)
Params #:  26928
MACs:  5277888
 
[16:48:35] (INFO) Building library...
[16:48:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5654152610085228, 'median': 1.5465087890625, 'mins': 1.4693603515625}
 
(14, 3, 1, 96, 176, 96)
Params #:  35376
MACs:  6933696
 
[16:48:52] (INFO) Building library...
[16:48:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5240678267045453, 'median': 2.4718017578125, 'mins': 2.371337890625}
 
(14, 3, 1, 96, 192, 48)
Params #:  29376
MACs:  5757696
 
[16:49:11] (INFO) Building library...
[16:49:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3497525301846591, 'median': 1.33544921875, 'mins': 1.262939453125}
 
(14, 3, 1, 96, 192, 96)
Params #:  38592
MACs:  7564032
 
[16:49:28] (INFO) Building library...
[16:49:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5594582297585227, 'median': 2.517822265625, 'mins': 2.4306640625}
 
(14, 3, 1, 96, 208, 48)
Params #:  31824
MACs:  6237504
 
[16:49:47] (INFO) Building library...
[16:49:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8563920454545455, 'median': 1.73095703125, 'mins': 1.606201171875}
 
(14, 3, 1, 96, 208, 96)
Params #:  41808
MACs:  8194368
 
[16:50:05] (INFO) Building library...
[16:50:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.250099875710227, 'median': 2.2271728515625, 'mins': 2.123291015625}
 
(14, 3, 1, 96, 224, 48)
Params #:  34272
MACs:  6717312
 
[16:50:24] (INFO) Building library...
[16:50:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5804343483664773, 'median': 1.5631103515625, 'mins': 1.488525390625}
 
(14, 3, 1, 96, 224, 96)
Params #:  45024
MACs:  8824704
 
[16:50:41] (INFO) Building library...
[16:50:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.550001109730114, 'median': 2.5203857421875, 'mins': 2.43408203125}
 
(14, 3, 1, 96, 240, 48)
Params #:  36720
MACs:  7197120
 
[16:51:01] (INFO) Building library...
[16:51:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7163230202414772, 'median': 1.6982421875, 'mins': 1.60546875}
 
(14, 3, 1, 96, 240, 96)
Params #:  48240
MACs:  9455040
 
[16:51:18] (INFO) Building library...
[16:51:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.542703524502841, 'median': 2.4967041015625, 'mins': 2.40576171875}
 
(14, 3, 1, 96, 256, 48)
Params #:  39168
MACs:  7676928
 
[16:51:37] (INFO) Building library...
[16:51:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0328258167613638, 'median': 2.0091552734375, 'mins': 1.7772216796875}
 
(14, 3, 1, 96, 256, 96)
Params #:  51456
MACs:  10085376
 
[16:51:54] (INFO) Building library...
[16:51:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3131458629261363, 'median': 2.2877197265625, 'mins': 2.2088623046875}
 
(14, 3, 1, 96, 272, 48)
Params #:  41616
MACs:  8156736
 
[16:52:13] (INFO) Building library...
[16:52:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5710893110795454, 'median': 1.551513671875, 'mins': 1.486328125}
 
(14, 3, 1, 96, 272, 96)
Params #:  54672
MACs:  10715712
 
[16:52:30] (INFO) Building library...
[16:52:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5390303178267044, 'median': 2.461181640625, 'mins': 2.3265380859375}
 
(14, 3, 1, 96, 288, 48)
Params #:  44064
MACs:  8636544
 
[16:52:50] (INFO) Building library...
[16:52:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4943592418323863, 'median': 1.45849609375, 'mins': 1.4061279296875}
 
(14, 3, 1, 96, 288, 96)
Params #:  57888
MACs:  11346048
 
[16:53:08] (INFO) Building library...
[16:53:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.39266357421875, 'median': 2.394775390625, 'mins': 2.2950439453125}
 
(14, 3, 1, 96, 304, 48)
Params #:  46512
MACs:  9116352
 
[16:53:27] (INFO) Building library...
[16:53:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7753462357954546, 'median': 1.7484130859375, 'mins': 1.65478515625}
 
(14, 3, 1, 96, 304, 96)
Params #:  61104
MACs:  11976384
 
[16:53:44] (INFO) Building library...
[16:53:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5896650834517048, 'median': 2.546142578125, 'mins': 2.4510498046875}
 
(14, 3, 1, 96, 320, 48)
Params #:  48960
MACs:  9596160
 
[16:54:04] (INFO) Building library...
[16:54:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4724109996448864, 'median': 1.45849609375, 'mins': 1.3916015625}
 
(14, 3, 1, 96, 320, 96)
Params #:  64320
MACs:  12606720
 
[16:54:21] (INFO) Building library...
[16:54:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4819868607954545, 'median': 2.462646484375, 'mins': 2.3753662109375}
 
(14, 3, 1, 96, 336, 48)
Params #:  51408
MACs:  10075968
 
[16:54:40] (INFO) Building library...
[16:54:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7604714133522728, 'median': 1.7393798828125, 'mins': 1.6419677734375}
 
(14, 3, 1, 96, 336, 96)
Params #:  67536
MACs:  13237056
 
[16:54:57] (INFO) Building library...
[16:54:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5232033469460227, 'median': 2.4976806640625, 'mins': 2.4017333984375}
 
(14, 3, 1, 96, 352, 48)
Params #:  53856
MACs:  10555776
 
[16:55:17] (INFO) Building library...
[16:55:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7431085759943181, 'median': 1.7227783203125, 'mins': 1.6182861328125}
 
(14, 3, 1, 96, 352, 96)
Params #:  70752
MACs:  13867392
 
[16:55:34] (INFO) Building library...
[16:55:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4859630237926136, 'median': 2.451904296875, 'mins': 2.334228515625}
 
(14, 3, 1, 96, 368, 48)
Params #:  56304
MACs:  11035584
 
[16:55:54] (INFO) Building library...
[16:55:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9068703391335227, 'median': 1.79736328125, 'mins': 1.6553955078125}
 
(14, 3, 1, 96, 368, 96)
Params #:  73968
MACs:  14497728
 
[16:56:11] (INFO) Building library...
[16:56:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.511110617897727, 'median': 2.4732666015625, 'mins': 2.3509521484375}
 
(14, 3, 1, 96, 384, 48)
Params #:  58752
MACs:  11515392
 
[16:56:31] (INFO) Building library...
[16:56:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5796431107954545, 'median': 1.55517578125, 'mins': 1.4710693359375}
 
(14, 3, 1, 96, 384, 96)
Params #:  77184
MACs:  15128064
 
[16:56:47] (INFO) Building library...
[16:56:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0586558948863636, 'median': 2.0262451171875, 'mins': 1.9371337890625}
 
(14, 3, 1, 96, 400, 48)
Params #:  61200
MACs:  11995200
 
[16:57:07] (INFO) Building library...
[16:57:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8672119140625, 'median': 1.75390625, 'mins': 1.6494140625}
 
(14, 3, 1, 96, 400, 96)
Params #:  80400
MACs:  15758400
 
[16:57:24] (INFO) Building library...
[16:57:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.470391290838068, 'median': 2.4398193359375, 'mins': 2.34423828125}
 
(14, 3, 1, 96, 416, 48)
Params #:  63648
MACs:  12475008
 
[16:57:44] (INFO) Building library...
[16:57:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.530685147372159, 'median': 1.50927734375, 'mins': 1.43994140625}
 
(14, 3, 1, 96, 416, 96)
Params #:  83616
MACs:  16388736
 
[16:58:01] (INFO) Building library...
[16:58:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.479117098721591, 'median': 2.447998046875, 'mins': 2.3436279296875}
 
(14, 3, 1, 96, 432, 48)
Params #:  66096
MACs:  12954816
 
[16:58:20] (INFO) Building library...
[16:58:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5040449662642046, 'median': 1.495361328125, 'mins': 1.42822265625}
 
(14, 3, 1, 96, 432, 96)
Params #:  86832
MACs:  17019072
 
[16:58:37] (INFO) Building library...
[16:58:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.57149658203125, 'median': 2.53662109375, 'mins': 2.443115234375}
 
(14, 3, 1, 96, 448, 48)
Params #:  68544
MACs:  13434624
 
[16:58:57] (INFO) Building library...
[16:58:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.788931551846591, 'median': 1.7657470703125, 'mins': 1.6737060546875}
 
(14, 3, 1, 96, 448, 96)
Params #:  90048
MACs:  17649408
 
[16:59:14] (INFO) Building library...
[16:59:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5855768377130683, 'median': 2.55810546875, 'mins': 2.451904296875}
 
(14, 3, 1, 96, 464, 48)
Params #:  70992
MACs:  13914432
 
[16:59:33] (INFO) Building library...
[16:59:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.747490900213068, 'median': 1.718505859375, 'mins': 1.6407470703125}
 
(14, 3, 1, 96, 464, 96)
Params #:  93264
MACs:  18279744
 
[16:59:50] (INFO) Building library...
[16:59:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.491057794744318, 'median': 2.4853515625, 'mins': 2.3836669921875}
 
(14, 3, 1, 96, 480, 48)
Params #:  73440
MACs:  14394240
 
[17:00:09] (INFO) Building library...
[17:00:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8997492009943182, 'median': 1.864501953125, 'mins': 1.767578125}
 
(14, 3, 1, 96, 480, 96)
Params #:  96480
MACs:  18910080
 
[17:00:27] (INFO) Building library...
[17:00:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5027976296164773, 'median': 2.472900390625, 'mins': 2.38134765625}
 
(14, 3, 1, 96, 496, 48)
Params #:  75888
MACs:  14874048
 
[17:00:46] (INFO) Building library...
[17:00:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9402598987926136, 'median': 1.9522705078125, 'mins': 1.7156982421875}
 
(14, 3, 1, 96, 496, 96)
Params #:  99696
MACs:  19540416
 
[17:01:03] (INFO) Building library...
[17:01:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6272050337357955, 'median': 2.58740234375, 'mins': 2.498291015625}
 
(14, 3, 1, 96, 512, 48)
Params #:  78336
MACs:  15353856
 
[17:01:23] (INFO) Building library...
[17:01:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8115001331676137, 'median': 1.7874755859375, 'mins': 1.677978515625}
 
(14, 3, 1, 96, 512, 96)
Params #:  102912
MACs:  20170752
 
[17:01:40] (INFO) Building library...
[17:01:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.514609596946023, 'median': 2.474609375, 'mins': 2.3643798828125}
 
(14, 3, 1, 96, 528, 48)
Params #:  80784
MACs:  15833664
 
[17:01:59] (INFO) Building library...
[17:01:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6145208185369317, 'median': 1.6031494140625, 'mins': 1.5377197265625}
 
(14, 3, 1, 96, 528, 96)
Params #:  106128
MACs:  20801088
 
[17:02:16] (INFO) Building library...
[17:02:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7610984108664773, 'median': 2.718505859375, 'mins': 2.6219482421875}
 
(14, 3, 1, 96, 544, 48)
Params #:  83232
MACs:  16313472
 
[17:02:36] (INFO) Building library...
[17:02:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8541237571022726, 'median': 1.8275146484375, 'mins': 1.700927734375}
 
(14, 3, 1, 96, 544, 96)
Params #:  109344
MACs:  21431424
 
[17:02:53] (INFO) Building library...
[17:02:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3029141512784093, 'median': 2.2694091796875, 'mins': 2.1820068359375}
 
(14, 3, 1, 96, 560, 48)
Params #:  85680
MACs:  16793280
 
[17:03:12] (INFO) Building library...
[17:03:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8950761274857955, 'median': 1.84423828125, 'mins': 1.7017822265625}
 
(14, 3, 1, 96, 560, 96)
Params #:  112560
MACs:  22061760
 
[17:03:30] (INFO) Building library...
[17:03:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6320911754261362, 'median': 2.5970458984375, 'mins': 2.494384765625}
 
(14, 3, 1, 96, 576, 48)
Params #:  88128
MACs:  17273088
 
[17:03:49] (INFO) Building library...
[17:03:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.437674227627841, 'median': 1.4195556640625, 'mins': 1.352783203125}
 
(14, 3, 1, 96, 576, 96)
Params #:  115776
MACs:  22692096
 
[17:04:06] (INFO) Building library...
[17:04:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8871282404119318, 'median': 1.8773193359375, 'mins': 1.801025390625}
 
(14, 3, 1, 96, 592, 48)
Params #:  90576
MACs:  17752896
 
[17:04:26] (INFO) Building library...
[17:04:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7928722034801137, 'median': 1.7698974609375, 'mins': 1.6795654296875}
 
(14, 3, 1, 96, 592, 96)
Params #:  118992
MACs:  23322432
 
[17:04:43] (INFO) Building library...
[17:04:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6454833984375, 'median': 2.6109619140625, 'mins': 2.51025390625}
 
(14, 3, 1, 96, 608, 48)
Params #:  93024
MACs:  18232704
 
[17:05:02] (INFO) Building library...
[17:05:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8155828302556818, 'median': 1.8104248046875, 'mins': 1.703369140625}
 
(14, 3, 1, 96, 608, 96)
Params #:  122208
MACs:  23952768
 
[17:05:19] (INFO) Building library...
[17:05:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.525421697443182, 'median': 2.492431640625, 'mins': 2.3941650390625}
 
(14, 3, 1, 96, 624, 48)
Params #:  95472
MACs:  18712512
 
[17:05:39] (INFO) Building library...
[17:05:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.851171875, 'median': 1.719970703125, 'mins': 1.5911865234375}
 
(14, 3, 1, 96, 624, 96)
Params #:  125424
MACs:  24583104
 
[17:05:56] (INFO) Building library...
[17:05:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.276028719815341, 'median': 2.274169921875, 'mins': 2.1756591796875}
 
(14, 3, 1, 96, 640, 48)
Params #:  97920
MACs:  19192320
 
[17:06:16] (INFO) Building library...
[17:06:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8647316672585228, 'median': 1.835205078125, 'mins': 1.7406005859375}
 
(14, 3, 1, 96, 640, 96)
Params #:  128640
MACs:  25213440
 
[17:06:33] (INFO) Building library...
[17:06:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.581036931818182, 'median': 2.5509033203125, 'mins': 2.4603271484375}
 
(14, 3, 1, 96, 656, 48)
Params #:  100368
MACs:  19672128
 
[17:06:52] (INFO) Building library...
[17:06:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7585582386363636, 'median': 1.7501220703125, 'mins': 1.677490234375}
 
(14, 3, 1, 96, 656, 96)
Params #:  131856
MACs:  25843776
 
[17:07:09] (INFO) Building library...
[17:07:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.70487060546875, 'median': 2.63916015625, 'mins': 2.49365234375}
 
(14, 3, 1, 96, 672, 48)
Params #:  102816
MACs:  20151936
 
[17:07:29] (INFO) Building library...
[17:07:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.915823641690341, 'median': 1.8486328125, 'mins': 1.753662109375}
 
(14, 3, 1, 96, 672, 96)
Params #:  135072
MACs:  26474112
 
[17:07:46] (INFO) Building library...
[17:07:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5156815962357952, 'median': 2.5006103515625, 'mins': 2.4149169921875}
 
(14, 3, 1, 96, 688, 48)
Params #:  105264
MACs:  20631744
 
[17:08:06] (INFO) Building library...
[17:08:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.999081143465909, 'median': 1.842041015625, 'mins': 1.7252197265625}
 
(14, 3, 1, 96, 688, 96)
Params #:  138288
MACs:  27104448
 
[17:08:23] (INFO) Building library...
[17:08:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.597328879616477, 'median': 2.587890625, 'mins': 2.499267578125}
 
(14, 3, 1, 96, 704, 48)
Params #:  107712
MACs:  21111552
 
[17:08:43] (INFO) Building library...
[17:08:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9654374556107954, 'median': 1.9697265625, 'mins': 1.697998046875}
 
(14, 3, 1, 96, 704, 96)
Params #:  141504
MACs:  27734784
 
[17:09:00] (INFO) Building library...
[17:09:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.556912508877841, 'median': 2.5224609375, 'mins': 2.41796875}
 
(14, 3, 1, 96, 720, 48)
Params #:  110160
MACs:  21591360
 
[17:09:19] (INFO) Building library...
[17:09:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.591437322443182, 'median': 1.572021484375, 'mins': 1.4964599609375}
 
(14, 3, 1, 96, 720, 96)
Params #:  144720
MACs:  28365120
 
[17:09:37] (INFO) Building library...
[17:09:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.688741787997159, 'median': 2.639404296875, 'mins': 2.5157470703125}
 
(14, 3, 1, 96, 736, 48)
Params #:  112608
MACs:  22071168
 
[17:09:56] (INFO) Building library...
[17:09:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.790328702059659, 'median': 1.7572021484375, 'mins': 1.6700439453125}
 
(14, 3, 1, 96, 736, 96)
Params #:  147936
MACs:  28995456
 
[17:10:13] (INFO) Building library...
[17:10:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.685336026278409, 'median': 2.6275634765625, 'mins': 2.5059814453125}
 
(14, 3, 1, 96, 752, 48)
Params #:  115056
MACs:  22550976
 
[17:10:33] (INFO) Building library...
[17:10:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.844418057528409, 'median': 1.8114013671875, 'mins': 1.71533203125}
 
(14, 3, 1, 96, 752, 96)
Params #:  151152
MACs:  29625792
 
[17:10:50] (INFO) Building library...
[17:10:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.52236328125, 'median': 2.4840087890625, 'mins': 2.391845703125}
 
(14, 3, 1, 96, 768, 48)
Params #:  117504
MACs:  23030784
 
[17:11:09] (INFO) Building library...
[17:11:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8537908380681818, 'median': 1.8349609375, 'mins': 1.71044921875}
 
(14, 3, 1, 96, 768, 96)
Params #:  154368
MACs:  30256128
 
[17:11:27] (INFO) Building library...
[17:11:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.631319912997159, 'median': 2.599609375, 'mins': 2.4796142578125}
 
(14, 3, 1, 96, 784, 48)
Params #:  119952
MACs:  23510592
 
[17:11:46] (INFO) Building library...
[17:11:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7414850408380682, 'median': 1.728271484375, 'mins': 1.6409912109375}
 
(14, 3, 1, 96, 784, 96)
Params #:  157584
MACs:  30886464
 
[17:12:04] (INFO) Building library...
[17:12:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.55042724609375, 'median': 2.517333984375, 'mins': 2.42578125}
 
(14, 3, 1, 96, 800, 48)
Params #:  122400
MACs:  23990400
 
[17:12:23] (INFO) Building library...
[17:12:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9938953746448864, 'median': 1.8583984375, 'mins': 1.7386474609375}
 
(14, 3, 1, 96, 800, 96)
Params #:  160800
MACs:  31516800
 
[17:12:41] (INFO) Building library...
[17:12:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5165605024857953, 'median': 2.4781494140625, 'mins': 2.3756103515625}
 
(14, 3, 1, 96, 816, 48)
Params #:  124848
MACs:  24470208
 
[17:13:00] (INFO) Building library...
[17:13:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7335094105113635, 'median': 1.713134765625, 'mins': 1.64013671875}
 
(14, 3, 1, 96, 816, 96)
Params #:  164016
MACs:  32147136
 
[17:13:18] (INFO) Building library...
[17:13:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.567159756747159, 'median': 2.524658203125, 'mins': 2.392333984375}
 
(14, 3, 1, 96, 832, 48)
Params #:  127296
MACs:  24950016
 
[17:13:37] (INFO) Building library...
[17:13:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8147838245738637, 'median': 1.776123046875, 'mins': 1.6993408203125}
 
(14, 3, 1, 96, 832, 96)
Params #:  167232
MACs:  32777472
 
[17:13:55] (INFO) Building library...
[17:13:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.580252352627841, 'median': 2.536376953125, 'mins': 2.4234619140625}
 
(14, 3, 1, 96, 848, 48)
Params #:  129744
MACs:  25429824
 
[17:14:14] (INFO) Building library...
[17:14:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7765802556818182, 'median': 1.7388916015625, 'mins': 1.6466064453125}
 
(14, 3, 1, 96, 848, 96)
Params #:  170448
MACs:  33407808
 
[17:14:32] (INFO) Building library...
[17:14:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.745956143465909, 'median': 2.701171875, 'mins': 2.6009521484375}
 
(14, 3, 2, 96, 16, 80)
Params #:  2960
MACs:  370832
 
[17:14:51] (INFO) Building library...
[17:14:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4004161487926137, 'median': 1.369384765625, 'mins': 1.2955322265625}
 
(14, 3, 2, 96, 16, 160)
Params #:  4240
MACs:  433552
 
[17:15:08] (INFO) Building library...
[17:15:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4177512428977272, 'median': 1.3804931640625, 'mins': 1.308349609375}
 
(14, 3, 2, 96, 32, 80)
Params #:  5920
MACs:  741664
 
[17:15:25] (INFO) Building library...
[17:15:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3720625443892045, 'median': 1.3641357421875, 'mins': 1.298583984375}
 
(14, 3, 2, 96, 32, 160)
Params #:  8480
MACs:  867104
 
[17:15:43] (INFO) Building library...
[17:15:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3814885919744317, 'median': 1.3770751953125, 'mins': 1.3193359375}
 
(14, 3, 2, 96, 48, 80)
Params #:  8880
MACs:  1112496
 
[17:16:00] (INFO) Building library...
[17:16:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5543712269176135, 'median': 1.5396728515625, 'mins': 1.466064453125}
 
(14, 3, 2, 96, 48, 160)
Params #:  12720
MACs:  1300656
 
[17:16:17] (INFO) Building library...
[17:16:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5952703302556819, 'median': 1.577392578125, 'mins': 1.498291015625}
 
(14, 3, 2, 96, 64, 80)
Params #:  11840
MACs:  1483328
 
[17:16:34] (INFO) Building library...
[17:16:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5119673295454545, 'median': 1.4912109375, 'mins': 1.4219970703125}
 
(14, 3, 2, 96, 64, 160)
Params #:  16960
MACs:  1734208
 
[17:16:51] (INFO) Building library...
[17:16:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5333396218039772, 'median': 1.5086669921875, 'mins': 1.4295654296875}
 
(14, 3, 2, 96, 80, 80)
Params #:  14800
MACs:  1854160
 
[17:17:09] (INFO) Building library...
[17:17:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5843583540482955, 'median': 1.5687255859375, 'mins': 1.506103515625}
 
(14, 3, 2, 96, 80, 160)
Params #:  21200
MACs:  2167760
 
[17:17:25] (INFO) Building library...
[17:17:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6042291814630683, 'median': 1.583251953125, 'mins': 1.51220703125}
 
(14, 3, 2, 96, 112, 80)
Params #:  20720
MACs:  2595824
 
[17:17:42] (INFO) Building library...
[17:17:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.847846013849432, 'median': 1.7215576171875, 'mins': 1.5904541015625}
 
(14, 3, 2, 96, 112, 160)
Params #:  29680
MACs:  3034864
 
[17:18:00] (INFO) Building library...
[17:18:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.958187588778409, 'median': 1.9208984375, 'mins': 1.798095703125}
 
(14, 3, 2, 96, 128, 80)
Params #:  23680
MACs:  2966656
 
[17:18:17] (INFO) Building library...
[17:18:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8622480912642045, 'median': 1.7080078125, 'mins': 1.59619140625}
 
(14, 3, 2, 96, 128, 160)
Params #:  33920
MACs:  3468416
 
[17:18:35] (INFO) Building library...
[17:18:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7475885564630682, 'median': 1.7137451171875, 'mins': 1.632568359375}
 
(14, 3, 2, 96, 144, 80)
Params #:  26640
MACs:  3337488
 
[17:18:52] (INFO) Building library...
[17:18:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7161843039772726, 'median': 1.6806640625, 'mins': 1.603759765625}
 
(14, 3, 2, 96, 144, 160)
Params #:  38160
MACs:  3901968
 
[17:19:09] (INFO) Building library...
[17:19:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7948952414772728, 'median': 1.7850341796875, 'mins': 1.6387939453125}
 
(14, 3, 2, 96, 160, 80)
Params #:  29600
MACs:  3708320
 
[17:19:26] (INFO) Building library...
[17:19:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7068769975142046, 'median': 1.6868896484375, 'mins': 1.600830078125}
 
(14, 3, 2, 96, 160, 160)
Params #:  42400
MACs:  4335520
 
[17:19:44] (INFO) Building library...
[17:19:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7048228870738635, 'median': 1.5924072265625, 'mins': 1.4439697265625}
 
(14, 3, 2, 96, 176, 80)
Params #:  32560
MACs:  4079152
 
[17:20:01] (INFO) Building library...
[17:20:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7972267844460228, 'median': 1.7735595703125, 'mins': 1.675048828125}
 
(14, 3, 2, 96, 176, 160)
Params #:  46640
MACs:  4769072
 
[17:20:18] (INFO) Building library...
[17:20:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.802474698153409, 'median': 1.7598876953125, 'mins': 1.6629638671875}
 
(14, 3, 2, 96, 192, 80)
Params #:  35520
MACs:  4449984
 
[17:20:36] (INFO) Building library...
[17:20:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7401444868607954, 'median': 1.71533203125, 'mins': 1.6322021484375}
 
(14, 3, 2, 96, 192, 160)
Params #:  50880
MACs:  5202624
 
[17:20:53] (INFO) Building library...
[17:20:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.795913973721591, 'median': 1.7398681640625, 'mins': 1.6417236328125}
 
(14, 3, 2, 96, 208, 80)
Params #:  38480
MACs:  4820816
 
[17:21:10] (INFO) Building library...
[17:21:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7289717240767046, 'median': 1.698974609375, 'mins': 1.6083984375}
 
(14, 3, 2, 96, 208, 160)
Params #:  55120
MACs:  5636176
 
[17:21:27] (INFO) Building library...
[17:21:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7646428888494319, 'median': 1.7393798828125, 'mins': 1.6363525390625}
 
(14, 3, 2, 96, 224, 80)
Params #:  41440
MACs:  5191648
 
[17:21:45] (INFO) Building library...
[17:21:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6607776988636365, 'median': 1.65185546875, 'mins': 1.5780029296875}
 
(14, 3, 2, 96, 224, 160)
Params #:  59360
MACs:  6069728
 
[17:22:02] (INFO) Building library...
[17:22:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7627208362926137, 'median': 1.7396240234375, 'mins': 1.646240234375}
 
(14, 3, 2, 96, 240, 80)
Params #:  44400
MACs:  5562480
 
[17:22:19] (INFO) Building library...
[17:22:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5483298561789773, 'median': 1.5247802734375, 'mins': 1.4512939453125}
 
(14, 3, 2, 96, 240, 160)
Params #:  63600
MACs:  6503280
 
[17:22:37] (INFO) Building library...
[17:22:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7847767223011364, 'median': 1.751220703125, 'mins': 1.656494140625}
 
(14, 3, 2, 96, 256, 80)
Params #:  47360
MACs:  5933312
 
[17:22:54] (INFO) Building library...
[17:22:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.729867276278409, 'median': 1.69970703125, 'mins': 1.6124267578125}
 
(14, 3, 2, 96, 256, 160)
Params #:  67840
MACs:  6936832
 
[17:23:11] (INFO) Building library...
[17:23:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7521584250710227, 'median': 1.7236328125, 'mins': 1.62451171875}
 
(14, 3, 2, 96, 272, 80)
Params #:  50320
MACs:  6304144
 
[17:23:27] (INFO) Building library...
[17:23:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.72747802734375, 'median': 1.7086181640625, 'mins': 1.6075439453125}
 
(14, 3, 2, 96, 272, 160)
Params #:  72080
MACs:  7370384
 
[17:23:45] (INFO) Building library...
[17:23:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7176657936789772, 'median': 1.696044921875, 'mins': 1.609375}
 
(14, 3, 2, 96, 288, 80)
Params #:  53280
MACs:  6674976
 
[17:24:02] (INFO) Building library...
[17:24:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5832841352982954, 'median': 1.5623779296875, 'mins': 1.4559326171875}
 
(14, 3, 2, 96, 288, 160)
Params #:  76320
MACs:  7803936
 
[17:24:20] (INFO) Building library...
[17:24:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7070234818892045, 'median': 1.6978759765625, 'mins': 1.625}
 
(14, 3, 2, 96, 304, 80)
Params #:  56240
MACs:  7045808
 
[17:24:37] (INFO) Building library...
[17:24:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6860751065340909, 'median': 1.667724609375, 'mins': 1.5867919921875}
 
(14, 3, 2, 96, 304, 160)
Params #:  80560
MACs:  8237488
 
[17:24:54] (INFO) Building library...
[17:24:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7620794122869319, 'median': 1.737548828125, 'mins': 1.63818359375}
 
(14, 3, 2, 96, 320, 80)
Params #:  59200
MACs:  7416640
 
[17:25:12] (INFO) Building library...
[17:25:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7291304154829545, 'median': 1.7132568359375, 'mins': 1.629638671875}
 
(14, 3, 2, 96, 320, 160)
Params #:  84800
MACs:  8671040
 
[17:25:29] (INFO) Building library...
[17:25:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7315118963068181, 'median': 1.7098388671875, 'mins': 1.62353515625}
 
(14, 3, 2, 96, 336, 80)
Params #:  62160
MACs:  7787472
 
[17:25:47] (INFO) Building library...
[17:25:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6891002308238636, 'median': 1.666259765625, 'mins': 1.58251953125}
 
(14, 3, 2, 96, 336, 160)
Params #:  89040
MACs:  9104592
 
[17:26:04] (INFO) Building library...
[17:26:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8581143465909091, 'median': 1.7242431640625, 'mins': 1.6005859375}
 
(14, 3, 2, 96, 352, 80)
Params #:  65120
MACs:  8158304
 
[17:26:21] (INFO) Building library...
[17:26:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7623668323863637, 'median': 1.736572265625, 'mins': 1.627685546875}
 
(14, 3, 2, 96, 352, 160)
Params #:  93280
MACs:  9538144
 
[17:26:39] (INFO) Building library...
[17:26:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5907648259943181, 'median': 1.559814453125, 'mins': 1.4915771484375}
 
(14, 3, 2, 96, 368, 80)
Params #:  68080
MACs:  8529136
 
[17:26:56] (INFO) Building library...
[17:26:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6735584605823863, 'median': 1.6541748046875, 'mins': 1.5787353515625}
 
(14, 3, 2, 96, 368, 160)
Params #:  97520
MACs:  9971696
 
[17:27:13] (INFO) Building library...
[17:27:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.735049715909091, 'median': 1.726806640625, 'mins': 1.646484375}
 
(14, 3, 2, 96, 384, 80)
Params #:  71040
MACs:  8899968
 
[17:27:30] (INFO) Building library...
[17:27:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7245505593039774, 'median': 1.6839599609375, 'mins': 1.6072998046875}
 
(14, 3, 2, 96, 384, 160)
Params #:  101760
MACs:  10405248
 
[17:27:47] (INFO) Building library...
[17:27:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7044533469460228, 'median': 1.6793212890625, 'mins': 1.588134765625}
 
(14, 3, 2, 96, 400, 80)
Params #:  74000
MACs:  9270800
 
[17:28:04] (INFO) Building library...
[17:28:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7368585759943183, 'median': 1.707275390625, 'mins': 1.62939453125}
 
(14, 3, 2, 96, 400, 160)
Params #:  106000
MACs:  10838800
 
[17:28:21] (INFO) Building library...
[17:28:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.754928311434659, 'median': 1.7235107421875, 'mins': 1.616943359375}
 
(14, 3, 2, 96, 416, 80)
Params #:  76960
MACs:  9641632
 
[17:28:38] (INFO) Building library...
[17:28:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7330610795454546, 'median': 1.7137451171875, 'mins': 1.6224365234375}
 
(14, 3, 2, 96, 416, 160)
Params #:  110240
MACs:  11272352
 
[17:28:55] (INFO) Building library...
[17:28:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8436512340198863, 'median': 1.820556640625, 'mins': 1.7110595703125}
 
(14, 3, 2, 96, 432, 80)
Params #:  79920
MACs:  10012464
 
[17:29:13] (INFO) Building library...
[17:29:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5316162109375, 'median': 1.5133056640625, 'mins': 1.44677734375}
 
(14, 3, 2, 96, 432, 160)
Params #:  114480
MACs:  11705904
 
[17:29:30] (INFO) Building library...
[17:29:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7440962357954546, 'median': 1.7308349609375, 'mins': 1.647216796875}
 
(14, 3, 2, 96, 448, 80)
Params #:  82880
MACs:  10383296
 
[17:29:47] (INFO) Building library...
[17:29:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6628884055397728, 'median': 1.566162109375, 'mins': 1.444580078125}
 
(14, 3, 2, 96, 448, 160)
Params #:  118720
MACs:  12139456
 
[17:30:04] (INFO) Building library...
[17:30:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8143454811789772, 'median': 1.7841796875, 'mins': 1.69091796875}
 
(14, 3, 2, 96, 464, 80)
Params #:  85840
MACs:  10754128
 
[17:30:22] (INFO) Building library...
[17:30:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8908214222301136, 'median': 1.8662109375, 'mins': 1.765869140625}
 
(14, 3, 2, 96, 464, 160)
Params #:  122960
MACs:  12573008
 
[17:30:39] (INFO) Building library...
[17:30:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.806258877840909, 'median': 1.768310546875, 'mins': 1.6798095703125}
 
(14, 3, 2, 96, 480, 80)
Params #:  88800
MACs:  11124960
 
[17:30:56] (INFO) Building library...
[17:30:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5629372336647727, 'median': 1.53515625, 'mins': 1.4556884765625}
 
(14, 3, 2, 96, 480, 160)
Params #:  127200
MACs:  13006560
 
[17:31:14] (INFO) Building library...
[17:31:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5292835582386364, 'median': 1.5206298828125, 'mins': 1.44921875}
 
(14, 3, 2, 96, 496, 80)
Params #:  91760
MACs:  11495792
 
[17:31:31] (INFO) Building library...
[17:31:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7161443536931817, 'median': 1.692138671875, 'mins': 1.6197509765625}
 
(14, 3, 2, 96, 496, 160)
Params #:  131440
MACs:  13440112
 
[17:31:49] (INFO) Building library...
[17:31:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.73345947265625, 'median': 1.710205078125, 'mins': 1.6136474609375}
 
(14, 3, 2, 96, 512, 80)
Params #:  94720
MACs:  11866624
 
[17:32:06] (INFO) Building library...
[17:32:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7390735973011364, 'median': 1.704833984375, 'mins': 1.604248046875}
 
(14, 3, 2, 96, 512, 160)
Params #:  135680
MACs:  13873664
 
[17:32:23] (INFO) Building library...
[17:32:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.056642844460227, 'median': 2.0059814453125, 'mins': 1.9078369140625}
 
(14, 3, 2, 96, 528, 80)
Params #:  97680
MACs:  12237456
 
[17:32:40] (INFO) Building library...
[17:32:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7489113547585227, 'median': 1.72265625, 'mins': 1.64013671875}
 
(14, 3, 2, 96, 528, 160)
Params #:  139920
MACs:  14307216
 
[17:32:57] (INFO) Building library...
[17:32:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6440507368607955, 'median': 1.6038818359375, 'mins': 1.5255126953125}
 
(14, 3, 2, 96, 544, 80)
Params #:  100640
MACs:  12608288
 
[17:33:14] (INFO) Building library...
[17:33:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8572365500710226, 'median': 1.848876953125, 'mins': 1.75439453125}
 
(14, 3, 2, 96, 544, 160)
Params #:  144160
MACs:  14740768
 
[17:33:32] (INFO) Building library...
[17:33:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6264348810369318, 'median': 1.6021728515625, 'mins': 1.52001953125}
 
(14, 3, 2, 96, 560, 80)
Params #:  103600
MACs:  12979120
 
[17:33:49] (INFO) Building library...
[17:33:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7492908824573863, 'median': 1.72265625, 'mins': 1.62060546875}
 
(14, 3, 2, 96, 560, 160)
Params #:  148400
MACs:  15174320
 
[17:34:06] (INFO) Building library...
[17:34:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.710269442471591, 'median': 1.670654296875, 'mins': 1.5950927734375}
 
(14, 3, 2, 96, 576, 80)
Params #:  106560
MACs:  13349952
 
[17:34:23] (INFO) Building library...
[17:34:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7153708718039773, 'median': 1.581787109375, 'mins': 1.4964599609375}
 
(14, 3, 2, 96, 576, 160)
Params #:  152640
MACs:  15607872
 
[17:34:41] (INFO) Building library...
[17:34:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.2396373401988636, 'median': 1.2181396484375, 'mins': 1.141357421875}
 
(14, 3, 2, 96, 592, 80)
Params #:  109520
MACs:  13720784
 
[17:34:58] (INFO) Building library...
[17:34:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5116932262073863, 'median': 1.501953125, 'mins': 1.4405517578125}
 
(14, 3, 2, 96, 592, 160)
Params #:  156880
MACs:  16041424
 
[17:35:15] (INFO) Building library...
[17:35:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8140691583806818, 'median': 1.7933349609375, 'mins': 1.6920166015625}
 
(14, 3, 2, 96, 608, 80)
Params #:  112480
MACs:  14091616
 
[17:35:32] (INFO) Building library...
[17:35:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8764881480823863, 'median': 1.862548828125, 'mins': 1.76611328125}
 
(14, 3, 2, 96, 608, 160)
Params #:  161120
MACs:  16474976
 
[17:35:50] (INFO) Building library...
[17:35:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9018088600852272, 'median': 1.8416748046875, 'mins': 1.747314453125}
 
(14, 3, 2, 96, 624, 80)
Params #:  115440
MACs:  14462448
 
[17:36:07] (INFO) Building library...
[17:36:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.877987393465909, 'median': 1.8590087890625, 'mins': 1.7498779296875}
 
(14, 3, 2, 96, 624, 160)
Params #:  165360
MACs:  16908528
 
[17:36:25] (INFO) Building library...
[17:36:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8235828746448863, 'median': 1.80224609375, 'mins': 1.7069091796875}
 
(14, 3, 2, 96, 640, 80)
Params #:  118400
MACs:  14833280
 
[17:36:42] (INFO) Building library...
[17:36:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.898861416903409, 'median': 1.8651123046875, 'mins': 1.7857666015625}
 
(14, 3, 2, 96, 640, 160)
Params #:  169600
MACs:  17342080
 
[17:36:59] (INFO) Building library...
[17:36:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8833662553267045, 'median': 1.8431396484375, 'mins': 1.727294921875}
 
(14, 3, 2, 96, 656, 80)
Params #:  121360
MACs:  15204112
 
[17:37:17] (INFO) Building library...
[17:37:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8805231267755682, 'median': 1.844482421875, 'mins': 1.7547607421875}
 
(14, 3, 2, 96, 656, 160)
Params #:  173840
MACs:  17775632
 
[17:37:34] (INFO) Building library...
[17:37:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7730335582386363, 'median': 1.7547607421875, 'mins': 1.6715087890625}
 
(14, 3, 2, 96, 672, 80)
Params #:  124320
MACs:  15574944
 
[17:37:52] (INFO) Building library...
[17:37:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.081068004261364, 'median': 1.95458984375, 'mins': 1.849609375}
 
(14, 3, 2, 96, 672, 160)
Params #:  178080
MACs:  18209184
 
[17:38:10] (INFO) Building library...
[17:38:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9333518288352274, 'median': 1.7762451171875, 'mins': 1.651611328125}
 
(14, 3, 2, 96, 688, 80)
Params #:  127280
MACs:  15945776
 
[17:38:27] (INFO) Building library...
[17:38:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6857776988636364, 'median': 1.564697265625, 'mins': 1.4776611328125}
 
(14, 3, 2, 96, 688, 160)
Params #:  182320
MACs:  18642736
 
[17:38:45] (INFO) Building library...
[17:38:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9922429865056819, 'median': 1.8363037109375, 'mins': 1.7391357421875}
 
(14, 3, 2, 96, 704, 80)
Params #:  130240
MACs:  16316608
 
[17:39:03] (INFO) Building library...
[17:39:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8385198419744317, 'median': 1.807861328125, 'mins': 1.692138671875}
 
(14, 3, 2, 96, 704, 160)
Params #:  186560
MACs:  19076288
 
[17:39:20] (INFO) Building library...
[17:39:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8110628995028408, 'median': 1.7672119140625, 'mins': 1.684326171875}
 
(14, 3, 2, 96, 720, 80)
Params #:  133200
MACs:  16687440
 
[17:39:38] (INFO) Building library...
[17:39:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7976939808238637, 'median': 1.7366943359375, 'mins': 1.6231689453125}
 
(14, 3, 2, 96, 720, 160)
Params #:  190800
MACs:  19509840
 
[17:39:55] (INFO) Building library...
[17:39:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9705344460227272, 'median': 1.7962646484375, 'mins': 1.6986083984375}
 
(14, 3, 2, 96, 736, 80)
Params #:  136160
MACs:  17058272
 
[17:40:13] (INFO) Building library...
[17:40:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.870856267755682, 'median': 1.845947265625, 'mins': 1.73876953125}
 
(14, 3, 2, 96, 736, 160)
Params #:  195040
MACs:  19943392
 
[17:40:30] (INFO) Building library...
[17:40:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.739315518465909, 'median': 1.69873046875, 'mins': 1.6116943359375}
 
(14, 3, 2, 96, 752, 80)
Params #:  139120
MACs:  17429104
 
[17:40:47] (INFO) Building library...
[17:40:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6930442116477273, 'median': 1.6761474609375, 'mins': 1.602294921875}
 
(14, 3, 2, 96, 752, 160)
Params #:  199280
MACs:  20376944
 
[17:41:05] (INFO) Building library...
[17:41:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7009532581676137, 'median': 1.6595458984375, 'mins': 1.575927734375}
 
(14, 3, 2, 96, 768, 80)
Params #:  142080
MACs:  17799936
 
[17:41:22] (INFO) Building library...
[17:41:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8809814453125, 'median': 1.75390625, 'mins': 1.6561279296875}
 
(14, 3, 2, 96, 768, 160)
Params #:  203520
MACs:  20810496
 
[17:41:40] (INFO) Building library...
[17:41:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8199818004261363, 'median': 1.7996826171875, 'mins': 1.7083740234375}
 
(14, 3, 2, 96, 784, 80)
Params #:  145040
MACs:  18170768
 
[17:41:57] (INFO) Building library...
[17:41:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7394708806818182, 'median': 1.7073974609375, 'mins': 1.62109375}
 
(14, 3, 2, 96, 784, 160)
Params #:  207760
MACs:  21244048
 
[17:42:15] (INFO) Building library...
[17:42:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7375732421875, 'median': 1.723388671875, 'mins': 1.6585693359375}
 
(14, 3, 2, 96, 800, 80)
Params #:  148000
MACs:  18541600
 
[17:42:32] (INFO) Building library...
[17:42:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7578302556818182, 'median': 1.73876953125, 'mins': 1.6387939453125}
 
(14, 3, 2, 96, 800, 160)
Params #:  212000
MACs:  21677600
 
[17:42:49] (INFO) Building library...
[17:42:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7349198774857955, 'median': 1.6875, 'mins': 1.60595703125}
 
(14, 3, 2, 96, 816, 80)
Params #:  150960
MACs:  18912432
 
[17:43:07] (INFO) Building library...
[17:43:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6966907848011363, 'median': 1.6861572265625, 'mins': 1.607177734375}
 
(14, 3, 2, 96, 816, 160)
Params #:  216240
MACs:  22111152
 
[17:43:24] (INFO) Building library...
[17:43:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8785089666193182, 'median': 1.859375, 'mins': 1.741943359375}
 
(14, 3, 2, 96, 832, 80)
Params #:  153920
MACs:  19283264
 
[17:43:41] (INFO) Building library...
[17:43:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.704226962002841, 'median': 1.6912841796875, 'mins': 1.628173828125}
 
(14, 3, 2, 96, 832, 160)
Params #:  220480
MACs:  22544704
 
[17:43:59] (INFO) Building library...
[17:43:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8861993963068182, 'median': 1.8721923828125, 'mins': 1.76806640625}
 
(14, 3, 2, 96, 848, 80)
Params #:  156880
MACs:  19654096
 
[17:44:16] (INFO) Building library...
[17:44:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7810680042613636, 'median': 1.7630615234375, 'mins': 1.6937255859375}
 
(14, 3, 2, 96, 848, 160)
Params #:  224720
MACs:  22978256
 
[17:44:34] (INFO) Building library...
[17:44:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7575439453125, 'median': 1.7274169921875, 'mins': 1.6239013671875}
 
(7, 3, 1, 160, 16, 80)
Params #:  3984
MACs:  195216
 
[17:44:51] (INFO) Building library...
[17:44:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.500430575284091, 'median': 1.478271484375, 'mins': 1.3953857421875}
 
(7, 3, 1, 160, 16, 160)
Params #:  5264
MACs:  257936
 
[17:45:08] (INFO) Building library...
[17:45:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.219388094815341, 'median': 2.2161865234375, 'mins': 2.1282958984375}
 
(7, 3, 1, 160, 32, 80)
Params #:  7968
MACs:  390432
 
[17:45:27] (INFO) Building library...
[17:45:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4997813831676137, 'median': 1.4869384765625, 'mins': 1.4189453125}
 
(7, 3, 1, 160, 32, 160)
Params #:  10528
MACs:  515872
 
[17:45:44] (INFO) Building library...
[17:45:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2594460227272726, 'median': 2.239990234375, 'mins': 2.12890625}
 
(7, 3, 1, 160, 48, 80)
Params #:  11952
MACs:  585648
 
[17:46:03] (INFO) Building library...
[17:46:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4807262073863636, 'median': 1.4619140625, 'mins': 1.3851318359375}
 
(7, 3, 1, 160, 48, 160)
Params #:  15792
MACs:  773808
 
[17:46:20] (INFO) Building library...
[17:46:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2265791459517046, 'median': 2.2191162109375, 'mins': 2.1417236328125}
 
(7, 3, 1, 160, 64, 80)
Params #:  15936
MACs:  780864
 
[17:46:39] (INFO) Building library...
[17:46:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5342917702414773, 'median': 1.515380859375, 'mins': 1.4439697265625}
 
(7, 3, 1, 160, 64, 160)
Params #:  21056
MACs:  1031744
 
[17:46:56] (INFO) Building library...
[17:46:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.373514071377841, 'median': 2.35595703125, 'mins': 2.24755859375}
 
(7, 3, 1, 160, 80, 80)
Params #:  19920
MACs:  976080
 
[17:47:15] (INFO) Building library...
[17:47:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5058571555397726, 'median': 1.499267578125, 'mins': 1.4241943359375}
 
(7, 3, 1, 160, 80, 160)
Params #:  26320
MACs:  1289680
 
[17:47:32] (INFO) Building library...
[17:47:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.270331365411932, 'median': 2.250732421875, 'mins': 2.15087890625}
 
(7, 3, 1, 160, 96, 80)
Params #:  23904
MACs:  1171296
 
[17:47:51] (INFO) Building library...
[17:47:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5072210138494317, 'median': 1.4874267578125, 'mins': 1.4154052734375}
 
(7, 3, 1, 160, 96, 160)
Params #:  31584
MACs:  1547616
 
[17:48:08] (INFO) Building library...
[17:48:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.205274547230114, 'median': 2.1912841796875, 'mins': 2.10595703125}
 
(7, 3, 1, 160, 112, 80)
Params #:  27888
MACs:  1366512
 
[17:48:27] (INFO) Building library...
[17:48:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7374589399857954, 'median': 1.68798828125, 'mins': 1.5029296875}
 
(7, 3, 1, 160, 112, 160)
Params #:  36848
MACs:  1805552
 
[17:48:44] (INFO) Building library...
[17:48:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.285498046875, 'median': 2.2713623046875, 'mins': 2.1871337890625}
 
(7, 3, 1, 160, 128, 80)
Params #:  31872
MACs:  1561728
 
[17:49:03] (INFO) Building library...
[17:49:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6940196644176135, 'median': 1.6673583984375, 'mins': 1.58349609375}
 
(7, 3, 1, 160, 128, 160)
Params #:  42112
MACs:  2063488
 
[17:49:20] (INFO) Building library...
[17:49:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1810191761363638, 'median': 2.1705322265625, 'mins': 2.080322265625}
 
(7, 3, 1, 160, 144, 80)
Params #:  35856
MACs:  1756944
 
[17:49:39] (INFO) Building library...
[17:49:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5131647283380683, 'median': 1.49755859375, 'mins': 1.4248046875}
 
(7, 3, 1, 160, 144, 160)
Params #:  47376
MACs:  2321424
 
[17:49:56] (INFO) Building library...
[17:49:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4200705788352272, 'median': 2.417236328125, 'mins': 2.3028564453125}
 
(7, 3, 1, 160, 176, 80)
Params #:  43824
MACs:  2147376
 
[17:50:15] (INFO) Building library...
[17:50:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.537369051846591, 'median': 1.527587890625, 'mins': 1.444580078125}
 
(7, 3, 1, 160, 176, 160)
Params #:  57904
MACs:  2837296
 
[17:50:32] (INFO) Building library...
[17:50:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2366588245738637, 'median': 2.2310791015625, 'mins': 2.1329345703125}
 
(7, 3, 1, 160, 192, 80)
Params #:  47808
MACs:  2342592
 
[17:50:51] (INFO) Building library...
[17:50:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4784823330965908, 'median': 1.4625244140625, 'mins': 1.397705078125}
 
(7, 3, 1, 160, 192, 160)
Params #:  63168
MACs:  3095232
 
[17:51:08] (INFO) Building library...
[17:51:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1927523526278407, 'median': 2.1865234375, 'mins': 2.099853515625}
 
(7, 3, 1, 160, 208, 80)
Params #:  51792
MACs:  2537808
 
[17:51:27] (INFO) Building library...
[17:51:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5135176225142046, 'median': 1.50537109375, 'mins': 1.4432373046875}
 
(7, 3, 1, 160, 208, 160)
Params #:  68432
MACs:  3353168
 
[17:51:44] (INFO) Building library...
[17:51:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.195172674005682, 'median': 2.1717529296875, 'mins': 2.08740234375}
 
(7, 3, 1, 160, 224, 80)
Params #:  55776
MACs:  2733024
 
[17:52:03] (INFO) Building library...
[17:52:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4761685458096592, 'median': 1.4693603515625, 'mins': 1.4129638671875}
 
(7, 3, 1, 160, 224, 160)
Params #:  73696
MACs:  3611104
 
[17:52:20] (INFO) Building library...
[17:52:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2683094371448864, 'median': 2.258544921875, 'mins': 2.17919921875}
 
(7, 3, 1, 160, 240, 80)
Params #:  59760
MACs:  2928240
 
[17:52:39] (INFO) Building library...
[17:52:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3451271750710228, 'median': 1.3302001953125, 'mins': 1.2694091796875}
 
(7, 3, 1, 160, 240, 160)
Params #:  78960
MACs:  3869040
 
[17:52:56] (INFO) Building library...
[17:52:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2934770063920453, 'median': 2.2774658203125, 'mins': 2.183837890625}
 
(7, 3, 1, 160, 256, 80)
Params #:  63744
MACs:  3123456
 
[17:53:14] (INFO) Building library...
[17:53:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6302734375, 'median': 1.6165771484375, 'mins': 1.5501708984375}
 
(7, 3, 1, 160, 256, 160)
Params #:  84224
MACs:  4126976
 
[17:53:31] (INFO) Building library...
[17:53:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3992953213778407, 'median': 2.36962890625, 'mins': 2.26708984375}
 
(7, 3, 1, 160, 272, 80)
Params #:  67728
MACs:  3318672
 
[17:53:50] (INFO) Building library...
[17:53:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5657526189630682, 'median': 1.5499267578125, 'mins': 1.475830078125}
 
(7, 3, 1, 160, 272, 160)
Params #:  89488
MACs:  4384912
 
[17:54:07] (INFO) Building library...
[17:54:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.366036709872159, 'median': 2.336181640625, 'mins': 2.226318359375}
 
(7, 3, 1, 160, 288, 80)
Params #:  71712
MACs:  3513888
 
[17:54:26] (INFO) Building library...
[17:54:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.549248712713068, 'median': 1.447509765625, 'mins': 1.3302001953125}
 
(7, 3, 1, 160, 288, 160)
Params #:  94752
MACs:  4642848
 
[17:54:43] (INFO) Building library...
[17:54:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3089499733664773, 'median': 2.29736328125, 'mins': 2.226806640625}
 
(7, 3, 1, 160, 304, 80)
Params #:  75696
MACs:  3709104
 
[17:55:03] (INFO) Building library...
[17:55:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5605635209517046, 'median': 1.5447998046875, 'mins': 1.4632568359375}
 
(7, 3, 1, 160, 304, 160)
Params #:  100016
MACs:  4900784
 
[17:55:20] (INFO) Building library...
[17:55:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.219957386363636, 'median': 2.2183837890625, 'mins': 2.1376953125}
 
(7, 3, 1, 160, 320, 80)
Params #:  79680
MACs:  3904320
 
[17:55:38] (INFO) Building library...
[17:55:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7031605113636363, 'median': 1.6871337890625, 'mins': 1.6102294921875}
 
(7, 3, 1, 160, 320, 160)
Params #:  105280
MACs:  5158720
 
[17:55:56] (INFO) Building library...
[17:55:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.49342041015625, 'median': 2.462890625, 'mins': 2.3541259765625}
 
(7, 3, 1, 160, 336, 80)
Params #:  83664
MACs:  4099536
 
[17:56:15] (INFO) Building library...
[17:56:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5703313654119317, 'median': 1.550048828125, 'mins': 1.4627685546875}
 
(7, 3, 1, 160, 336, 160)
Params #:  110544
MACs:  5416656
 
[17:56:32] (INFO) Building library...
[17:56:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3495316938920454, 'median': 2.32421875, 'mins': 2.24267578125}
 
(7, 3, 1, 160, 352, 80)
Params #:  87648
MACs:  4294752
 
[17:56:51] (INFO) Building library...
[17:56:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7007956764914773, 'median': 1.6878662109375, 'mins': 1.5999755859375}
 
(7, 3, 1, 160, 352, 160)
Params #:  115808
MACs:  5674592
 
[17:57:08] (INFO) Building library...
[17:57:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.360765491832386, 'median': 2.32470703125, 'mins': 2.1875}
 
(7, 3, 1, 160, 368, 80)
Params #:  91632
MACs:  4489968
 
[17:57:27] (INFO) Building library...
[17:57:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.73310546875, 'median': 1.7064208984375, 'mins': 1.6207275390625}
 
(7, 3, 1, 160, 368, 160)
Params #:  121072
MACs:  5932528
 
[17:57:44] (INFO) Building library...
[17:57:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4515025745738637, 'median': 2.4307861328125, 'mins': 2.334228515625}
 
(7, 3, 1, 160, 384, 80)
Params #:  95616
MACs:  4685184
 
[17:58:04] (INFO) Building library...
[17:58:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7243696732954545, 'median': 1.6976318359375, 'mins': 1.6092529296875}
 
(7, 3, 1, 160, 384, 160)
Params #:  126336
MACs:  6190464
 
[17:58:21] (INFO) Building library...
[17:58:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.359291770241477, 'median': 2.328857421875, 'mins': 2.2353515625}
 
(7, 3, 1, 160, 400, 80)
Params #:  99600
MACs:  4880400
 
[17:58:40] (INFO) Building library...
[17:58:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6595592151988636, 'median': 1.6553955078125, 'mins': 1.4517822265625}
 
(7, 3, 1, 160, 400, 160)
Params #:  131600
MACs:  6448400
 
[17:58:57] (INFO) Building library...
[17:58:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4974254261363638, 'median': 2.455078125, 'mins': 2.369873046875}
 
(7, 3, 1, 160, 416, 80)
Params #:  103584
MACs:  5075616
 
[17:59:16] (INFO) Building library...
[17:59:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5093916459517045, 'median': 1.4801025390625, 'mins': 1.4139404296875}
 
(7, 3, 1, 160, 416, 160)
Params #:  136864
MACs:  6706336
 
[17:59:33] (INFO) Building library...
[17:59:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.465223277698864, 'median': 2.4530029296875, 'mins': 2.346923828125}
 
(7, 3, 1, 160, 432, 80)
Params #:  107568
MACs:  5270832
 
[17:59:52] (INFO) Building library...
[17:59:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.477859774502841, 'median': 1.469482421875, 'mins': 1.40576171875}
 
(7, 3, 1, 160, 432, 160)
Params #:  142128
MACs:  6964272
 
[18:00:10] (INFO) Building library...
[18:00:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4336015181107955, 'median': 2.4052734375, 'mins': 2.3009033203125}
 
(7, 3, 1, 160, 448, 80)
Params #:  111552
MACs:  5466048
 
[18:00:29] (INFO) Building library...
[18:00:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5025934392755682, 'median': 1.4901123046875, 'mins': 1.413330078125}
 
(7, 3, 1, 160, 448, 160)
Params #:  147392
MACs:  7222208
 
[18:00:46] (INFO) Building library...
[18:00:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5218949751420454, 'median': 2.460693359375, 'mins': 2.3670654296875}
 
(7, 3, 1, 160, 464, 80)
Params #:  115536
MACs:  5661264
 
[18:01:05] (INFO) Building library...
[18:01:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.678280362215909, 'median': 1.66748046875, 'mins': 1.5933837890625}
 
(7, 3, 1, 160, 464, 160)
Params #:  152656
MACs:  7480144
 
[18:01:22] (INFO) Building library...
[18:01:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4926336115056817, 'median': 2.4866943359375, 'mins': 2.3983154296875}
 
(7, 3, 1, 160, 480, 80)
Params #:  119520
MACs:  5856480
 
[18:01:42] (INFO) Building library...
[18:01:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7273148970170455, 'median': 1.6734619140625, 'mins': 1.5740966796875}
 
(7, 3, 1, 160, 480, 160)
Params #:  157920
MACs:  7738080
 
[18:01:59] (INFO) Building library...
[18:01:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0002430308948864, 'median': 1.99462890625, 'mins': 1.902099609375}
 
(7, 3, 1, 160, 496, 80)
Params #:  123504
MACs:  6051696
 
[18:02:18] (INFO) Building library...
[18:02:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7228471235795455, 'median': 1.703369140625, 'mins': 1.5843505859375}
 
(7, 3, 1, 160, 496, 160)
Params #:  163184
MACs:  7996016
 
[18:02:35] (INFO) Building library...
[18:02:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4832996715198865, 'median': 2.438232421875, 'mins': 2.34716796875}
 
(7, 3, 1, 160, 512, 80)
Params #:  127488
MACs:  6246912
 
[18:02:54] (INFO) Building library...
[18:02:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7721957120028409, 'median': 1.7412109375, 'mins': 1.6656494140625}
 
(7, 3, 1, 160, 512, 160)
Params #:  168448
MACs:  8253952
 
[18:03:11] (INFO) Building library...
[18:03:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4598532936789774, 'median': 2.44873046875, 'mins': 2.3697509765625}
 
(7, 3, 1, 160, 528, 80)
Params #:  131472
MACs:  6442128
 
[18:03:30] (INFO) Building library...
[18:03:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9864280007102273, 'median': 1.9693603515625, 'mins': 1.7645263671875}
 
(7, 3, 1, 160, 528, 160)
Params #:  173712
MACs:  8511888
 
[18:03:47] (INFO) Building library...
[18:03:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.285223943536932, 'median': 2.2518310546875, 'mins': 2.1568603515625}
 
(7, 3, 1, 160, 544, 80)
Params #:  135456
MACs:  6637344
 
[18:04:06] (INFO) Building library...
[18:04:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6453957297585227, 'median': 1.636962890625, 'mins': 1.5743408203125}
 
(7, 3, 1, 160, 544, 160)
Params #:  178976
MACs:  8769824
 
[18:04:24] (INFO) Building library...
[18:04:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2566261985085228, 'median': 2.245849609375, 'mins': 2.15478515625}
 
(7, 3, 1, 160, 560, 80)
Params #:  139440
MACs:  6832560
 
[18:04:43] (INFO) Building library...
[18:04:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7018443714488636, 'median': 1.67529296875, 'mins': 1.5919189453125}
 
(7, 3, 1, 160, 560, 160)
Params #:  184240
MACs:  9027760
 
[18:05:00] (INFO) Building library...
[18:05:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.364166814630682, 'median': 2.3192138671875, 'mins': 2.22119140625}
 
(7, 3, 1, 160, 576, 80)
Params #:  143424
MACs:  7027776
 
[18:05:19] (INFO) Building library...
[18:05:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4787764115767046, 'median': 1.47021484375, 'mins': 1.4102783203125}
 
(7, 3, 1, 160, 576, 160)
Params #:  189504
MACs:  9285696
 
[18:05:37] (INFO) Building library...
[18:05:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0147516424005683, 'median': 1.99267578125, 'mins': 1.92529296875}
 
(7, 3, 1, 160, 592, 80)
Params #:  147408
MACs:  7222992
 
[18:05:56] (INFO) Building library...
[18:05:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8498235529119318, 'median': 1.833251953125, 'mins': 1.736328125}
 
(7, 3, 1, 160, 592, 160)
Params #:  194768
MACs:  9543632
 
[18:06:13] (INFO) Building library...
[18:06:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.581564053622159, 'median': 2.4735107421875, 'mins': 2.354736328125}
 
(7, 3, 1, 160, 608, 80)
Params #:  151392
MACs:  7418208
 
[18:06:32] (INFO) Building library...
[18:06:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9473055752840909, 'median': 1.8128662109375, 'mins': 1.6964111328125}
 
(7, 3, 1, 160, 608, 160)
Params #:  200032
MACs:  9801568
 
[18:06:50] (INFO) Building library...
[18:06:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.397528631036932, 'median': 2.3792724609375, 'mins': 2.2852783203125}
 
(7, 3, 1, 160, 624, 80)
Params #:  155376
MACs:  7613424
 
[18:07:09] (INFO) Building library...
[18:07:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.714789373224432, 'median': 1.6820068359375, 'mins': 1.5904541015625}
 
(7, 3, 1, 160, 624, 160)
Params #:  205296
MACs:  10059504
 
[18:07:26] (INFO) Building library...
[18:07:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2583729137073862, 'median': 2.229736328125, 'mins': 2.141357421875}
 
(7, 3, 1, 160, 640, 80)
Params #:  159360
MACs:  7808640
 
[18:07:45] (INFO) Building library...
[18:07:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.721673029119318, 'median': 1.6981201171875, 'mins': 1.6153564453125}
 
(7, 3, 1, 160, 640, 160)
Params #:  210560
MACs:  10317440
 
[18:08:02] (INFO) Building library...
[18:08:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4184736772017046, 'median': 2.3927001953125, 'mins': 2.273681640625}
 
(7, 3, 1, 160, 656, 80)
Params #:  163344
MACs:  8003856
 
[18:08:22] (INFO) Building library...
[18:08:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8335105202414772, 'median': 1.72119140625, 'mins': 1.621337890625}
 
(7, 3, 1, 160, 656, 160)
Params #:  215824
MACs:  10575376
 
[18:08:39] (INFO) Building library...
[18:08:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.490640536221591, 'median': 2.4593505859375, 'mins': 2.352783203125}
 
(7, 3, 1, 160, 672, 80)
Params #:  167328
MACs:  8199072
 
[18:08:58] (INFO) Building library...
[18:08:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8311567826704545, 'median': 1.8017578125, 'mins': 1.7100830078125}
 
(7, 3, 1, 160, 672, 160)
Params #:  221088
MACs:  10833312
 
[18:09:16] (INFO) Building library...
[18:09:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3220114968039773, 'median': 2.3116455078125, 'mins': 2.232666015625}
 
(7, 3, 1, 160, 688, 80)
Params #:  171312
MACs:  8394288
 
[18:09:35] (INFO) Building library...
[18:09:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5117775656960226, 'median': 1.4915771484375, 'mins': 1.4088134765625}
 
(7, 3, 1, 160, 688, 160)
Params #:  226352
MACs:  11091248
 
[18:09:52] (INFO) Building library...
[18:09:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4608686967329545, 'median': 2.4246826171875, 'mins': 2.3173828125}
 
(7, 3, 1, 160, 704, 80)
Params #:  175296
MACs:  8589504
 
[18:10:11] (INFO) Building library...
[18:10:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7673417524857955, 'median': 1.7200927734375, 'mins': 1.6346435546875}
 
(7, 3, 1, 160, 704, 160)
Params #:  231616
MACs:  11349184
 
[18:10:29] (INFO) Building library...
[18:10:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4451216264204545, 'median': 2.440673828125, 'mins': 2.347900390625}
 
(7, 3, 1, 160, 720, 80)
Params #:  179280
MACs:  8784720
 
[18:10:48] (INFO) Building library...
[18:10:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8218439275568181, 'median': 1.7994384765625, 'mins': 1.6932373046875}
 
(7, 3, 1, 160, 720, 160)
Params #:  236880
MACs:  11607120
 
[18:11:05] (INFO) Building library...
[18:11:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3425581498579544, 'median': 2.333740234375, 'mins': 2.196044921875}
 
(7, 3, 1, 160, 736, 80)
Params #:  183264
MACs:  8979936
 
[18:11:24] (INFO) Building library...
[18:11:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6821799538352273, 'median': 1.6529541015625, 'mins': 1.586181640625}
 
(7, 3, 1, 160, 736, 160)
Params #:  242144
MACs:  11865056
 
[18:11:41] (INFO) Building library...
[18:11:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5223577325994317, 'median': 2.484130859375, 'mins': 2.3822021484375}
 
(7, 3, 1, 160, 752, 80)
Params #:  187248
MACs:  9175152
 
[18:12:01] (INFO) Building library...
[18:12:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6630459872159091, 'median': 1.6431884765625, 'mins': 1.564208984375}
 
(7, 3, 1, 160, 752, 160)
Params #:  247408
MACs:  12122992
 
[18:12:18] (INFO) Building library...
[18:12:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2507357510653407, 'median': 2.2225341796875, 'mins': 2.1373291015625}
 
(7, 3, 1, 160, 768, 80)
Params #:  191232
MACs:  9370368
 
[18:12:37] (INFO) Building library...
[18:12:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6620150479403408, 'median': 1.654541015625, 'mins': 1.5406494140625}
 
(7, 3, 1, 160, 768, 160)
Params #:  252672
MACs:  12380928
 
[18:12:55] (INFO) Building library...
[18:12:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.295132723721591, 'median': 2.26806640625, 'mins': 2.180419921875}
 
(7, 3, 1, 160, 784, 80)
Params #:  195216
MACs:  9565584
 
[18:13:14] (INFO) Building library...
[18:13:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8067671342329545, 'median': 1.789794921875, 'mins': 1.6953125}
 
(7, 3, 1, 160, 784, 160)
Params #:  257936
MACs:  12638864
 
[18:13:32] (INFO) Building library...
[18:13:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2201737837357953, 'median': 2.18896484375, 'mins': 2.089111328125}
 
(7, 3, 1, 160, 800, 80)
Params #:  199200
MACs:  9760800
 
[18:13:51] (INFO) Building library...
[18:13:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8229547674005682, 'median': 1.8121337890625, 'mins': 1.738037109375}
 
(7, 3, 1, 160, 800, 160)
Params #:  263200
MACs:  12896800
 
[18:14:08] (INFO) Building library...
[18:14:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1974232066761363, 'median': 2.156005859375, 'mins': 2.0733642578125}
 
(7, 3, 1, 160, 816, 80)
Params #:  203184
MACs:  9956016
 
[18:14:27] (INFO) Building library...
[18:14:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8317094282670454, 'median': 1.80517578125, 'mins': 1.7198486328125}
 
(7, 3, 1, 160, 816, 160)
Params #:  268464
MACs:  13154736
 
[18:14:44] (INFO) Building library...
[18:14:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.395209295099432, 'median': 2.36865234375, 'mins': 2.243896484375}
 
(7, 3, 1, 160, 832, 80)
Params #:  207168
MACs:  10151232
 
[18:15:04] (INFO) Building library...
[18:15:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6810813210227273, 'median': 1.6541748046875, 'mins': 1.5950927734375}
 
(7, 3, 1, 160, 832, 160)
Params #:  273728
MACs:  13412672
 
[18:15:21] (INFO) Building library...
[18:15:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.636888538707386, 'median': 2.61865234375, 'mins': 2.5445556640625}
 
(7, 3, 1, 160, 848, 80)
Params #:  211152
MACs:  10346448
 
[18:15:40] (INFO) Building library...
[18:15:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.797675115411932, 'median': 1.7822265625, 'mins': 1.6968994140625}
 
(7, 3, 1, 160, 848, 160)
Params #:  278992
MACs:  13670608
 
[18:15:57] (INFO) Building library...
[18:15:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4246115944602273, 'median': 2.4041748046875, 'mins': 2.3046875}
 
(7, 3, 1, 160, 864, 80)
Params #:  215136
MACs:  10541664
 
[18:16:16] (INFO) Building library...
[18:16:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7421164772727273, 'median': 1.7169189453125, 'mins': 1.6239013671875}
 
(7, 3, 1, 160, 864, 160)
Params #:  284256
MACs:  13928544
 
[18:16:34] (INFO) Building library...
[18:16:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.288413307883523, 'median': 2.2567138671875, 'mins': 2.1484375}
 
(7, 3, 1, 160, 880, 80)
Params #:  219120
MACs:  10736880
 
[18:16:53] (INFO) Building library...
[18:16:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6553644353693182, 'median': 1.648681640625, 'mins': 1.5797119140625}
 
(7, 3, 1, 160, 880, 160)
Params #:  289520
MACs:  14186480
 
[18:17:10] (INFO) Building library...
[18:17:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5543623490767047, 'median': 2.51708984375, 'mins': 2.430908203125}
 
(7, 3, 1, 160, 896, 80)
Params #:  223104
MACs:  10932096
 
[18:17:30] (INFO) Building library...
[18:17:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.686507901278409, 'median': 1.6597900390625, 'mins': 1.5809326171875}
 
(7, 3, 1, 160, 896, 160)
Params #:  294784
MACs:  14444416
 
[18:17:47] (INFO) Building library...
[18:17:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.561556729403409, 'median': 2.5257568359375, 'mins': 2.44873046875}
 
(7, 3, 1, 160, 912, 80)
Params #:  227088
MACs:  11127312
 
[18:18:06] (INFO) Building library...
[18:18:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7899636008522728, 'median': 1.781494140625, 'mins': 1.69384765625}
 
(7, 3, 1, 160, 912, 160)
Params #:  300048
MACs:  14702352
 
[18:18:23] (INFO) Building library...
[18:18:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1696832830255683, 'median': 2.1573486328125, 'mins': 2.0765380859375}
 
(7, 3, 1, 160, 928, 80)
Params #:  231072
MACs:  11322528
 
[18:18:42] (INFO) Building library...
[18:18:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9514237837357955, 'median': 1.9522705078125, 'mins': 1.7496337890625}
 
(7, 3, 1, 160, 928, 160)
Params #:  305312
MACs:  14960288
 
[18:19:00] (INFO) Building library...
[18:19:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2707197709517044, 'median': 2.228271484375, 'mins': 2.08935546875}
 
(7, 3, 1, 160, 944, 80)
Params #:  235056
MACs:  11517744
 
[18:19:19] (INFO) Building library...
[18:19:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8046985973011365, 'median': 1.7979736328125, 'mins': 1.722412109375}
 
(7, 3, 1, 160, 944, 160)
Params #:  310576
MACs:  15218224
 
[18:19:36] (INFO) Building library...
[18:19:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3991732510653407, 'median': 2.37841796875, 'mins': 2.298095703125}
 
(7, 3, 1, 160, 960, 80)
Params #:  239040
MACs:  11712960
 
[18:19:56] (INFO) Building library...
[18:19:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5295621004971591, 'median': 1.5003662109375, 'mins': 1.41357421875}
 
(7, 3, 1, 160, 960, 160)
Params #:  315840
MACs:  15476160
 
[18:20:13] (INFO) Building library...
[18:20:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7940773703835227, 'median': 1.7769775390625, 'mins': 1.7113037109375}
 
(7, 3, 1, 160, 976, 80)
Params #:  243024
MACs:  11908176
 
[18:20:32] (INFO) Building library...
[18:20:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8582597212357954, 'median': 1.8411865234375, 'mins': 1.73291015625}
 
(7, 3, 1, 160, 976, 160)
Params #:  321104
MACs:  15734096
 
[18:20:50] (INFO) Building library...
[18:20:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.404462224786932, 'median': 2.3775634765625, 'mins': 2.291015625}
 
(7, 3, 1, 160, 992, 80)
Params #:  247008
MACs:  12103392
 
[18:21:09] (INFO) Building library...
[18:21:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8369062943892045, 'median': 1.8125, 'mins': 1.7122802734375}
 
(7, 3, 1, 160, 992, 160)
Params #:  326368
MACs:  15992032
 
[18:21:27] (INFO) Building library...
[18:21:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.544060724431818, 'median': 2.51220703125, 'mins': 2.4031982421875}
 
(7, 3, 1, 160, 1008, 80)
Params #:  250992
MACs:  12298608
 
[18:21:46] (INFO) Building library...
[18:21:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.643195134943182, 'median': 1.634033203125, 'mins': 1.567626953125}
 
(7, 3, 1, 160, 1008, 160)
Params #:  331632
MACs:  16249968
 
[18:22:03] (INFO) Building library...
[18:22:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5703369140625, 'median': 2.5321044921875, 'mins': 2.432373046875}
 
(7, 3, 1, 160, 1024, 80)
Params #:  254976
MACs:  12493824
 
[18:22:23] (INFO) Building library...
[18:22:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6924138849431818, 'median': 1.6676025390625, 'mins': 1.5894775390625}
 
(7, 3, 1, 160, 1024, 160)
Params #:  336896
MACs:  16507904
 
[18:22:40] (INFO) Building library...
[18:22:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.9025989879261362, 'median': 2.82958984375, 'mins': 2.7430419921875}
 
(7, 3, 1, 160, 1040, 80)
Params #:  258960
MACs:  12689040
 
[18:22:59] (INFO) Building library...
[18:22:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.724429598721591, 'median': 1.6988525390625, 'mins': 1.59423828125}
 
(7, 3, 1, 160, 1040, 160)
Params #:  342160
MACs:  16765840
 
[18:23:17] (INFO) Building library...
[18:23:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4641257546164774, 'median': 2.426513671875, 'mins': 2.3328857421875}
 
(7, 3, 1, 160, 1056, 80)
Params #:  262944
MACs:  12884256
 
[18:23:36] (INFO) Building library...
[18:23:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.664599609375, 'median': 1.65673828125, 'mins': 1.593505859375}
 
(7, 3, 1, 160, 1056, 160)
Params #:  347424
MACs:  17023776
 
[18:23:54] (INFO) Building library...
[18:23:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6510975230823863, 'median': 2.606201171875, 'mins': 2.508544921875}
 
(7, 3, 1, 160, 1072, 80)
Params #:  266928
MACs:  13079472
 
[18:24:13] (INFO) Building library...
[18:24:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.837212579900568, 'median': 1.816650390625, 'mins': 1.71142578125}
 
(7, 3, 1, 160, 1072, 160)
Params #:  352688
MACs:  17281712
 
[18:24:31] (INFO) Building library...
[18:24:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.427094060724432, 'median': 2.39208984375, 'mins': 2.30224609375}
 
(7, 3, 1, 160, 1088, 80)
Params #:  270912
MACs:  13274688
 
[18:24:50] (INFO) Building library...
[18:24:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8097223455255682, 'median': 1.7919921875, 'mins': 1.7122802734375}
 
(7, 3, 1, 160, 1088, 160)
Params #:  357952
MACs:  17539648
 
[18:25:07] (INFO) Building library...
[18:25:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2744173916903407, 'median': 2.250732421875, 'mins': 2.148193359375}
 
(7, 3, 1, 160, 1104, 80)
Params #:  274896
MACs:  13469904
 
[18:25:27] (INFO) Building library...
[18:25:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8424727006392045, 'median': 1.818603515625, 'mins': 1.7242431640625}
 
(7, 3, 1, 160, 1104, 160)
Params #:  363216
MACs:  17797584
 
[18:25:44] (INFO) Building library...
[18:25:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5637939453125, 'median': 2.5277099609375, 'mins': 2.4378662109375}
 
(7, 3, 1, 160, 1120, 80)
Params #:  278880
MACs:  13665120
 
[18:26:03] (INFO) Building library...
[18:26:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.81734619140625, 'median': 1.80224609375, 'mins': 1.69921875}
 
(7, 3, 1, 160, 1120, 160)
Params #:  368480
MACs:  18055520
 
[18:26:21] (INFO) Building library...
[18:26:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6484264026988638, 'median': 2.5537109375, 'mins': 2.4385986328125}
 
(7, 3, 1, 160, 1136, 80)
Params #:  282864
MACs:  13860336
 
[18:26:40] (INFO) Building library...
[18:26:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1811867453835228, 'median': 2.052734375, 'mins': 1.9447021484375}
 
(7, 3, 1, 160, 1136, 160)
Params #:  373744
MACs:  18313456
 
[18:26:58] (INFO) Building library...
[18:26:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.420941716974432, 'median': 2.3907470703125, 'mins': 2.2882080078125}
 
(7, 3, 1, 160, 1152, 80)
Params #:  286848
MACs:  14055552
 
[18:27:17] (INFO) Building library...
[18:27:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.846133700284091, 'median': 1.802001953125, 'mins': 1.706298828125}
 
(7, 3, 1, 160, 1152, 160)
Params #:  379008
MACs:  18571392
 
[18:27:34] (INFO) Building library...
[18:27:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5500388405539773, 'median': 2.451904296875, 'mins': 2.3306884765625}
 
(7, 3, 1, 160, 1168, 80)
Params #:  290832
MACs:  14250768
 
[18:27:54] (INFO) Building library...
[18:27:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.815384188565341, 'median': 1.7794189453125, 'mins': 1.685791015625}
 
(7, 3, 1, 160, 1168, 160)
Params #:  384272
MACs:  18829328
 
[18:28:11] (INFO) Building library...
[18:28:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4583451704545456, 'median': 2.3719482421875, 'mins': 2.2677001953125}
 
(7, 3, 1, 160, 1184, 80)
Params #:  294816
MACs:  14445984
 
[18:28:30] (INFO) Building library...
[18:28:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8369706587357955, 'median': 1.8179931640625, 'mins': 1.7169189453125}
 
(7, 3, 1, 160, 1184, 160)
Params #:  389536
MACs:  19087264
 
[18:28:48] (INFO) Building library...
[18:28:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5271439985795454, 'median': 2.4833984375, 'mins': 2.397216796875}
 
(7, 3, 1, 160, 1200, 80)
Params #:  298800
MACs:  14641200
 
[18:29:07] (INFO) Building library...
[18:29:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7564164595170455, 'median': 1.737060546875, 'mins': 1.6658935546875}
 
(7, 3, 1, 160, 1200, 160)
Params #:  394800
MACs:  19345200
 
[18:29:24] (INFO) Building library...
[18:29:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4434758966619317, 'median': 2.41455078125, 'mins': 2.311279296875}
 
(7, 3, 1, 160, 1216, 80)
Params #:  302784
MACs:  14836416
 
[18:29:44] (INFO) Building library...
[18:29:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7804454456676135, 'median': 1.759033203125, 'mins': 1.671630859375}
 
(7, 3, 1, 160, 1216, 160)
Params #:  400064
MACs:  19603136
 
[18:30:01] (INFO) Building library...
[18:30:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5488636363636363, 'median': 2.510498046875, 'mins': 2.4315185546875}
 
(7, 3, 1, 160, 1232, 80)
Params #:  306768
MACs:  15031632
 
[18:30:21] (INFO) Building library...
[18:30:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9441151012073863, 'median': 1.896728515625, 'mins': 1.7918701171875}
 
(7, 3, 1, 160, 1232, 160)
Params #:  405328
MACs:  19861072
 
[18:30:38] (INFO) Building library...
[18:30:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.386932927911932, 'median': 2.3536376953125, 'mins': 2.246826171875}
 
(7, 3, 1, 160, 1248, 80)
Params #:  310752
MACs:  15226848
 
[18:30:57] (INFO) Building library...
[18:30:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6434592507102272, 'median': 1.6292724609375, 'mins': 1.5596923828125}
 
(7, 3, 1, 160, 1248, 160)
Params #:  410592
MACs:  20119008
 
[18:31:14] (INFO) Building library...
[18:31:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3748368696732953, 'median': 2.371826171875, 'mins': 2.2642822265625}
 
(7, 3, 1, 160, 1264, 80)
Params #:  314736
MACs:  15422064
 
[18:31:34] (INFO) Building library...
[18:31:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8459805575284092, 'median': 1.8275146484375, 'mins': 1.7255859375}
 
(7, 3, 1, 160, 1264, 160)
Params #:  415856
MACs:  20376944
 
[18:31:51] (INFO) Building library...
[18:31:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4443892045454545, 'median': 2.3994140625, 'mins': 2.2938232421875}
 
(7, 3, 1, 160, 1280, 80)
Params #:  318720
MACs:  15617280
 
[18:32:10] (INFO) Building library...
[18:32:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.53187255859375, 'median': 1.5147705078125, 'mins': 1.437744140625}
 
(7, 3, 1, 160, 1280, 160)
Params #:  421120
MACs:  20634880
 
[18:32:28] (INFO) Building library...
[18:32:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.233566006747159, 'median': 2.193603515625, 'mins': 2.0791015625}
 
(7, 3, 1, 160, 1296, 80)
Params #:  322704
MACs:  15812496
 
[18:32:48] (INFO) Building library...
[18:32:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7994817560369318, 'median': 1.76904296875, 'mins': 1.69140625}
 
(7, 3, 1, 160, 1296, 160)
Params #:  426384
MACs:  20892816
 
[18:33:06] (INFO) Building library...
[18:33:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.492958762428977, 'median': 2.4615478515625, 'mins': 2.367919921875}
 
(7, 3, 1, 160, 1312, 80)
Params #:  326688
MACs:  16007712
 
[18:33:25] (INFO) Building library...
[18:33:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8369551225142045, 'median': 1.79931640625, 'mins': 1.7305908203125}
 
(7, 3, 1, 160, 1312, 160)
Params #:  431648
MACs:  21150752
 
[18:33:43] (INFO) Building library...
[18:33:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.510674493963068, 'median': 2.48828125, 'mins': 2.3994140625}
 
(7, 3, 1, 160, 1328, 80)
Params #:  330672
MACs:  16202928
 
[18:34:02] (INFO) Building library...
[18:34:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.699752530184659, 'median': 1.664794921875, 'mins': 1.5860595703125}
 
(7, 3, 1, 160, 1328, 160)
Params #:  436912
MACs:  21408688
 
[18:34:19] (INFO) Building library...
[18:34:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.632407448508523, 'median': 2.606689453125, 'mins': 2.5281982421875}
 
(7, 3, 1, 160, 1344, 80)
Params #:  334656
MACs:  16398144
 
[18:34:39] (INFO) Building library...
[18:34:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8000710227272727, 'median': 1.787109375, 'mins': 1.689453125}
 
(7, 3, 1, 160, 1344, 160)
Params #:  442176
MACs:  21666624
 
[18:34:56] (INFO) Building library...
[18:34:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.439590731534091, 'median': 2.39794921875, 'mins': 2.31591796875}
 
(7, 3, 1, 160, 1360, 80)
Params #:  338640
MACs:  16593360
 
[18:35:16] (INFO) Building library...
[18:35:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7755459872159092, 'median': 1.755615234375, 'mins': 1.680908203125}
 
(7, 3, 1, 160, 1360, 160)
Params #:  447440
MACs:  21924560
 
[18:35:33] (INFO) Building library...
[18:35:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5694435813210226, 'median': 2.5374755859375, 'mins': 2.4383544921875}
 
(7, 3, 1, 160, 1376, 80)
Params #:  342624
MACs:  16788576
 
[18:35:52] (INFO) Building library...
[18:35:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7452647816051137, 'median': 1.7091064453125, 'mins': 1.6241455078125}
 
(7, 3, 1, 160, 1376, 160)
Params #:  452704
MACs:  22182496
 
[18:36:10] (INFO) Building library...
[18:36:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.361538973721591, 'median': 2.3541259765625, 'mins': 2.2713623046875}
 
(7, 3, 1, 160, 1392, 80)
Params #:  346608
MACs:  16983792
 
[18:36:29] (INFO) Building library...
[18:36:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6939719460227274, 'median': 1.6748046875, 'mins': 1.587646484375}
 
(7, 3, 1, 160, 1392, 160)
Params #:  457968
MACs:  22440432
 
[18:36:46] (INFO) Building library...
[18:36:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.538186922940341, 'median': 2.5032958984375, 'mins': 2.3843994140625}
 
(7, 3, 1, 160, 1408, 80)
Params #:  350592
MACs:  17179008
 
[18:37:06] (INFO) Building library...
[18:37:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7961314808238635, 'median': 1.7882080078125, 'mins': 1.7113037109375}
 
(7, 3, 1, 160, 1408, 160)
Params #:  463232
MACs:  22698368
 
[18:37:23] (INFO) Building library...
[18:37:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.489007013494318, 'median': 2.4517822265625, 'mins': 2.354248046875}
 
(7, 3, 1, 160, 1424, 80)
Params #:  354576
MACs:  17374224
 
[18:37:42] (INFO) Building library...
[18:37:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.769534579190341, 'median': 1.7552490234375, 'mins': 1.6796875}
 
(7, 3, 1, 160, 1424, 160)
Params #:  468496
MACs:  22956304
 
[18:38:00] (INFO) Building library...
[18:38:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3560524680397728, 'median': 2.3416748046875, 'mins': 2.25439453125}
 
(7, 3, 1, 160, 16, 320)
Params #:  7824
MACs:  383376
 
[18:38:19] (INFO) Building library...
[18:38:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4931474165482954, 'median': 1.4718017578125, 'mins': 1.389404296875}
 
(7, 3, 1, 160, 32, 320)
Params #:  15648
MACs:  766752
 
[18:38:36] (INFO) Building library...
[18:38:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6242276278409091, 'median': 1.6043701171875, 'mins': 1.5159912109375}
 
(7, 3, 1, 160, 48, 320)
Params #:  23472
MACs:  1150128
 
[18:38:53] (INFO) Building library...
[18:38:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5590010209517045, 'median': 1.5347900390625, 'mins': 1.4586181640625}
 
(7, 3, 1, 160, 64, 320)
Params #:  31296
MACs:  1533504
 
[18:39:10] (INFO) Building library...
[18:39:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6836414683948864, 'median': 1.6627197265625, 'mins': 1.572265625}
 
(7, 3, 1, 160, 80, 320)
Params #:  39120
MACs:  1916880
 
[18:39:28] (INFO) Building library...
[18:39:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6012184836647727, 'median': 1.5052490234375, 'mins': 1.4290771484375}
 
(7, 3, 1, 160, 96, 320)
Params #:  46944
MACs:  2300256
 
[18:39:45] (INFO) Building library...
[18:39:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.735986328125, 'median': 1.5810546875, 'mins': 1.4461669921875}
 
(7, 3, 1, 160, 112, 320)
Params #:  54768
MACs:  2683632
 
[18:40:03] (INFO) Building library...
[18:40:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.593063077059659, 'median': 1.4761962890625, 'mins': 1.397705078125}
 
(7, 3, 1, 160, 128, 320)
Params #:  62592
MACs:  3067008
 
[18:40:20] (INFO) Building library...
[18:40:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8192138671875, 'median': 1.7945556640625, 'mins': 1.699462890625}
 
(7, 3, 1, 160, 144, 320)
Params #:  70416
MACs:  3450384
 
[18:40:37] (INFO) Building library...
[18:40:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5293046431107955, 'median': 1.5115966796875, 'mins': 1.440185546875}
 
(7, 3, 1, 160, 176, 320)
Params #:  86064
MACs:  4217136
 
[18:40:54] (INFO) Building library...
[18:40:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5392933238636364, 'median': 1.5186767578125, 'mins': 1.46337890625}
 
(7, 3, 1, 160, 192, 320)
Params #:  93888
MACs:  4600512
 
[18:41:12] (INFO) Building library...
[18:41:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6682173295454545, 'median': 1.6494140625, 'mins': 1.5589599609375}
 
(7, 3, 1, 160, 208, 320)
Params #:  101712
MACs:  4983888
 
[18:41:29] (INFO) Building library...
[18:41:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5323264382102273, 'median': 1.5201416015625, 'mins': 1.447509765625}
 
(7, 3, 1, 160, 224, 320)
Params #:  109536
MACs:  5367264
 
[18:41:46] (INFO) Building library...
[18:41:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5084173029119319, 'median': 1.486083984375, 'mins': 1.4228515625}
 
(7, 3, 1, 160, 240, 320)
Params #:  117360
MACs:  5750640
 
[18:42:02] (INFO) Building library...
[18:42:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.663811700994318, 'median': 1.65966796875, 'mins': 1.5902099609375}
 
(7, 3, 1, 160, 256, 320)
Params #:  125184
MACs:  6134016
 
[18:42:19] (INFO) Building library...
[18:42:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7810258345170455, 'median': 1.734130859375, 'mins': 1.6494140625}
 
(7, 3, 1, 160, 272, 320)
Params #:  133008
MACs:  6517392
 
[18:42:36] (INFO) Building library...
[18:42:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.711123934659091, 'median': 1.686279296875, 'mins': 1.6180419921875}
 
(7, 3, 1, 160, 288, 320)
Params #:  140832
MACs:  6900768
 
[18:42:53] (INFO) Building library...
[18:42:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5044222745028408, 'median': 1.4910888671875, 'mins': 1.42626953125}
 
(7, 3, 1, 160, 304, 320)
Params #:  148656
MACs:  7284144
 
[18:43:10] (INFO) Building library...
[18:43:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5515791459517045, 'median': 1.5245361328125, 'mins': 1.44580078125}
 
(7, 3, 1, 160, 320, 320)
Params #:  156480
MACs:  7667520
 
[18:43:27] (INFO) Building library...
[18:43:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6473721590909092, 'median': 1.6234130859375, 'mins': 1.550048828125}
 
(7, 3, 1, 160, 336, 320)
Params #:  164304
MACs:  8050896
 
[18:43:44] (INFO) Building library...
[18:43:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5370017311789772, 'median': 1.5164794921875, 'mins': 1.4420166015625}
 
(7, 3, 1, 160, 352, 320)
Params #:  172128
MACs:  8434272
 
[18:44:01] (INFO) Building library...
[18:44:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6837790749289774, 'median': 1.5323486328125, 'mins': 1.4193115234375}
 
(7, 3, 1, 160, 368, 320)
Params #:  179952
MACs:  8817648
 
[18:44:19] (INFO) Building library...
[18:44:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.486443536931818, 'median': 1.4786376953125, 'mins': 1.4249267578125}
 
(7, 3, 1, 160, 384, 320)
Params #:  187776
MACs:  9201024
 
[18:44:36] (INFO) Building library...
[18:44:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7716508345170454, 'median': 1.763671875, 'mins': 1.6875}
 
(7, 3, 1, 160, 400, 320)
Params #:  195600
MACs:  9584400
 
[18:44:53] (INFO) Building library...
[18:44:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4813243519176136, 'median': 1.467041015625, 'mins': 1.3992919921875}
 
(7, 3, 1, 160, 416, 320)
Params #:  203424
MACs:  9967776
 
[18:45:11] (INFO) Building library...
[18:45:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66773681640625, 'median': 1.6414794921875, 'mins': 1.5556640625}
 
(7, 3, 1, 160, 432, 320)
Params #:  211248
MACs:  10351152
 
[18:45:28] (INFO) Building library...
[18:45:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7363658558238637, 'median': 1.7060546875, 'mins': 1.600830078125}
 
(7, 3, 1, 160, 448, 320)
Params #:  219072
MACs:  10734528
 
[18:45:45] (INFO) Building library...
[18:45:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.672555264559659, 'median': 1.6328125, 'mins': 1.5665283203125}
 
(7, 3, 1, 160, 464, 320)
Params #:  226896
MACs:  11117904
 
[18:46:02] (INFO) Building library...
[18:46:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.638375577059659, 'median': 1.6302490234375, 'mins': 1.5667724609375}
 
(7, 3, 1, 160, 480, 320)
Params #:  234720
MACs:  11501280
 
[18:46:19] (INFO) Building library...
[18:46:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6741111061789773, 'median': 1.6448974609375, 'mins': 1.5751953125}
 
(7, 3, 1, 160, 496, 320)
Params #:  242544
MACs:  11884656
 
[18:46:36] (INFO) Building library...
[18:46:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6856434215198863, 'median': 1.642822265625, 'mins': 1.5662841796875}
 
(7, 3, 1, 160, 512, 320)
Params #:  250368
MACs:  12268032
 
[18:46:53] (INFO) Building library...
[18:46:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4977916370738635, 'median': 1.478759765625, 'mins': 1.412353515625}
 
(7, 3, 1, 160, 528, 320)
Params #:  258192
MACs:  12651408
 
[18:47:10] (INFO) Building library...
[18:47:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.649773615056818, 'median': 1.6224365234375, 'mins': 1.5474853515625}
 
(7, 3, 1, 160, 544, 320)
Params #:  266016
MACs:  13034784
 
[18:47:27] (INFO) Building library...
[18:47:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5045387961647727, 'median': 1.4864501953125, 'mins': 1.408935546875}
 
(7, 3, 1, 160, 560, 320)
Params #:  273840
MACs:  13418160
 
[18:47:44] (INFO) Building library...
[18:47:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6598333185369318, 'median': 1.634521484375, 'mins': 1.5479736328125}
 
(7, 3, 1, 160, 576, 320)
Params #:  281664
MACs:  13801536
 
[18:48:01] (INFO) Building library...
[18:48:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4890214399857955, 'median': 1.4715576171875, 'mins': 1.3917236328125}
 
(7, 3, 1, 160, 592, 320)
Params #:  289488
MACs:  14184912
 
[18:48:18] (INFO) Building library...
[18:48:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4734918767755683, 'median': 1.456298828125, 'mins': 1.3795166015625}
 
(7, 3, 1, 160, 608, 320)
Params #:  297312
MACs:  14568288
 
[18:48:35] (INFO) Building library...
[18:48:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5020607688210228, 'median': 1.4747314453125, 'mins': 1.4085693359375}
 
(7, 3, 1, 160, 624, 320)
Params #:  305136
MACs:  14951664
 
[18:48:52] (INFO) Building library...
[18:48:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7082408558238635, 'median': 1.6748046875, 'mins': 1.6005859375}
 
(7, 3, 1, 160, 640, 320)
Params #:  312960
MACs:  15335040
 
[18:49:09] (INFO) Building library...
[18:49:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.786758700284091, 'median': 1.77734375, 'mins': 1.7061767578125}
 
(7, 3, 1, 160, 656, 320)
Params #:  320784
MACs:  15718416
 
[18:49:26] (INFO) Building library...
[18:49:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7796775124289772, 'median': 1.7567138671875, 'mins': 1.6759033203125}
 
(7, 3, 1, 160, 672, 320)
Params #:  328608
MACs:  16101792
 
[18:49:44] (INFO) Building library...
[18:49:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8529541015625, 'median': 1.827392578125, 'mins': 1.732177734375}
 
(7, 3, 1, 160, 688, 320)
Params #:  336432
MACs:  16485168
 
[18:50:01] (INFO) Building library...
[18:50:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7634776722301135, 'median': 1.76123046875, 'mins': 1.691650390625}
 
(7, 3, 1, 160, 704, 320)
Params #:  344256
MACs:  16868544
 
[18:50:18] (INFO) Building library...
[18:50:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7955854936079545, 'median': 1.7662353515625, 'mins': 1.68505859375}
 
(7, 3, 1, 160, 720, 320)
Params #:  352080
MACs:  17251920
 
[18:50:36] (INFO) Building library...
[18:50:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7143754438920455, 'median': 1.6739501953125, 'mins': 1.58544921875}
 
(7, 3, 1, 160, 736, 320)
Params #:  359904
MACs:  17635296
 
[18:50:53] (INFO) Building library...
[18:50:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6805475408380681, 'median': 1.6466064453125, 'mins': 1.5540771484375}
 
(7, 3, 1, 160, 752, 320)
Params #:  367728
MACs:  18018672
 
[18:51:10] (INFO) Building library...
[18:51:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7960537997159092, 'median': 1.772216796875, 'mins': 1.6922607421875}
 
(7, 3, 1, 160, 768, 320)
Params #:  375552
MACs:  18402048
 
[18:51:27] (INFO) Building library...
[18:51:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7769220525568181, 'median': 1.75537109375, 'mins': 1.66748046875}
 
(7, 3, 1, 160, 784, 320)
Params #:  383376
MACs:  18785424
 
[18:51:45] (INFO) Building library...
[18:51:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.85826416015625, 'median': 1.8289794921875, 'mins': 1.7178955078125}
 
(7, 3, 1, 160, 800, 320)
Params #:  391200
MACs:  19168800
 
[18:52:02] (INFO) Building library...
[18:52:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6711004083806817, 'median': 1.6517333984375, 'mins': 1.5911865234375}
 
(7, 3, 1, 160, 816, 320)
Params #:  399024
MACs:  19552176
 
[18:52:20] (INFO) Building library...
[18:52:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6972179066051136, 'median': 1.678955078125, 'mins': 1.576171875}
 
(7, 3, 1, 160, 832, 320)
Params #:  406848
MACs:  19935552
 
[18:52:37] (INFO) Building library...
[18:52:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6586581143465908, 'median': 1.641357421875, 'mins': 1.576416015625}
 
(7, 3, 1, 160, 848, 320)
Params #:  414672
MACs:  20318928
 
[18:52:54] (INFO) Building library...
[18:52:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6879450017755682, 'median': 1.6612548828125, 'mins': 1.572509765625}
 
(7, 3, 1, 160, 864, 320)
Params #:  422496
MACs:  20702304
 
[18:53:12] (INFO) Building library...
[18:53:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7673728249289773, 'median': 1.6680908203125, 'mins': 1.5628662109375}
 
(7, 3, 1, 160, 880, 320)
Params #:  430320
MACs:  21085680
 
[18:53:30] (INFO) Building library...
[18:53:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7994850852272728, 'median': 1.7752685546875, 'mins': 1.685302734375}
 
(7, 3, 1, 160, 896, 320)
Params #:  438144
MACs:  21469056
 
[18:53:47] (INFO) Building library...
[18:53:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9434470436789772, 'median': 1.8128662109375, 'mins': 1.686767578125}
 
(7, 3, 1, 160, 912, 320)
Params #:  445968
MACs:  21852432
 
[18:54:04] (INFO) Building library...
[18:54:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6432051225142046, 'median': 1.6307373046875, 'mins': 1.5679931640625}
 
(7, 3, 1, 160, 928, 320)
Params #:  453792
MACs:  22235808
 
[18:54:22] (INFO) Building library...
[18:54:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8098477450284092, 'median': 1.792236328125, 'mins': 1.702392578125}
 
(7, 3, 1, 160, 944, 320)
Params #:  461616
MACs:  22619184
 
[18:54:39] (INFO) Building library...
[18:54:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8204700816761363, 'median': 1.781005859375, 'mins': 1.7012939453125}
 
(7, 3, 1, 160, 960, 320)
Params #:  469440
MACs:  23002560
 
[18:54:56] (INFO) Building library...
[18:54:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.1115334250710227, 'median': 1.1014404296875, 'mins': 1.0533447265625}
 
(7, 3, 1, 160, 976, 320)
Params #:  477264
MACs:  23385936
 
[18:55:14] (INFO) Building library...
[18:55:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6523492986505681, 'median': 1.623779296875, 'mins': 1.548828125}
 
(7, 3, 1, 160, 992, 320)
Params #:  485088
MACs:  23769312
 
[18:55:31] (INFO) Building library...
[18:55:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6933094371448865, 'median': 1.6689453125, 'mins': 1.575439453125}
 
(7, 3, 1, 160, 1008, 320)
Params #:  492912
MACs:  24152688
 
[18:55:48] (INFO) Building library...
[18:55:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8215964577414774, 'median': 1.7955322265625, 'mins': 1.71728515625}
 
(7, 3, 1, 160, 1024, 320)
Params #:  500736
MACs:  24536064
 
[18:56:06] (INFO) Building library...
[18:56:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8341042258522726, 'median': 1.81640625, 'mins': 1.738037109375}
 
(7, 3, 1, 160, 1040, 320)
Params #:  508560
MACs:  24919440
 
[18:56:23] (INFO) Building library...
[18:56:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9964066938920455, 'median': 1.8250732421875, 'mins': 1.6910400390625}
 
(7, 3, 1, 160, 1056, 320)
Params #:  516384
MACs:  25302816
 
[18:56:40] (INFO) Building library...
[18:56:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7585682262073863, 'median': 1.7467041015625, 'mins': 1.66259765625}
 
(7, 3, 1, 160, 1072, 320)
Params #:  524208
MACs:  25686192
 
[18:56:58] (INFO) Building library...
[18:56:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7984308416193182, 'median': 1.7783203125, 'mins': 1.68359375}
 
(7, 3, 1, 160, 1088, 320)
Params #:  532032
MACs:  26069568
 
[18:57:15] (INFO) Building library...
[18:57:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8342340642755681, 'median': 1.8004150390625, 'mins': 1.7218017578125}
 
(7, 3, 1, 160, 1104, 320)
Params #:  539856
MACs:  26452944
 
[18:57:33] (INFO) Building library...
[18:57:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6672407670454545, 'median': 1.658935546875, 'mins': 1.5858154296875}
 
(7, 3, 1, 160, 1120, 320)
Params #:  547680
MACs:  26836320
 
[18:57:50] (INFO) Building library...
[18:57:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6827581232244317, 'median': 1.6597900390625, 'mins': 1.5733642578125}
 
(7, 3, 1, 160, 1136, 320)
Params #:  555504
MACs:  27219696
 
[18:58:07] (INFO) Building library...
[18:58:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6633478338068182, 'median': 1.6544189453125, 'mins': 1.58984375}
 
(7, 3, 1, 160, 1152, 320)
Params #:  563328
MACs:  27603072
 
[18:58:25] (INFO) Building library...
[18:58:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8591663707386363, 'median': 1.833740234375, 'mins': 1.7279052734375}
 
(7, 3, 1, 160, 1168, 320)
Params #:  571152
MACs:  27986448
 
[18:58:42] (INFO) Building library...
[18:58:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7812266956676137, 'median': 1.765625, 'mins': 1.6820068359375}
 
(7, 3, 1, 160, 1184, 320)
Params #:  578976
MACs:  28369824
 
[18:59:00] (INFO) Building library...
[18:59:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8005260120738635, 'median': 1.781494140625, 'mins': 1.6890869140625}
 
(7, 3, 1, 160, 1200, 320)
Params #:  586800
MACs:  28753200
 
[18:59:17] (INFO) Building library...
[18:59:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.789779385653409, 'median': 1.75439453125, 'mins': 1.6873779296875}
 
(7, 3, 1, 160, 1216, 320)
Params #:  594624
MACs:  29136576
 
[18:59:34] (INFO) Building library...
[18:59:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.78372802734375, 'median': 1.7705078125, 'mins': 1.6854248046875}
 
(7, 3, 1, 160, 1232, 320)
Params #:  602448
MACs:  29519952
 
[18:59:52] (INFO) Building library...
[18:59:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8398814808238637, 'median': 1.8018798828125, 'mins': 1.728759765625}
 
(7, 3, 1, 160, 1248, 320)
Params #:  610272
MACs:  29903328
 
[19:00:09] (INFO) Building library...
[19:00:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8593583540482954, 'median': 1.8121337890625, 'mins': 1.7076416015625}
 
(7, 3, 1, 160, 1264, 320)
Params #:  618096
MACs:  30286704
 
[19:00:27] (INFO) Building library...
[19:00:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7691517223011364, 'median': 1.741455078125, 'mins': 1.678466796875}
 
(7, 3, 1, 160, 1280, 320)
Params #:  625920
MACs:  30670080
 
[19:00:45] (INFO) Building library...
[19:00:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3678111683238636, 'median': 1.3594970703125, 'mins': 1.3128662109375}
 
(7, 3, 1, 160, 1296, 320)
Params #:  633744
MACs:  31053456
 
[19:01:03] (INFO) Building library...
[19:01:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.778579989346591, 'median': 1.7576904296875, 'mins': 1.6685791015625}
 
(7, 3, 1, 160, 1312, 320)
Params #:  641568
MACs:  31436832
 
[19:01:20] (INFO) Building library...
[19:01:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.823723810369318, 'median': 1.784423828125, 'mins': 1.7042236328125}
 
(7, 3, 1, 160, 1328, 320)
Params #:  649392
MACs:  31820208
 
[19:01:38] (INFO) Building library...
[19:01:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4719127308238635, 'median': 1.463134765625, 'mins': 1.404052734375}
 
(7, 3, 1, 160, 1344, 320)
Params #:  657216
MACs:  32203584
 
[19:01:55] (INFO) Building library...
[19:01:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8897927024147727, 'median': 1.8394775390625, 'mins': 1.7562255859375}
 
(7, 3, 1, 160, 1360, 320)
Params #:  665040
MACs:  32586960
 
[19:02:13] (INFO) Building library...
[19:02:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7787531072443181, 'median': 1.7674560546875, 'mins': 1.6905517578125}
 
(7, 3, 1, 160, 1376, 320)
Params #:  672864
MACs:  32970336
 
[19:02:30] (INFO) Building library...
[19:02:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8012506658380683, 'median': 1.7816162109375, 'mins': 1.703857421875}
 
(7, 3, 1, 160, 1392, 320)
Params #:  680688
MACs:  33353712
 
[19:02:47] (INFO) Building library...
[19:02:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9291814630681818, 'median': 1.846435546875, 'mins': 1.7266845703125}
 
(7, 3, 1, 160, 1408, 320)
Params #:  688512
MACs:  33737088
 
[19:03:05] (INFO) Building library...
[19:03:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7002141779119317, 'median': 1.6783447265625, 'mins': 1.6041259765625}
 
(7, 3, 1, 160, 1424, 320)
Params #:  696336
MACs:  34120464
 
[19:03:22] (INFO) Building library...
[19:03:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7537897283380681, 'median': 1.7445068359375, 'mins': 1.676513671875}
 
(56, 5, 2, 24, 16, 20)
Params #:  1104
MACs:  1768704
 
[19:03:40] (INFO) Building library...
[19:03:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8089189009232955, 'median': 1.79638671875, 'mins': 1.7020263671875}
 
(56, 5, 2, 24, 16, 40)
Params #:  1424
MACs:  2019584
 
[19:03:57] (INFO) Building library...
[19:03:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7389315518465909, 'median': 1.7154541015625, 'mins': 1.62890625}
 
(56, 5, 2, 24, 32, 20)
Params #:  2208
MACs:  3537408
 
[19:04:15] (INFO) Building library...
[19:04:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7619384765625, 'median': 1.7254638671875, 'mins': 1.642822265625}
 
(56, 5, 2, 24, 32, 40)
Params #:  2848
MACs:  4039168
 
[19:04:32] (INFO) Building library...
[19:04:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6850408380681818, 'median': 1.663330078125, 'mins': 1.561767578125}
 
(56, 5, 2, 24, 48, 20)
Params #:  3312
MACs:  5306112
 
[19:04:50] (INFO) Building library...
[19:04:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6331287730823865, 'median': 1.596923828125, 'mins': 1.51708984375}
 
(56, 5, 2, 24, 48, 40)
Params #:  4272
MACs:  6058752
 
[19:05:07] (INFO) Building library...
[19:05:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7493430397727272, 'median': 1.724365234375, 'mins': 1.642333984375}
 
(56, 5, 2, 24, 64, 20)
Params #:  4416
MACs:  7074816
 
[19:05:24] (INFO) Building library...
[19:05:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9327891956676135, 'median': 1.899658203125, 'mins': 1.8148193359375}
 
(56, 5, 2, 24, 64, 40)
Params #:  5696
MACs:  8078336
 
[19:05:42] (INFO) Building library...
[19:05:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8928666548295454, 'median': 1.8697509765625, 'mins': 1.7755126953125}
 
(56, 5, 2, 24, 72, 40)
Params #:  6408
MACs:  9088128
 
[19:05:59] (INFO) Building library...
[19:05:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9926680131392045, 'median': 1.940673828125, 'mins': 1.8343505859375}
 
(56, 5, 2, 24, 80, 20)
Params #:  5520
MACs:  8843520
 
[19:06:17] (INFO) Building library...
[19:06:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8747636274857955, 'median': 1.8472900390625, 'mins': 1.7532958984375}
 
(56, 5, 2, 24, 80, 40)
Params #:  7120
MACs:  10097920
 
[19:06:34] (INFO) Building library...
[19:06:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.89488525390625, 'median': 1.8570556640625, 'mins': 1.7696533203125}
 
(56, 5, 2, 24, 96, 20)
Params #:  6624
MACs:  10612224
 
[19:06:52] (INFO) Building library...
[19:06:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9642345081676136, 'median': 1.9141845703125, 'mins': 1.7886962890625}
 
(56, 5, 2, 24, 96, 40)
Params #:  8544
MACs:  12117504
 
[19:07:10] (INFO) Building library...
[19:07:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8861283735795455, 'median': 1.8602294921875, 'mins': 1.767822265625}
 
(28, 5, 1, 40, 16, 20)
Params #:  1360
MACs:  1066240
 
[19:07:28] (INFO) Building library...
[19:07:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4822465376420455, 'median': 1.46533203125, 'mins': 1.39794921875}
 
(28, 5, 1, 40, 16, 40)
Params #:  1680
MACs:  1317120
 
[19:07:44] (INFO) Building library...
[19:07:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.264829323508523, 'median': 2.22998046875, 'mins': 2.1480712890625}
 
(28, 5, 1, 40, 32, 20)
Params #:  2720
MACs:  2132480
 
[19:08:03] (INFO) Building library...
[19:08:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5014981356534092, 'median': 1.4764404296875, 'mins': 1.40625}
 
(28, 5, 1, 40, 32, 40)
Params #:  3360
MACs:  2634240
 
[19:08:20] (INFO) Building library...
[19:08:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2754106001420453, 'median': 2.2462158203125, 'mins': 2.1439208984375}
 
(28, 5, 1, 40, 48, 20)
Params #:  4080
MACs:  3198720
 
[19:08:40] (INFO) Building library...
[19:08:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7197287819602274, 'median': 1.68798828125, 'mins': 1.5897216796875}
 
(28, 5, 1, 40, 48, 40)
Params #:  5040
MACs:  3951360
 
[19:08:57] (INFO) Building library...
[19:08:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3053733132102274, 'median': 2.2747802734375, 'mins': 2.1802978515625}
 
(28, 5, 1, 40, 64, 20)
Params #:  5440
MACs:  4264960
 
[19:09:16] (INFO) Building library...
[19:09:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7666437322443183, 'median': 1.725341796875, 'mins': 1.6300048828125}
 
(28, 5, 1, 40, 64, 40)
Params #:  6720
MACs:  5268480
 
[19:09:33] (INFO) Building library...
[19:09:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.438797274502841, 'median': 2.4102783203125, 'mins': 2.3116455078125}
 
(28, 5, 1, 40, 80, 20)
Params #:  6800
MACs:  5331200
 
[19:09:52] (INFO) Building library...
[19:09:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7963134765625, 'median': 1.7294921875, 'mins': 1.6444091796875}
 
(28, 5, 1, 40, 80, 40)
Params #:  8400
MACs:  6585600
 
[19:10:09] (INFO) Building library...
[19:10:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.284756747159091, 'median': 2.2789306640625, 'mins': 2.1884765625}
 
(28, 5, 1, 40, 96, 20)
Params #:  8160
MACs:  6397440
 
[19:10:28] (INFO) Building library...
[19:10:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8801125266335228, 'median': 1.708984375, 'mins': 1.604248046875}
 
(28, 5, 1, 40, 96, 40)
Params #:  10080
MACs:  7902720
 
[19:10:46] (INFO) Building library...
[19:10:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.45694580078125, 'median': 2.44580078125, 'mins': 2.3629150390625}
 
(28, 5, 1, 40, 112, 20)
Params #:  9520
MACs:  7463680
 
[19:11:05] (INFO) Building library...
[19:11:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.778976162997159, 'median': 1.7564697265625, 'mins': 1.6507568359375}
 
(28, 5, 1, 40, 112, 40)
Params #:  11760
MACs:  9219840
 
[19:11:23] (INFO) Building library...
[19:11:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4535600142045455, 'median': 2.42431640625, 'mins': 2.3355712890625}
 
(28, 5, 1, 40, 120, 40)
Params #:  12600
MACs:  9878400
 
[19:11:42] (INFO) Building library...
[19:11:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8907115589488637, 'median': 2.8380126953125, 'mins': 2.7470703125}
 
(28, 5, 1, 40, 128, 20)
Params #:  10880
MACs:  8529920
 
[19:12:01] (INFO) Building library...
[19:12:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7716907848011363, 'median': 1.7288818359375, 'mins': 1.654541015625}
 
(28, 5, 1, 40, 128, 40)
Params #:  13440
MACs:  10536960
 
[19:12:18] (INFO) Building library...
[19:12:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4735218394886362, 'median': 2.4642333984375, 'mins': 2.35205078125}
 
(28, 5, 1, 40, 144, 20)
Params #:  12240
MACs:  9596160
 
[19:12:37] (INFO) Building library...
[19:12:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6017078746448863, 'median': 1.58349609375, 'mins': 1.5106201171875}
 
(28, 5, 1, 40, 144, 40)
Params #:  15120
MACs:  11854080
 
[19:12:55] (INFO) Building library...
[19:12:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.593856534090909, 'median': 2.5709228515625, 'mins': 2.422119140625}
 
(28, 5, 1, 40, 160, 20)
Params #:  13600
MACs:  10662400
 
[19:13:14] (INFO) Building library...
[19:13:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7319236061789773, 'median': 1.72021484375, 'mins': 1.639404296875}
 
(28, 5, 1, 40, 160, 40)
Params #:  16800
MACs:  13171200
 
[19:13:31] (INFO) Building library...
[19:13:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.394479092684659, 'median': 2.3636474609375, 'mins': 2.2607421875}
 
(28, 5, 1, 40, 176, 20)
Params #:  14960
MACs:  11728640
 
[19:13:51] (INFO) Building library...
[19:13:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7960671164772728, 'median': 1.7613525390625, 'mins': 1.675048828125}
 
(28, 5, 1, 40, 176, 40)
Params #:  18480
MACs:  14488320
 
[19:14:08] (INFO) Building library...
[19:14:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.295026189630682, 'median': 2.280029296875, 'mins': 2.2020263671875}
 
(28, 5, 2, 40, 16, 40)
Params #:  1680
MACs:  705600
 
[19:14:28] (INFO) Building library...
[19:14:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5482488458806818, 'median': 1.52587890625, 'mins': 1.455078125}
 
(28, 5, 2, 40, 16, 80)
Params #:  2320
MACs:  831040
 
[19:14:45] (INFO) Building library...
[19:14:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.537118252840909, 'median': 1.5126953125, 'mins': 1.4476318359375}
 
(28, 5, 2, 40, 32, 40)
Params #:  3360
MACs:  1411200
 
[19:15:02] (INFO) Building library...
[19:15:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7795221502130683, 'median': 1.73583984375, 'mins': 1.647216796875}
 
(28, 5, 2, 40, 32, 80)
Params #:  4640
MACs:  1662080
 
[19:15:20] (INFO) Building library...
[19:15:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5457020152698864, 'median': 1.530029296875, 'mins': 1.455810546875}
 
(28, 5, 2, 40, 48, 40)
Params #:  5040
MACs:  2116800
 
[19:15:37] (INFO) Building library...
[19:15:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5603781960227272, 'median': 1.5423583984375, 'mins': 1.4727783203125}
 
(28, 5, 2, 40, 48, 80)
Params #:  6960
MACs:  2493120
 
[19:15:54] (INFO) Building library...
[19:15:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.724374112215909, 'median': 1.715576171875, 'mins': 1.64794921875}
 
(28, 5, 2, 40, 64, 40)
Params #:  6720
MACs:  2822400
 
[19:16:12] (INFO) Building library...
[19:16:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5147150213068181, 'median': 1.4951171875, 'mins': 1.42578125}
 
(28, 5, 2, 40, 64, 80)
Params #:  9280
MACs:  3324160
 
[19:16:29] (INFO) Building library...
[19:16:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6992697975852273, 'median': 1.6707763671875, 'mins': 1.5899658203125}
 
(28, 5, 2, 40, 80, 40)
Params #:  8400
MACs:  3528000
 
[19:16:46] (INFO) Building library...
[19:16:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.691796875, 'median': 1.670166015625, 'mins': 1.59130859375}
 
(28, 5, 2, 40, 80, 80)
Params #:  11600
MACs:  4155200
 
[19:17:04] (INFO) Building library...
[19:17:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.72705078125, 'median': 1.7061767578125, 'mins': 1.61767578125}
 
(28, 5, 2, 40, 96, 40)
Params #:  10080
MACs:  4233600
 
[19:17:21] (INFO) Building library...
[19:17:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7216497247869318, 'median': 1.7103271484375, 'mins': 1.6353759765625}
 
(28, 5, 2, 40, 96, 80)
Params #:  13920
MACs:  4986240
 
[19:17:39] (INFO) Building library...
[19:17:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.707828036221591, 'median': 1.6812744140625, 'mins': 1.590576171875}
 
(28, 5, 2, 40, 112, 40)
Params #:  11760
MACs:  4939200
 
[19:17:56] (INFO) Building library...
[19:17:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6875377308238637, 'median': 1.6761474609375, 'mins': 1.596923828125}
 
(28, 5, 2, 40, 112, 80)
Params #:  16240
MACs:  5817280
 
[19:18:14] (INFO) Building library...
[19:18:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7463168057528409, 'median': 1.722412109375, 'mins': 1.6375732421875}
 
(28, 5, 2, 40, 128, 40)
Params #:  13440
MACs:  5644800
 
[19:18:31] (INFO) Building library...
[19:18:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6990711558948863, 'median': 1.68359375, 'mins': 1.593994140625}
 
(28, 5, 2, 40, 128, 80)
Params #:  18560
MACs:  6648320
 
[19:18:49] (INFO) Building library...
[19:18:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7753118341619318, 'median': 1.7431640625, 'mins': 1.6510009765625}
 
(28, 5, 2, 40, 144, 40)
Params #:  15120
MACs:  6350400
 
[19:19:06] (INFO) Building library...
[19:19:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5741355202414773, 'median': 1.547607421875, 'mins': 1.4732666015625}
 
(28, 5, 2, 40, 144, 80)
Params #:  20880
MACs:  7479360
 
[19:19:24] (INFO) Building library...
[19:19:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7625221946022727, 'median': 1.737060546875, 'mins': 1.6583251953125}
 
(28, 5, 2, 40, 160, 40)
Params #:  16800
MACs:  7056000
 
[19:19:41] (INFO) Building library...
[19:19:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7393532492897728, 'median': 1.7281494140625, 'mins': 1.6541748046875}
 
(28, 5, 2, 40, 160, 80)
Params #:  23200
MACs:  8310400
 
[19:19:59] (INFO) Building library...
[19:19:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7421564275568182, 'median': 1.70947265625, 'mins': 1.6312255859375}
 
(28, 5, 2, 40, 176, 40)
Params #:  18480
MACs:  7761600
 
[19:20:16] (INFO) Building library...
[19:20:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5872802734375, 'median': 1.560791015625, 'mins': 1.484130859375}
 
(28, 5, 2, 40, 176, 80)
Params #:  25520
MACs:  9141440
 
[19:20:34] (INFO) Building library...
[19:20:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6989302201704546, 'median': 1.6923828125, 'mins': 1.6207275390625}
 
(28, 5, 2, 40, 192, 40)
Params #:  20160
MACs:  8467200
 
[19:20:52] (INFO) Building library...
[19:20:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7288330078125, 'median': 1.696533203125, 'mins': 1.6124267578125}
 
(28, 5, 2, 40, 192, 80)
Params #:  27840
MACs:  9972480
 
[19:21:09] (INFO) Building library...
[19:21:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7229636452414774, 'median': 1.7113037109375, 'mins': 1.61279296875}
 
(28, 5, 2, 40, 208, 40)
Params #:  21840
MACs:  9172800
 
[19:21:27] (INFO) Building library...
[19:21:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7014626242897728, 'median': 1.6722412109375, 'mins': 1.5906982421875}
 
(28, 5, 2, 40, 208, 80)
Params #:  30160
MACs:  10803520
 
[19:21:44] (INFO) Building library...
[19:21:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7931651722301136, 'median': 1.770263671875, 'mins': 1.6678466796875}
 
(28, 5, 2, 40, 224, 40)
Params #:  23520
MACs:  9878400
 
[19:22:02] (INFO) Building library...
[19:22:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7233675870028409, 'median': 1.7064208984375, 'mins': 1.640625}
 
(28, 5, 2, 40, 224, 80)
Params #:  32480
MACs:  11634560
 
[19:22:20] (INFO) Building library...
[19:22:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8184148615056819, 'median': 1.782958984375, 'mins': 1.6932373046875}
 
(28, 5, 2, 40, 240, 40)
Params #:  25200
MACs:  10584000
 
[19:22:37] (INFO) Building library...
[19:22:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6735828746448864, 'median': 1.6588134765625, 'mins': 1.5792236328125}
 
(28, 5, 2, 40, 240, 80)
Params #:  34800
MACs:  12465600
 
[19:22:55] (INFO) Building library...
[19:22:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.804741876775568, 'median': 1.6478271484375, 'mins': 1.5601806640625}
 
(28, 5, 2, 40, 256, 40)
Params #:  26880
MACs:  11289600
 
[19:23:12] (INFO) Building library...
[19:23:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.83939208984375, 'median': 1.809326171875, 'mins': 1.6986083984375}
 
(28, 5, 2, 40, 256, 80)
Params #:  37120
MACs:  13296640
 
[19:23:30] (INFO) Building library...
[19:23:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8879605379971591, 'median': 1.7532958984375, 'mins': 1.6622314453125}
 
(28, 5, 2, 40, 272, 40)
Params #:  28560
MACs:  11995200
 
[19:23:48] (INFO) Building library...
[19:23:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.73446044921875, 'median': 1.708984375, 'mins': 1.6263427734375}
 
(28, 5, 2, 40, 272, 80)
Params #:  39440
MACs:  14127680
 
[19:24:05] (INFO) Building library...
[19:24:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7258023348721592, 'median': 1.7120361328125, 'mins': 1.6346435546875}
 
(28, 5, 2, 40, 288, 40)
Params #:  30240
MACs:  12700800
 
[19:24:23] (INFO) Building library...
[19:24:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7614635120738635, 'median': 1.7344970703125, 'mins': 1.6409912109375}
 
(28, 5, 2, 40, 288, 80)
Params #:  41760
MACs:  14958720
 
[19:24:40] (INFO) Building library...
[19:24:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6974587180397727, 'median': 1.6793212890625, 'mins': 1.6051025390625}
 
(28, 5, 2, 40, 304, 40)
Params #:  31920
MACs:  13406400
 
[19:24:58] (INFO) Building library...
[19:24:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.896484375, 'median': 1.748779296875, 'mins': 1.6568603515625}
 
(28, 5, 2, 40, 304, 80)
Params #:  44080
MACs:  15789760
 
[19:25:16] (INFO) Building library...
[19:25:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8230490944602273, 'median': 1.78955078125, 'mins': 1.6971435546875}
 
(28, 5, 2, 40, 320, 40)
Params #:  33600
MACs:  14112000
 
[19:25:33] (INFO) Building library...
[19:25:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7892567027698865, 'median': 1.7789306640625, 'mins': 1.714111328125}
 
(28, 5, 2, 40, 320, 80)
Params #:  46400
MACs:  16620800
 
[19:25:51] (INFO) Building library...
[19:25:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9587957208806819, 'median': 1.8251953125, 'mins': 1.724853515625}
 
(28, 5, 2, 40, 336, 40)
Params #:  35280
MACs:  14817600
 
[19:26:09] (INFO) Building library...
[19:26:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8447376598011365, 'median': 1.690673828125, 'mins': 1.6080322265625}
 
(28, 5, 2, 40, 336, 80)
Params #:  48720
MACs:  17451840
 
[19:26:27] (INFO) Building library...
[19:26:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9423162286931819, 'median': 1.7783203125, 'mins': 1.695068359375}
 
(28, 5, 2, 40, 352, 40)
Params #:  36960
MACs:  15523200
 
[19:26:45] (INFO) Building library...
[19:26:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7448286576704546, 'median': 1.7216796875, 'mins': 1.644775390625}
 
(28, 5, 2, 40, 352, 80)
Params #:  51040
MACs:  18282880
 
[19:27:02] (INFO) Building library...
[19:27:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8891912286931818, 'median': 1.7498779296875, 'mins': 1.6710205078125}
 
(14, 5, 1, 80, 16, 40)
Params #:  2320
MACs:  454720
 
[19:27:20] (INFO) Building library...
[19:27:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4720869584517045, 'median': 1.4554443359375, 'mins': 1.39599609375}
 
(14, 5, 1, 80, 16, 80)
Params #:  2960
MACs:  580160
 
[19:27:37] (INFO) Building library...
[19:27:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1667014382102274, 'median': 2.136962890625, 'mins': 2.0538330078125}
 
(14, 5, 1, 80, 32, 40)
Params #:  4640
MACs:  909440
 
[19:27:56] (INFO) Building library...
[19:27:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8657426313920455, 'median': 1.832275390625, 'mins': 1.7373046875}
 
(14, 5, 1, 80, 32, 80)
Params #:  5920
MACs:  1160320
 
[19:28:13] (INFO) Building library...
[19:28:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2197210138494317, 'median': 2.19189453125, 'mins': 2.0926513671875}
 
(14, 5, 1, 80, 48, 40)
Params #:  6960
MACs:  1364160
 
[19:28:32] (INFO) Building library...
[19:28:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.48719482421875, 'median': 1.4698486328125, 'mins': 1.4051513671875}
 
(14, 5, 1, 80, 48, 80)
Params #:  8880
MACs:  1740480
 
[19:28:49] (INFO) Building library...
[19:28:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1580078125, 'median': 2.1343994140625, 'mins': 2.0472412109375}
 
(14, 5, 1, 80, 64, 40)
Params #:  9280
MACs:  1818880
 
[19:29:08] (INFO) Building library...
[19:29:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.62669677734375, 'median': 1.6082763671875, 'mins': 1.5277099609375}
 
(14, 5, 1, 80, 64, 80)
Params #:  11840
MACs:  2320640
 
[19:29:25] (INFO) Building library...
[19:29:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.355489834872159, 'median': 2.3223876953125, 'mins': 2.220947265625}
 
(14, 5, 1, 80, 96, 40)
Params #:  13920
MACs:  2728320
 
[19:29:44] (INFO) Building library...
[19:29:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5230635209517045, 'median': 1.5074462890625, 'mins': 1.431396484375}
 
(14, 5, 1, 80, 96, 80)
Params #:  17760
MACs:  3480960
 
[19:30:01] (INFO) Building library...
[19:30:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1581454190340907, 'median': 2.1380615234375, 'mins': 2.0472412109375}
 
(14, 5, 1, 80, 112, 40)
Params #:  16240
MACs:  3183040
 
[19:30:20] (INFO) Building library...
[19:30:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6414306640625, 'median': 1.6226806640625, 'mins': 1.549560546875}
 
(14, 5, 1, 80, 112, 80)
Params #:  20720
MACs:  4061120
 
[19:30:37] (INFO) Building library...
[19:30:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5619185014204544, 'median': 2.582763671875, 'mins': 2.2657470703125}
 
(14, 5, 1, 80, 128, 40)
Params #:  18560
MACs:  3637760
 
[19:30:56] (INFO) Building library...
[19:30:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.570083895596591, 'median': 1.4962158203125, 'mins': 1.40966796875}
 
(14, 5, 1, 80, 128, 80)
Params #:  23680
MACs:  4641280
 
[19:31:14] (INFO) Building library...
[19:31:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4093117453835227, 'median': 2.3824462890625, 'mins': 2.2774658203125}
 
(14, 5, 1, 80, 144, 40)
Params #:  20880
MACs:  4092480
 
[19:31:33] (INFO) Building library...
[19:31:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6378828568892045, 'median': 1.622314453125, 'mins': 1.5576171875}
 
(14, 5, 1, 80, 144, 80)
Params #:  26640
MACs:  5221440
 
[19:31:50] (INFO) Building library...
[19:31:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3544777610085226, 'median': 2.3367919921875, 'mins': 2.250244140625}
 
(14, 5, 1, 80, 160, 40)
Params #:  23200
MACs:  4547200
 
[19:32:09] (INFO) Building library...
[19:32:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5254638671875, 'median': 1.5029296875, 'mins': 1.4219970703125}
 
(14, 5, 1, 80, 160, 80)
Params #:  29600
MACs:  5801600
 
[19:32:26] (INFO) Building library...
[19:32:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.426431551846591, 'median': 2.3846435546875, 'mins': 2.2908935546875}
 
(14, 5, 1, 80, 176, 40)
Params #:  25520
MACs:  5001920
 
[19:32:46] (INFO) Building library...
[19:32:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6550437233664772, 'median': 1.642578125, 'mins': 1.5687255859375}
 
(14, 5, 1, 80, 176, 80)
Params #:  32560
MACs:  6381760
 
[19:33:03] (INFO) Building library...
[19:33:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4317982066761363, 'median': 2.401123046875, 'mins': 2.3028564453125}
 
(14, 5, 1, 80, 192, 40)
Params #:  27840
MACs:  5456640
 
[19:33:22] (INFO) Building library...
[19:33:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.626571377840909, 'median': 1.6070556640625, 'mins': 1.536865234375}
 
(14, 5, 1, 80, 192, 80)
Params #:  35520
MACs:  6961920
 
[19:33:39] (INFO) Building library...
[19:33:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2312943892045456, 'median': 2.2110595703125, 'mins': 2.130615234375}
 
(14, 5, 1, 80, 208, 40)
Params #:  30160
MACs:  5911360
 
[19:33:58] (INFO) Building library...
[19:33:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.616522771661932, 'median': 1.60546875, 'mins': 1.5367431640625}
 
(14, 5, 1, 80, 208, 80)
Params #:  38480
MACs:  7542080
 
[19:34:15] (INFO) Building library...
[19:34:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.470807439630682, 'median': 2.445068359375, 'mins': 2.3143310546875}
 
(14, 5, 1, 80, 224, 40)
Params #:  32480
MACs:  6366080
 
[19:34:35] (INFO) Building library...
[19:34:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.64007568359375, 'median': 1.5985107421875, 'mins': 1.53369140625}
 
(14, 5, 1, 80, 224, 80)
Params #:  41440
MACs:  8122240
 
[19:34:52] (INFO) Building library...
[19:34:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.622726162997159, 'median': 2.6153564453125, 'mins': 2.5089111328125}
 
(14, 5, 1, 80, 240, 40)
Params #:  34800
MACs:  6820800
 
[19:35:11] (INFO) Building library...
[19:35:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6799205433238635, 'median': 1.6539306640625, 'mins': 1.5771484375}
 
(14, 5, 1, 80, 240, 80)
Params #:  44400
MACs:  8702400
 
[19:35:29] (INFO) Building library...
[19:35:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.436600008877841, 'median': 2.402099609375, 'mins': 2.3038330078125}
 
(14, 5, 1, 80, 256, 40)
Params #:  37120
MACs:  7275520
 
[19:35:48] (INFO) Building library...
[19:35:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6904707475142045, 'median': 1.6575927734375, 'mins': 1.5833740234375}
 
(14, 5, 1, 80, 256, 80)
Params #:  47360
MACs:  9282560
 
[19:36:05] (INFO) Building library...
[19:36:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3606068004261362, 'median': 2.3563232421875, 'mins': 2.2733154296875}
 
(14, 5, 1, 80, 272, 40)
Params #:  39440
MACs:  7730240
 
[19:36:24] (INFO) Building library...
[19:36:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6438731800426136, 'median': 1.61865234375, 'mins': 1.5372314453125}
 
(14, 5, 1, 80, 272, 80)
Params #:  50320
MACs:  9862720
 
[19:36:42] (INFO) Building library...
[19:36:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4561567826704547, 'median': 2.4271240234375, 'mins': 2.3255615234375}
 
(14, 5, 1, 80, 288, 40)
Params #:  41760
MACs:  8184960
 
[19:37:01] (INFO) Building library...
[19:37:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7491155450994318, 'median': 1.726318359375, 'mins': 1.64892578125}
 
(14, 5, 1, 80, 288, 80)
Params #:  53280
MACs:  10442880
 
[19:37:18] (INFO) Building library...
[19:37:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.163535378196023, 'median': 2.13720703125, 'mins': 2.0469970703125}
 
(14, 5, 1, 80, 304, 40)
Params #:  44080
MACs:  8639680
 
[19:37:37] (INFO) Building library...
[19:37:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.645315829190341, 'median': 1.6329345703125, 'mins': 1.5645751953125}
 
(14, 5, 1, 80, 304, 80)
Params #:  56240
MACs:  11023040
 
[19:37:54] (INFO) Building library...
[19:37:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.399884588068182, 'median': 2.3792724609375, 'mins': 2.284912109375}
 
(14, 5, 1, 80, 320, 40)
Params #:  46400
MACs:  9094400
 
[19:38:14] (INFO) Building library...
[19:38:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.473504083806818, 'median': 1.458984375, 'mins': 1.3956298828125}
 
(14, 5, 1, 80, 320, 80)
Params #:  59200
MACs:  11603200
 
[19:38:31] (INFO) Building library...
[19:38:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.149298650568182, 'median': 2.118896484375, 'mins': 2.0379638671875}
 
(14, 5, 1, 80, 336, 40)
Params #:  48720
MACs:  9549120
 
[19:38:50] (INFO) Building library...
[19:38:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6545121626420454, 'median': 1.62548828125, 'mins': 1.5516357421875}
 
(14, 5, 1, 80, 336, 80)
Params #:  62160
MACs:  12183360
 
[19:39:07] (INFO) Building library...
[19:39:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2067149769176138, 'median': 2.1693115234375, 'mins': 2.088134765625}
 
(14, 5, 1, 80, 352, 40)
Params #:  51040
MACs:  10003840
 
[19:39:26] (INFO) Building library...
[19:39:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.45859375, 'median': 1.44677734375, 'mins': 1.38623046875}
 
(14, 5, 1, 80, 352, 80)
Params #:  65120
MACs:  12763520
 
[19:39:43] (INFO) Building library...
[19:39:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.32122802734375, 'median': 2.317626953125, 'mins': 2.2255859375}
 
(14, 5, 1, 80, 368, 40)
Params #:  53360
MACs:  10458560
 
[19:40:02] (INFO) Building library...
[19:40:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7112537730823865, 'median': 1.6785888671875, 'mins': 1.59765625}
 
(14, 5, 1, 80, 368, 80)
Params #:  68080
MACs:  13343680
 
[19:40:20] (INFO) Building library...
[19:40:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.394334827769886, 'median': 2.361328125, 'mins': 2.2623291015625}
 
(14, 5, 1, 80, 384, 40)
Params #:  55680
MACs:  10913280
 
[19:40:39] (INFO) Building library...
[19:40:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7517256303267046, 'median': 1.7122802734375, 'mins': 1.6444091796875}
 
(14, 5, 1, 80, 384, 80)
Params #:  71040
MACs:  13923840
 
[19:40:56] (INFO) Building library...
[19:40:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.137559925426136, 'median': 2.103271484375, 'mins': 2.0262451171875}
 
(14, 5, 1, 80, 400, 40)
Params #:  58000
MACs:  11368000
 
[19:41:15] (INFO) Building library...
[19:41:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8858231977982955, 'median': 1.83251953125, 'mins': 1.7525634765625}
 
(14, 5, 1, 80, 400, 80)
Params #:  74000
MACs:  14504000
 
[19:41:33] (INFO) Building library...
[19:41:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4064286665482952, 'median': 2.378173828125, 'mins': 2.2843017578125}
 
(14, 5, 1, 80, 416, 40)
Params #:  60320
MACs:  11822720
 
[19:41:52] (INFO) Building library...
[19:41:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6339111328125, 'median': 1.604736328125, 'mins': 1.5303955078125}
 
(14, 5, 1, 80, 416, 80)
Params #:  76960
MACs:  15084160
 
[19:42:09] (INFO) Building library...
[19:42:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.406576260653409, 'median': 2.3924560546875, 'mins': 2.3216552734375}
 
(14, 5, 1, 80, 432, 40)
Params #:  62640
MACs:  12277440
 
[19:42:29] (INFO) Building library...
[19:42:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9182572798295454, 'median': 1.7349853515625, 'mins': 1.6278076171875}
 
(14, 5, 1, 80, 432, 80)
Params #:  79920
MACs:  15664320
 
[19:42:46] (INFO) Building library...
[19:42:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5104203657670454, 'median': 2.50634765625, 'mins': 2.309814453125}
 
(14, 5, 1, 80, 448, 40)
Params #:  64960
MACs:  12732160
 
[19:43:05] (INFO) Building library...
[19:43:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.716116610440341, 'median': 1.6845703125, 'mins': 1.60546875}
 
(14, 5, 1, 80, 448, 80)
Params #:  82880
MACs:  16244480
 
[19:43:23] (INFO) Building library...
[19:43:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.191805752840909, 'median': 2.170654296875, 'mins': 2.085693359375}
 
(14, 5, 1, 80, 464, 40)
Params #:  67280
MACs:  13186880
 
[19:43:42] (INFO) Building library...
[19:43:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7364402077414773, 'median': 1.7083740234375, 'mins': 1.620361328125}
 
(14, 5, 1, 80, 464, 80)
Params #:  85840
MACs:  16824640
 
[19:43:59] (INFO) Building library...
[19:43:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3360151811079546, 'median': 2.2330322265625, 'mins': 2.1021728515625}
 
(14, 5, 1, 80, 480, 40)
Params #:  69600
MACs:  13641600
 
[19:44:18] (INFO) Building library...
[19:44:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6562688654119317, 'median': 1.64697265625, 'mins': 1.5845947265625}
 
(14, 5, 1, 80, 480, 80)
Params #:  88800
MACs:  17404800
 
[19:44:35] (INFO) Building library...
[19:44:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3635364879261362, 'median': 2.3572998046875, 'mins': 2.265625}
 
(14, 5, 1, 80, 496, 40)
Params #:  71920
MACs:  14096320
 
[19:44:55] (INFO) Building library...
[19:44:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7436301491477273, 'median': 1.7177734375, 'mins': 1.6285400390625}
 
(14, 5, 1, 80, 496, 80)
Params #:  91760
MACs:  17984960
 
[19:45:12] (INFO) Building library...
[19:45:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4416392933238638, 'median': 2.4056396484375, 'mins': 2.3067626953125}
 
(14, 5, 1, 80, 512, 40)
Params #:  74240
MACs:  14551040
 
[19:45:31] (INFO) Building library...
[19:45:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7039894797585227, 'median': 1.6776123046875, 'mins': 1.5908203125}
 
(14, 5, 1, 80, 512, 80)
Params #:  94720
MACs:  18565120
 
[19:45:48] (INFO) Building library...
[19:45:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3880859375, 'median': 2.37744140625, 'mins': 2.2974853515625}
 
(14, 5, 1, 80, 528, 40)
Params #:  76560
MACs:  15005760
 
[19:46:08] (INFO) Building library...
[19:46:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9222789417613637, 'median': 1.7352294921875, 'mins': 1.636474609375}
 
(14, 5, 1, 80, 528, 80)
Params #:  97680
MACs:  19145280
 
[19:46:25] (INFO) Building library...
[19:46:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4187111594460227, 'median': 2.385986328125, 'mins': 2.279541015625}
 
(14, 5, 1, 80, 544, 40)
Params #:  78880
MACs:  15460480
 
[19:46:45] (INFO) Building library...
[19:46:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.704583185369318, 'median': 1.6751708984375, 'mins': 1.6063232421875}
 
(14, 5, 1, 80, 544, 80)
Params #:  100640
MACs:  19725440
 
[19:47:02] (INFO) Building library...
[19:47:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5822654030539773, 'median': 2.543212890625, 'mins': 2.41796875}
 
(14, 5, 1, 80, 560, 40)
Params #:  81200
MACs:  15915200
 
[19:47:22] (INFO) Building library...
[19:47:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6983953302556818, 'median': 1.68994140625, 'mins': 1.621337890625}
 
(14, 5, 1, 80, 560, 80)
Params #:  103600
MACs:  20305600
 
[19:47:40] (INFO) Building library...
[19:47:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2177001953125, 'median': 2.184326171875, 'mins': 2.092041015625}
 
(14, 5, 1, 80, 576, 40)
Params #:  83520
MACs:  16369920
 
[19:47:59] (INFO) Building library...
[19:47:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7165249911221592, 'median': 1.7109375, 'mins': 1.6461181640625}
 
(14, 5, 1, 80, 576, 80)
Params #:  106560
MACs:  20885760
 
[19:48:17] (INFO) Building library...
[19:48:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4799050071022726, 'median': 2.44287109375, 'mins': 2.336181640625}
 
(14, 5, 1, 80, 592, 40)
Params #:  85840
MACs:  16824640
 
[19:48:36] (INFO) Building library...
[19:48:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7511863014914772, 'median': 1.7318115234375, 'mins': 1.652099609375}
 
(14, 5, 1, 80, 592, 80)
Params #:  109520
MACs:  21465920
 
[19:48:54] (INFO) Building library...
[19:48:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.57471923828125, 'median': 2.53466796875, 'mins': 2.4354248046875}
 
(14, 5, 1, 80, 608, 40)
Params #:  88160
MACs:  17279360
 
[19:49:13] (INFO) Building library...
[19:49:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.759878817471591, 'median': 1.7315673828125, 'mins': 1.644287109375}
 
(14, 5, 1, 80, 608, 80)
Params #:  112480
MACs:  22046080
 
[19:49:31] (INFO) Building library...
[19:49:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5572432084517045, 'median': 2.5380859375, 'mins': 2.4423828125}
 
(14, 5, 1, 80, 624, 40)
Params #:  90480
MACs:  17734080
 
[19:49:51] (INFO) Building library...
[19:49:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8387673117897727, 'median': 1.6966552734375, 'mins': 1.617431640625}
 
(14, 5, 1, 80, 624, 80)
Params #:  115440
MACs:  22626240
 
[19:50:09] (INFO) Building library...
[19:50:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5267888849431817, 'median': 2.4544677734375, 'mins': 2.3507080078125}
 
(14, 5, 1, 80, 640, 40)
Params #:  92800
MACs:  18188800
 
[19:50:28] (INFO) Building library...
[19:50:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7317549272017045, 'median': 1.7021484375, 'mins': 1.6322021484375}
 
(14, 5, 1, 80, 640, 80)
Params #:  118400
MACs:  23206400
 
[19:50:46] (INFO) Building library...
[19:50:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6821277965198864, 'median': 2.634765625, 'mins': 2.5364990234375}
 
(14, 5, 1, 80, 656, 40)
Params #:  95120
MACs:  18643520
 
[19:51:05] (INFO) Building library...
[19:51:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6773459694602273, 'median': 1.6695556640625, 'mins': 1.6011962890625}
 
(14, 5, 1, 80, 656, 80)
Params #:  121360
MACs:  23786560
 
[19:51:23] (INFO) Building library...
[19:51:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4572509765625, 'median': 2.4251708984375, 'mins': 2.317138671875}
 
(14, 5, 1, 80, 672, 40)
Params #:  97440
MACs:  19098240
 
[19:51:42] (INFO) Building library...
[19:51:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8301791104403409, 'median': 1.701171875, 'mins': 1.604736328125}
 
(14, 5, 1, 80, 672, 80)
Params #:  124320
MACs:  24366720
 
[19:52:00] (INFO) Building library...
[19:52:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2323441938920454, 'median': 2.2174072265625, 'mins': 2.1282958984375}
 
(14, 5, 1, 80, 688, 40)
Params #:  99760
MACs:  19552960
 
[19:52:19] (INFO) Building library...
[19:52:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7568958629261364, 'median': 1.7308349609375, 'mins': 1.6483154296875}
 
(14, 5, 1, 80, 688, 80)
Params #:  127280
MACs:  24946880
 
[19:52:36] (INFO) Building library...
[19:52:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5798961292613636, 'median': 2.5482177734375, 'mins': 2.4595947265625}
 
(14, 5, 1, 80, 704, 40)
Params #:  102080
MACs:  20007680
 
[19:52:56] (INFO) Building library...
[19:52:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7777920809659091, 'median': 1.768798828125, 'mins': 1.69140625}
 
(14, 5, 1, 80, 704, 80)
Params #:  130240
MACs:  25527040
 
[19:53:13] (INFO) Building library...
[19:53:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.439333274147727, 'median': 2.4111328125, 'mins': 2.285400390625}
 
(14, 3, 1, 80, 16, 48)
Params #:  2192
MACs:  429632
 
[19:53:32] (INFO) Building library...
[19:53:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5913141424005681, 'median': 1.569580078125, 'mins': 1.46875}
 
(14, 3, 1, 80, 16, 96)
Params #:  2960
MACs:  580160
 
[19:53:49] (INFO) Building library...
[19:53:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.28406982421875, 'median': 1.277099609375, 'mins': 1.2158203125}
 
(14, 3, 1, 80, 32, 48)
Params #:  4384
MACs:  859264
 
[19:54:07] (INFO) Building library...
[19:54:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4407126686789773, 'median': 1.4271240234375, 'mins': 1.3671875}
 
(14, 3, 1, 80, 32, 96)
Params #:  5920
MACs:  1160320
 
[19:54:24] (INFO) Building library...
[19:54:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4773781516335227, 'median': 1.468017578125, 'mins': 1.4072265625}
 
(14, 3, 1, 80, 48, 48)
Params #:  6576
MACs:  1288896
 
[19:54:41] (INFO) Building library...
[19:54:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4620194868607954, 'median': 1.4437255859375, 'mins': 1.375732421875}
 
(14, 3, 1, 80, 48, 96)
Params #:  8880
MACs:  1740480
 
[19:54:57] (INFO) Building library...
[19:54:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4682839133522727, 'median': 1.45263671875, 'mins': 1.39013671875}
 
(14, 3, 1, 80, 64, 48)
Params #:  8768
MACs:  1718528
 
[19:55:14] (INFO) Building library...
[19:55:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6353415749289772, 'median': 1.61474609375, 'mins': 1.5406494140625}
 
(14, 3, 1, 80, 64, 96)
Params #:  11840
MACs:  2320640
 
[19:55:32] (INFO) Building library...
[19:55:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4682384144176137, 'median': 1.4498291015625, 'mins': 1.3787841796875}
 
(14, 3, 1, 80, 96, 48)
Params #:  13152
MACs:  2577792
 
[19:55:48] (INFO) Building library...
[19:55:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4757679332386364, 'median': 1.456298828125, 'mins': 1.38525390625}
 
(14, 3, 1, 80, 96, 96)
Params #:  17760
MACs:  3480960
 
[19:56:05] (INFO) Building library...
[19:56:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4622991388494317, 'median': 1.4298095703125, 'mins': 1.372314453125}
 
(14, 3, 1, 80, 112, 48)
Params #:  15344
MACs:  3007424
 
[19:56:22] (INFO) Building library...
[19:56:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6400312943892046, 'median': 1.6131591796875, 'mins': 1.5443115234375}
 
(14, 3, 1, 80, 112, 96)
Params #:  20720
MACs:  4061120
 
[19:56:38] (INFO) Building library...
[19:56:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6687355735085228, 'median': 1.6356201171875, 'mins': 1.5474853515625}
 
(14, 3, 1, 80, 128, 48)
Params #:  17536
MACs:  3437056
 
[19:56:56] (INFO) Building library...
[19:56:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.658642578125, 'median': 1.6287841796875, 'mins': 1.5528564453125}
 
(14, 3, 1, 80, 128, 96)
Params #:  23680
MACs:  4641280
 
[19:57:12] (INFO) Building library...
[19:57:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6619040749289773, 'median': 1.63623046875, 'mins': 1.553955078125}
 
(14, 3, 1, 80, 144, 48)
Params #:  19728
MACs:  3866688
 
[19:57:29] (INFO) Building library...
[19:57:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4203446821732955, 'median': 1.414306640625, 'mins': 1.34912109375}
 
(14, 3, 1, 80, 144, 96)
Params #:  26640
MACs:  5221440
 
[19:57:47] (INFO) Building library...
[19:57:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.639962491122159, 'median': 1.622802734375, 'mins': 1.5428466796875}
 
(14, 3, 1, 80, 160, 48)
Params #:  21920
MACs:  4296320
 
[19:58:03] (INFO) Building library...
[19:58:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6430464311079545, 'median': 1.617431640625, 'mins': 1.5443115234375}
 
(14, 3, 1, 80, 160, 96)
Params #:  29600
MACs:  5801600
 
[19:58:20] (INFO) Building library...
[19:58:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5708917791193182, 'median': 1.519775390625, 'mins': 1.4215087890625}
 
(14, 3, 1, 80, 176, 48)
Params #:  24112
MACs:  4725952
 
[19:58:37] (INFO) Building library...
[19:58:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5083362926136363, 'median': 1.4901123046875, 'mins': 1.418701171875}
 
(14, 3, 1, 80, 176, 96)
Params #:  32560
MACs:  6381760
 
[19:58:54] (INFO) Building library...
[19:58:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6682683771306819, 'median': 1.641845703125, 'mins': 1.56640625}
 
(14, 3, 1, 80, 192, 48)
Params #:  26304
MACs:  5155584
 
[19:59:12] (INFO) Building library...
[19:59:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.323370916193182, 'median': 1.2996826171875, 'mins': 1.2200927734375}
 
(14, 3, 1, 80, 192, 96)
Params #:  35520
MACs:  6961920
 
[19:59:28] (INFO) Building library...
[19:59:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6847578568892045, 'median': 1.6568603515625, 'mins': 1.5789794921875}
 
(14, 3, 1, 80, 208, 48)
Params #:  28496
MACs:  5585216
 
[19:59:45] (INFO) Building library...
[19:59:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8611716530539772, 'median': 1.6787109375, 'mins': 1.5943603515625}
 
(14, 3, 1, 80, 208, 96)
Params #:  38480
MACs:  7542080
 
[20:00:03] (INFO) Building library...
[20:00:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6627053000710228, 'median': 1.65478515625, 'mins': 1.5904541015625}
 
(14, 3, 1, 80, 224, 48)
Params #:  30688
MACs:  6014848
 
[20:00:20] (INFO) Building library...
[20:00:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5876043146306817, 'median': 1.5716552734375, 'mins': 1.5125732421875}
 
(14, 3, 1, 80, 224, 96)
Params #:  41440
MACs:  8122240
 
[20:00:37] (INFO) Building library...
[20:00:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6312855113636364, 'median': 1.60888671875, 'mins': 1.5186767578125}
 
(14, 3, 1, 80, 240, 48)
Params #:  32880
MACs:  6444480
 
[20:00:54] (INFO) Building library...
[20:00:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5529618696732954, 'median': 1.5250244140625, 'mins': 1.4510498046875}
 
(14, 3, 1, 80, 240, 96)
Params #:  44400
MACs:  8702400
 
[20:01:11] (INFO) Building library...
[20:01:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9128972833806819, 'median': 1.87353515625, 'mins': 1.7791748046875}
 
(14, 3, 1, 80, 256, 48)
Params #:  35072
MACs:  6874112
 
[20:01:29] (INFO) Building library...
[20:01:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6520818536931818, 'median': 1.6234130859375, 'mins': 1.536376953125}
 
(14, 3, 1, 80, 256, 96)
Params #:  47360
MACs:  9282560
 
[20:01:45] (INFO) Building library...
[20:01:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.483096590909091, 'median': 1.4779052734375, 'mins': 1.4205322265625}
 
(14, 3, 1, 80, 272, 48)
Params #:  37264
MACs:  7303744
 
[20:02:02] (INFO) Building library...
[20:02:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6785478071732955, 'median': 1.654296875, 'mins': 1.5704345703125}
 
(14, 3, 1, 80, 272, 96)
Params #:  50320
MACs:  9862720
 
[20:02:20] (INFO) Building library...
[20:02:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8271317915482954, 'median': 1.6806640625, 'mins': 1.6025390625}
 
(14, 3, 1, 80, 288, 48)
Params #:  39456
MACs:  7733376
 
[20:02:37] (INFO) Building library...
[20:02:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3537198153409091, 'median': 1.33740234375, 'mins': 1.2838134765625}
 
(14, 3, 1, 80, 288, 96)
Params #:  53280
MACs:  10442880
 
[20:02:55] (INFO) Building library...
[20:02:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.68916015625, 'median': 1.6539306640625, 'mins': 1.5545654296875}
 
(14, 3, 1, 80, 304, 48)
Params #:  41648
MACs:  8163008
 
[20:03:13] (INFO) Building library...
[20:03:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8037775213068181, 'median': 1.6986083984375, 'mins': 1.6068115234375}
 
(14, 3, 1, 80, 304, 96)
Params #:  56240
MACs:  11023040
 
[20:03:30] (INFO) Building library...
[20:03:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8970636541193182, 'median': 1.87255859375, 'mins': 1.7098388671875}
 
(14, 3, 1, 80, 320, 48)
Params #:  43840
MACs:  8592640
 
[20:03:48] (INFO) Building library...
[20:03:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8380604137073864, 'median': 1.685546875, 'mins': 1.5941162109375}
 
(14, 3, 1, 80, 320, 96)
Params #:  59200
MACs:  11603200
 
[20:04:05] (INFO) Building library...
[20:04:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8650290749289773, 'median': 1.732421875, 'mins': 1.618896484375}
 
(14, 3, 1, 80, 336, 48)
Params #:  46032
MACs:  9022272
 
[20:04:23] (INFO) Building library...
[20:04:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.897579678622159, 'median': 1.7283935546875, 'mins': 1.63037109375}
 
(14, 3, 1, 80, 336, 96)
Params #:  62160
MACs:  12183360
 
[20:04:40] (INFO) Building library...
[20:04:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6692604758522727, 'median': 1.663818359375, 'mins': 1.5826416015625}
 
(14, 3, 1, 80, 352, 48)
Params #:  48224
MACs:  9451904
 
[20:04:57] (INFO) Building library...
[20:04:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6973921342329545, 'median': 1.6678466796875, 'mins': 1.5985107421875}
 
(14, 3, 1, 80, 352, 96)
Params #:  65120
MACs:  12763520
 
[20:05:15] (INFO) Building library...
[20:05:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7050648082386364, 'median': 1.671630859375, 'mins': 1.5994873046875}
 
(14, 3, 1, 80, 368, 48)
Params #:  50416
MACs:  9881536
 
[20:05:32] (INFO) Building library...
[20:05:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6427334872159092, 'median': 1.6177978515625, 'mins': 1.53515625}
 
(14, 3, 1, 80, 368, 96)
Params #:  68080
MACs:  13343680
 
[20:05:49] (INFO) Building library...
[20:05:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.65308837890625, 'median': 1.6280517578125, 'mins': 1.5440673828125}
 
(14, 3, 1, 80, 384, 48)
Params #:  52608
MACs:  10311168
 
[20:06:06] (INFO) Building library...
[20:06:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6596047141335226, 'median': 1.5118408203125, 'mins': 1.4384765625}
 
(14, 3, 1, 80, 384, 96)
Params #:  71040
MACs:  13923840
 
[20:06:23] (INFO) Building library...
[20:06:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4582231001420454, 'median': 1.445068359375, 'mins': 1.3802490234375}
 
(14, 3, 1, 80, 400, 48)
Params #:  54800
MACs:  10740800
 
[20:06:40] (INFO) Building library...
[20:06:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4460915305397728, 'median': 1.4359130859375, 'mins': 1.383056640625}
 
(14, 3, 1, 80, 400, 96)
Params #:  74000
MACs:  14504000
 
[20:06:57] (INFO) Building library...
[20:06:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7039395419034091, 'median': 1.67626953125, 'mins': 1.602294921875}
 
(14, 3, 1, 80, 416, 48)
Params #:  56992
MACs:  11170432
 
[20:07:15] (INFO) Building library...
[20:07:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.65958251953125, 'median': 1.626953125, 'mins': 1.5496826171875}
 
(14, 3, 1, 80, 416, 96)
Params #:  76960
MACs:  15084160
 
[20:07:32] (INFO) Building library...
[20:07:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6635941938920455, 'median': 1.6353759765625, 'mins': 1.5645751953125}
 
(14, 3, 1, 80, 432, 48)
Params #:  59184
MACs:  11600064
 
[20:07:49] (INFO) Building library...
[20:07:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7135941938920454, 'median': 1.69091796875, 'mins': 1.628662109375}
 
(14, 3, 1, 80, 432, 96)
Params #:  79920
MACs:  15664320
 
[20:08:07] (INFO) Building library...
[20:08:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.737533291903409, 'median': 1.719970703125, 'mins': 1.6331787109375}
 
(14, 3, 1, 80, 448, 48)
Params #:  61376
MACs:  12029696
 
[20:08:24] (INFO) Building library...
[20:08:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.83135986328125, 'median': 1.720947265625, 'mins': 1.6322021484375}
 
(14, 3, 1, 80, 448, 96)
Params #:  82880
MACs:  16244480
 
[20:08:41] (INFO) Building library...
[20:08:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.869766512784091, 'median': 1.8446044921875, 'mins': 1.741455078125}
 
(14, 3, 1, 80, 464, 48)
Params #:  63568
MACs:  12459328
 
[20:08:59] (INFO) Building library...
[20:08:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6472245649857955, 'median': 1.511962890625, 'mins': 1.4368896484375}
 
(14, 3, 1, 80, 464, 96)
Params #:  85840
MACs:  16824640
 
[20:09:16] (INFO) Building library...
[20:09:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7349909002130681, 'median': 1.704345703125, 'mins': 1.608642578125}
 
(14, 3, 1, 80, 480, 48)
Params #:  65760
MACs:  12888960
 
[20:09:34] (INFO) Building library...
[20:09:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7217029918323863, 'median': 1.6912841796875, 'mins': 1.6146240234375}
 
(14, 3, 1, 80, 480, 96)
Params #:  88800
MACs:  17404800
 
[20:09:51] (INFO) Building library...
[20:09:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5956898082386364, 'median': 1.5361328125, 'mins': 1.44873046875}
 
(14, 3, 1, 80, 496, 48)
Params #:  67952
MACs:  13318592
 
[20:10:08] (INFO) Building library...
[20:10:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7361683238636363, 'median': 1.719482421875, 'mins': 1.66162109375}
 
(14, 3, 1, 80, 496, 96)
Params #:  91760
MACs:  17984960
 
[20:10:25] (INFO) Building library...
[20:10:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6575761274857954, 'median': 1.6483154296875, 'mins': 1.5782470703125}
 
(14, 3, 1, 80, 512, 48)
Params #:  70144
MACs:  13748224
 
[20:10:43] (INFO) Building library...
[20:10:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5268199573863637, 'median': 1.51904296875, 'mins': 1.4521484375}
 
(14, 3, 1, 80, 512, 96)
Params #:  94720
MACs:  18565120
 
[20:10:59] (INFO) Building library...
[20:10:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8336259321732955, 'median': 1.814697265625, 'mins': 1.743896484375}
 
(14, 3, 1, 80, 528, 48)
Params #:  72336
MACs:  14177856
 
[20:11:16] (INFO) Building library...
[20:11:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6751143022017045, 'median': 1.6656494140625, 'mins': 1.59765625}
 
(14, 3, 1, 80, 528, 96)
Params #:  97680
MACs:  19145280
 
[20:11:34] (INFO) Building library...
[20:11:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7215443004261364, 'median': 1.6961669921875, 'mins': 1.6124267578125}
 
(14, 3, 1, 80, 544, 48)
Params #:  74528
MACs:  14607488
 
[20:11:51] (INFO) Building library...
[20:11:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6499167702414772, 'median': 1.51953125, 'mins': 1.42919921875}
 
(14, 3, 1, 80, 544, 96)
Params #:  100640
MACs:  19725440
 
[20:12:08] (INFO) Building library...
[20:12:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5154896129261364, 'median': 1.5025634765625, 'mins': 1.4495849609375}
 
(14, 3, 1, 80, 560, 48)
Params #:  76720
MACs:  15037120
 
[20:12:26] (INFO) Building library...
[20:12:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8168690074573863, 'median': 1.798828125, 'mins': 1.703857421875}
 
(14, 3, 1, 80, 560, 96)
Params #:  103600
MACs:  20305600
 
[20:12:43] (INFO) Building library...
[20:12:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.097250088778409, 'median': 2.03515625, 'mins': 1.9364013671875}
 
(14, 3, 1, 80, 576, 48)
Params #:  78912
MACs:  15466752
 
[20:13:00] (INFO) Building library...
[20:13:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7481977982954546, 'median': 1.72705078125, 'mins': 1.6466064453125}
 
(14, 3, 1, 80, 576, 96)
Params #:  106560
MACs:  20885760
 
[20:13:17] (INFO) Building library...
[20:13:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3760853160511364, 'median': 1.3658447265625, 'mins': 1.2890625}
 
(14, 3, 1, 80, 592, 48)
Params #:  81104
MACs:  15896384
 
[20:13:34] (INFO) Building library...
[20:13:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.71510009765625, 'median': 1.6944580078125, 'mins': 1.622802734375}
 
(14, 3, 1, 80, 592, 96)
Params #:  109520
MACs:  21465920
 
[20:13:51] (INFO) Building library...
[20:13:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7262839577414772, 'median': 1.7010498046875, 'mins': 1.6239013671875}
 
(14, 3, 1, 80, 608, 48)
Params #:  83296
MACs:  16326016
 
[20:14:08] (INFO) Building library...
[20:14:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8600386186079545, 'median': 1.740478515625, 'mins': 1.647705078125}
 
(14, 3, 1, 80, 608, 96)
Params #:  112480
MACs:  22046080
 
[20:14:26] (INFO) Building library...
[20:14:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8637229225852272, 'median': 1.838134765625, 'mins': 1.7584228515625}
 
(14, 3, 1, 80, 624, 48)
Params #:  85488
MACs:  16755648
 
[20:14:43] (INFO) Building library...
[20:14:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7352583451704546, 'median': 1.6986083984375, 'mins': 1.61767578125}
 
(14, 3, 1, 80, 624, 96)
Params #:  115440
MACs:  22626240
 
[20:15:00] (INFO) Building library...
[20:15:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5488791725852273, 'median': 1.5323486328125, 'mins': 1.4781494140625}
 
(14, 3, 1, 80, 640, 48)
Params #:  87680
MACs:  17185280
 
[20:15:18] (INFO) Building library...
[20:15:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7389848188920454, 'median': 1.7161865234375, 'mins': 1.6229248046875}
 
(14, 3, 1, 80, 640, 96)
Params #:  118400
MACs:  23206400
 
[20:15:35] (INFO) Building library...
[20:15:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9147560813210227, 'median': 1.767333984375, 'mins': 1.667236328125}
 
(14, 3, 1, 80, 656, 48)
Params #:  89872
MACs:  17614912
 
[20:15:53] (INFO) Building library...
[20:15:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7109763405539773, 'median': 1.6781005859375, 'mins': 1.601318359375}
 
(14, 3, 1, 80, 656, 96)
Params #:  121360
MACs:  23786560
 
[20:16:10] (INFO) Building library...
[20:16:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5456653941761365, 'median': 1.524658203125, 'mins': 1.447509765625}
 
(14, 3, 1, 80, 672, 48)
Params #:  92064
MACs:  18044544
 
[20:16:27] (INFO) Building library...
[20:16:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8759954279119317, 'median': 1.755615234375, 'mins': 1.658447265625}
 
(14, 3, 1, 80, 672, 96)
Params #:  124320
MACs:  24366720
 
[20:16:45] (INFO) Building library...
[20:16:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8563532049005682, 'median': 1.8341064453125, 'mins': 1.746337890625}
 
(14, 3, 1, 80, 688, 48)
Params #:  94256
MACs:  18474176
 
[20:17:02] (INFO) Building library...
[20:17:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8242331764914772, 'median': 1.7227783203125, 'mins': 1.6397705078125}
 
(14, 3, 1, 80, 688, 96)
Params #:  127280
MACs:  24946880
 
[20:17:20] (INFO) Building library...
[20:17:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5044777610085227, 'median': 1.496826171875, 'mins': 1.4434814453125}
 
(14, 3, 1, 80, 704, 48)
Params #:  96448
MACs:  18903808
 
[20:17:37] (INFO) Building library...
[20:17:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7271373401988637, 'median': 1.703369140625, 'mins': 1.62451171875}
 
(14, 3, 1, 80, 704, 96)
Params #:  130240
MACs:  25527040
 
[20:17:54] (INFO) Building library...
[20:17:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8503972833806819, 'median': 1.7164306640625, 'mins': 1.6243896484375}
 
(14, 5, 2, 96, 16, 96)
Params #:  3472
MACs:  395920
 
[20:18:12] (INFO) Building library...
[20:18:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.346137029474432, 'median': 1.32568359375, 'mins': 1.25537109375}
 
(14, 5, 2, 96, 16, 192)
Params #:  5008
MACs:  471184
 
[20:18:29] (INFO) Building library...
[20:18:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3819713245738636, 'median': 1.3631591796875, 'mins': 1.2908935546875}
 
(14, 5, 2, 96, 32, 96)
Params #:  6944
MACs:  791840
 
[20:18:46] (INFO) Building library...
[20:18:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3563643022017045, 'median': 1.3521728515625, 'mins': 1.292724609375}
 
(14, 5, 2, 96, 32, 192)
Params #:  10016
MACs:  942368
 
[20:19:04] (INFO) Building library...
[20:19:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3287364612926136, 'median': 1.3270263671875, 'mins': 1.272705078125}
 
(14, 5, 2, 96, 48, 96)
Params #:  10416
MACs:  1187760
 
[20:19:22] (INFO) Building library...
[20:19:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4742731267755682, 'median': 1.46240234375, 'mins': 1.41162109375}
 
(14, 5, 2, 96, 48, 192)
Params #:  15024
MACs:  1413552
 
[20:19:39] (INFO) Building library...
[20:19:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4993796608664773, 'median': 1.4915771484375, 'mins': 1.432861328125}
 
(14, 5, 2, 96, 64, 96)
Params #:  13888
MACs:  1583680
 
[20:19:56] (INFO) Building library...
[20:19:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6956787109375, 'median': 1.6673583984375, 'mins': 1.58837890625}
 
(14, 5, 2, 96, 64, 192)
Params #:  20032
MACs:  1884736
 
[20:20:13] (INFO) Building library...
[20:20:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.643612393465909, 'median': 1.6363525390625, 'mins': 1.56005859375}
 
(14, 5, 2, 96, 80, 96)
Params #:  17360
MACs:  1979600
 
[20:20:31] (INFO) Building library...
[20:20:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.512505548650568, 'median': 1.4923095703125, 'mins': 1.421875}
 
(14, 5, 2, 96, 80, 192)
Params #:  25040
MACs:  2355920
 
[20:20:48] (INFO) Building library...
[20:20:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5073286576704545, 'median': 1.493896484375, 'mins': 1.4072265625}
 
(14, 5, 2, 96, 112, 96)
Params #:  24304
MACs:  2771440
 
[20:21:05] (INFO) Building library...
[20:21:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6752374822443181, 'median': 1.6424560546875, 'mins': 1.5654296875}
 
(14, 5, 2, 96, 112, 192)
Params #:  35056
MACs:  3298288
 
[20:21:23] (INFO) Building library...
[20:21:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6316816850142046, 'median': 1.621337890625, 'mins': 1.554931640625}
 
(14, 5, 2, 96, 128, 96)
Params #:  27776
MACs:  3167360
 
[20:21:40] (INFO) Building library...
[20:21:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5103271484375, 'median': 1.484375, 'mins': 1.4091796875}
 
(14, 5, 2, 96, 128, 192)
Params #:  40064
MACs:  3769472
 
[20:21:58] (INFO) Building library...
[20:21:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6166637073863637, 'median': 1.605712890625, 'mins': 1.5369873046875}
 
(14, 5, 2, 96, 144, 96)
Params #:  31248
MACs:  3563280
 
[20:22:15] (INFO) Building library...
[20:22:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6697265625, 'median': 1.642822265625, 'mins': 1.550537109375}
 
(14, 5, 2, 96, 144, 192)
Params #:  45072
MACs:  4240656
 
[20:22:32] (INFO) Building library...
[20:22:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6714754971590908, 'median': 1.6456298828125, 'mins': 1.5723876953125}
 
(14, 5, 2, 96, 160, 96)
Params #:  34720
MACs:  3959200
 
[20:22:50] (INFO) Building library...
[20:22:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7192105379971592, 'median': 1.697265625, 'mins': 1.6099853515625}
 
(14, 5, 2, 96, 160, 192)
Params #:  50080
MACs:  4711840
 
[20:23:07] (INFO) Building library...
[20:23:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6772782759232954, 'median': 1.645751953125, 'mins': 1.570068359375}
 
(14, 5, 2, 96, 176, 96)
Params #:  38192
MACs:  4355120
 
[20:23:25] (INFO) Building library...
[20:23:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5335582386363635, 'median': 1.5211181640625, 'mins': 1.45947265625}
 
(14, 5, 2, 96, 176, 192)
Params #:  55088
MACs:  5183024
 
[20:23:42] (INFO) Building library...
[20:23:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.896337890625, 'median': 1.860595703125, 'mins': 1.752197265625}
 
(14, 5, 2, 96, 192, 96)
Params #:  41664
MACs:  4751040
 
[20:23:59] (INFO) Building library...
[20:23:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6527976296164772, 'median': 1.645263671875, 'mins': 1.58544921875}
 
(14, 5, 2, 96, 192, 192)
Params #:  60096
MACs:  5654208
 
[20:24:17] (INFO) Building library...
[20:24:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7427634499289772, 'median': 1.7200927734375, 'mins': 1.6246337890625}
 
(14, 5, 2, 96, 208, 96)
Params #:  45136
MACs:  5146960
 
[20:24:34] (INFO) Building library...
[20:24:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5265647194602272, 'median': 1.509521484375, 'mins': 1.4337158203125}
 
(14, 5, 2, 96, 208, 192)
Params #:  65104
MACs:  6125392
 
[20:24:52] (INFO) Building library...
[20:24:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6420598810369318, 'median': 1.6297607421875, 'mins': 1.566162109375}
 
(14, 5, 2, 96, 224, 96)
Params #:  48608
MACs:  5542880
 
[20:25:09] (INFO) Building library...
[20:25:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6724931196732955, 'median': 1.6441650390625, 'mins': 1.5592041015625}
 
(14, 5, 2, 96, 224, 192)
Params #:  70112
MACs:  6596576
 
[20:25:26] (INFO) Building library...
[20:25:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6652110706676135, 'median': 1.633544921875, 'mins': 1.5775146484375}
 
(14, 5, 2, 96, 240, 96)
Params #:  52080
MACs:  5938800
 
[20:25:44] (INFO) Building library...
[20:25:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6787342418323863, 'median': 1.6632080078125, 'mins': 1.5908203125}
 
(14, 5, 2, 96, 240, 192)
Params #:  75120
MACs:  7067760
 
[20:26:01] (INFO) Building library...
[20:26:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7489501953125, 'median': 1.7205810546875, 'mins': 1.64892578125}
 
(14, 5, 2, 96, 256, 96)
Params #:  55552
MACs:  6334720
 
[20:26:19] (INFO) Building library...
[20:26:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.93154296875, 'median': 1.876953125, 'mins': 1.7816162109375}
 
(14, 5, 2, 96, 256, 192)
Params #:  80128
MACs:  7538944
 
[20:26:36] (INFO) Building library...
[20:26:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6706776012073863, 'median': 1.64990234375, 'mins': 1.5697021484375}
 
(14, 5, 2, 96, 272, 96)
Params #:  59024
MACs:  6730640
 
[20:26:54] (INFO) Building library...
[20:26:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4906116832386365, 'median': 1.480712890625, 'mins': 1.4141845703125}
 
(14, 5, 2, 96, 272, 192)
Params #:  85136
MACs:  8010128
 
[20:27:11] (INFO) Building library...
[20:27:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8774591619318182, 'median': 1.84423828125, 'mins': 1.752197265625}
 
(14, 5, 2, 96, 288, 96)
Params #:  62496
MACs:  7126560
 
[20:27:28] (INFO) Building library...
[20:27:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7465742631392045, 'median': 1.7091064453125, 'mins': 1.641357421875}
 
(14, 5, 2, 96, 288, 192)
Params #:  90144
MACs:  8481312
 
[20:27:46] (INFO) Building library...
[20:27:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.688736239346591, 'median': 1.6644287109375, 'mins': 1.579833984375}
 
(14, 5, 2, 96, 304, 96)
Params #:  65968
MACs:  7522480
 
[20:28:03] (INFO) Building library...
[20:28:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6885098544034092, 'median': 1.6619873046875, 'mins': 1.5792236328125}
 
(14, 5, 2, 96, 304, 192)
Params #:  95152
MACs:  8952496
 
[20:28:21] (INFO) Building library...
[20:28:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4853105024857955, 'median': 1.4638671875, 'mins': 1.3941650390625}
 
(14, 5, 2, 96, 320, 96)
Params #:  69440
MACs:  7918400
 
[20:28:38] (INFO) Building library...
[20:28:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5208052201704545, 'median': 1.500732421875, 'mins': 1.4202880859375}
 
(14, 5, 2, 96, 320, 192)
Params #:  100160
MACs:  9423680
 
[20:28:56] (INFO) Building library...
[20:28:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.659173029119318, 'median': 1.650146484375, 'mins': 1.580078125}
 
(14, 5, 2, 96, 336, 96)
Params #:  72912
MACs:  8314320
 
[20:29:13] (INFO) Building library...
[20:29:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6953469016335228, 'median': 1.6734619140625, 'mins': 1.5982666015625}
 
(14, 5, 2, 96, 336, 192)
Params #:  105168
MACs:  9894864
 
[20:29:31] (INFO) Building library...
[20:29:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5054853959517045, 'median': 1.486328125, 'mins': 1.416015625}
 
(14, 5, 2, 96, 352, 96)
Params #:  76384
MACs:  8710240
 
[20:29:48] (INFO) Building library...
[20:29:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6802756569602273, 'median': 1.6539306640625, 'mins': 1.5697021484375}
 
(14, 5, 2, 96, 352, 192)
Params #:  110176
MACs:  10366048
 
[20:30:06] (INFO) Building library...
[20:30:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6903830788352272, 'median': 1.6715087890625, 'mins': 1.5921630859375}
 
(14, 5, 2, 96, 368, 96)
Params #:  79856
MACs:  9106160
 
[20:30:23] (INFO) Building library...
[20:30:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6964488636363637, 'median': 1.67822265625, 'mins': 1.5968017578125}
 
(14, 5, 2, 96, 368, 192)
Params #:  115184
MACs:  10837232
 
[20:30:41] (INFO) Building library...
[20:30:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.724063387784091, 'median': 1.6953125, 'mins': 1.6190185546875}
 
(14, 5, 2, 96, 384, 96)
Params #:  83328
MACs:  9502080
 
[20:30:58] (INFO) Building library...
[20:30:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7015691583806818, 'median': 1.6783447265625, 'mins': 1.58837890625}
 
(14, 5, 2, 96, 384, 192)
Params #:  120192
MACs:  11308416
 
[20:31:16] (INFO) Building library...
[20:31:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.787472256747159, 'median': 1.7763671875, 'mins': 1.702880859375}
 
(14, 5, 2, 96, 400, 96)
Params #:  86800
MACs:  9898000
 
[20:31:33] (INFO) Building library...
[20:31:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6703047318892046, 'median': 1.6485595703125, 'mins': 1.56689453125}
 
(14, 5, 2, 96, 400, 192)
Params #:  125200
MACs:  11779600
 
[20:31:51] (INFO) Building library...
[20:31:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6719171697443183, 'median': 1.6434326171875, 'mins': 1.561767578125}
 
(14, 5, 2, 96, 416, 96)
Params #:  90272
MACs:  10293920
 
[20:32:08] (INFO) Building library...
[20:32:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5114235617897727, 'median': 1.484130859375, 'mins': 1.4166259765625}
 
(14, 5, 2, 96, 416, 192)
Params #:  130208
MACs:  12250784
 
[20:32:26] (INFO) Building library...
[20:32:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5052345969460228, 'median': 1.492919921875, 'mins': 1.4249267578125}
 
(14, 5, 2, 96, 432, 96)
Params #:  93744
MACs:  10689840
 
[20:32:43] (INFO) Building library...
[20:32:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5758977716619318, 'median': 1.5523681640625, 'mins': 1.451416015625}
 
(14, 5, 2, 96, 432, 192)
Params #:  135216
MACs:  12721968
 
[20:33:01] (INFO) Building library...
[20:33:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.46212158203125, 'median': 1.44970703125, 'mins': 1.3880615234375}
 
(14, 5, 2, 96, 448, 96)
Params #:  97216
MACs:  11085760
 
[20:33:18] (INFO) Building library...
[20:33:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.81688232421875, 'median': 1.8228759765625, 'mins': 1.5982666015625}
 
(14, 5, 2, 96, 448, 192)
Params #:  140224
MACs:  13193152
 
[20:33:36] (INFO) Building library...
[20:33:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8760276100852273, 'median': 1.847900390625, 'mins': 1.769287109375}
 
(14, 5, 2, 96, 464, 96)
Params #:  100688
MACs:  11481680
 
[20:33:54] (INFO) Building library...
[20:33:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6674172141335226, 'median': 1.6434326171875, 'mins': 1.57275390625}
 
(14, 5, 2, 96, 464, 192)
Params #:  145232
MACs:  13664336
 
[20:34:11] (INFO) Building library...
[20:34:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8423162286931818, 'median': 1.83203125, 'mins': 1.7498779296875}
 
(14, 5, 2, 96, 480, 96)
Params #:  104160
MACs:  11877600
 
[20:34:28] (INFO) Building library...
[20:34:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7495738636363636, 'median': 1.7259521484375, 'mins': 1.6328125}
 
(14, 5, 2, 96, 480, 192)
Params #:  150240
MACs:  14135520
 
[20:34:46] (INFO) Building library...
[20:34:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8602927468039774, 'median': 1.8193359375, 'mins': 1.7322998046875}
 
(14, 5, 2, 96, 496, 96)
Params #:  107632
MACs:  12273520
 
[20:35:04] (INFO) Building library...
[20:35:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.722103604403409, 'median': 1.6983642578125, 'mins': 1.611328125}
 
(14, 5, 2, 96, 496, 192)
Params #:  155248
MACs:  14606704
 
[20:35:21] (INFO) Building library...
[20:35:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8385187322443182, 'median': 1.822265625, 'mins': 1.755126953125}
 
(14, 5, 2, 96, 512, 96)
Params #:  111104
MACs:  12669440
 
[20:35:38] (INFO) Building library...
[20:35:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8661865234375, 'median': 1.841796875, 'mins': 1.74365234375}
 
(14, 5, 2, 96, 512, 192)
Params #:  160256
MACs:  15077888
 
[20:35:56] (INFO) Building library...
[20:35:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6856245561079546, 'median': 1.6806640625, 'mins': 1.615234375}
 
(14, 5, 2, 96, 528, 96)
Params #:  114576
MACs:  13065360
 
[20:36:13] (INFO) Building library...
[20:36:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7030528675426135, 'median': 1.6805419921875, 'mins': 1.597900390625}
 
(14, 5, 2, 96, 528, 192)
Params #:  165264
MACs:  15549072
 
[20:36:31] (INFO) Building library...
[20:36:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7406216708096591, 'median': 1.7059326171875, 'mins': 1.626708984375}
 
(14, 5, 2, 96, 544, 96)
Params #:  118048
MACs:  13461280
 
[20:36:48] (INFO) Building library...
[20:36:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8380049272017045, 'median': 1.8243408203125, 'mins': 1.7406005859375}
 
(14, 5, 2, 96, 544, 192)
Params #:  170272
MACs:  16020256
 
[20:37:06] (INFO) Building library...
[20:37:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8899347478693183, 'median': 1.8575439453125, 'mins': 1.758544921875}
 
(14, 5, 2, 96, 560, 96)
Params #:  121520
MACs:  13857200
 
[20:37:23] (INFO) Building library...
[20:37:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.631399813565341, 'median': 1.625, 'mins': 1.554931640625}
 
(14, 5, 2, 96, 560, 192)
Params #:  175280
MACs:  16491440
 
[20:37:41] (INFO) Building library...
[20:37:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.99600830078125, 'median': 1.8509521484375, 'mins': 1.7493896484375}
 
(14, 5, 2, 96, 576, 96)
Params #:  124992
MACs:  14253120
 
[20:37:58] (INFO) Building library...
[20:37:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4020075017755682, 'median': 1.3837890625, 'mins': 1.3123779296875}
 
(14, 5, 2, 96, 576, 192)
Params #:  180288
MACs:  16962624
 
[20:38:16] (INFO) Building library...
[20:38:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.385504705255682, 'median': 1.373046875, 'mins': 1.2987060546875}
 
(14, 5, 2, 96, 592, 96)
Params #:  128464
MACs:  14649040
 
[20:38:34] (INFO) Building library...
[20:38:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7225519353693182, 'median': 1.697265625, 'mins': 1.61669921875}
 
(14, 5, 2, 96, 592, 192)
Params #:  185296
MACs:  17433808
 
[20:38:51] (INFO) Building library...
[20:38:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6803910688920454, 'median': 1.6702880859375, 'mins': 1.6058349609375}
 
(14, 5, 2, 96, 608, 96)
Params #:  131936
MACs:  15044960
 
[20:39:08] (INFO) Building library...
[20:39:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7124644886363636, 'median': 1.6844482421875, 'mins': 1.6011962890625}
 
(14, 5, 2, 96, 608, 192)
Params #:  190304
MACs:  17904992
 
[20:39:26] (INFO) Building library...
[20:39:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7484841086647727, 'median': 1.726318359375, 'mins': 1.6578369140625}
 
(14, 5, 2, 96, 624, 96)
Params #:  135408
MACs:  15440880
 
[20:39:44] (INFO) Building library...
[20:39:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.009412730823864, 'median': 1.8531494140625, 'mins': 1.7611083984375}
 
(14, 5, 2, 96, 624, 192)
Params #:  195312
MACs:  18376176
 
[20:40:01] (INFO) Building library...
[20:40:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0161232688210227, 'median': 1.97265625, 'mins': 1.5904541015625}
 
(14, 5, 2, 96, 640, 96)
Params #:  138880
MACs:  15836800
 
[20:40:19] (INFO) Building library...
[20:40:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7414883700284092, 'median': 1.728515625, 'mins': 1.66650390625}
 
(14, 5, 2, 96, 640, 192)
Params #:  200320
MACs:  18847360
 
[20:40:37] (INFO) Building library...
[20:40:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5401966441761363, 'median': 1.5098876953125, 'mins': 1.4388427734375}
 
(14, 5, 2, 96, 656, 96)
Params #:  142352
MACs:  16232720
 
[20:40:54] (INFO) Building library...
[20:40:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.929812899502841, 'median': 1.88232421875, 'mins': 1.7977294921875}
 
(14, 5, 2, 96, 656, 192)
Params #:  205328
MACs:  19318544
 
[20:41:12] (INFO) Building library...
[20:41:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5224509499289773, 'median': 1.5013427734375, 'mins': 1.4365234375}
 
(14, 5, 2, 96, 672, 96)
Params #:  145824
MACs:  16628640
 
[20:41:29] (INFO) Building library...
[20:41:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.690194424715909, 'median': 1.67529296875, 'mins': 1.6075439453125}
 
(14, 5, 2, 96, 672, 192)
Params #:  210336
MACs:  19789728
 
[20:41:47] (INFO) Building library...
[20:41:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9046242453835227, 'median': 1.7352294921875, 'mins': 1.6243896484375}
 
(14, 5, 2, 96, 688, 96)
Params #:  149296
MACs:  17024560
 
[20:42:04] (INFO) Building library...
[20:42:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8591741388494318, 'median': 1.8533935546875, 'mins': 1.7021484375}
 
(14, 5, 2, 96, 688, 192)
Params #:  215344
MACs:  20260912
 
[20:42:22] (INFO) Building library...
[20:42:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.86513671875, 'median': 1.8349609375, 'mins': 1.7606201171875}
 
(14, 5, 2, 96, 704, 96)
Params #:  152768
MACs:  17420480
 
[20:42:39] (INFO) Building library...
[20:42:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6817205255681817, 'median': 1.673828125, 'mins': 1.6065673828125}
 
(14, 5, 2, 96, 704, 192)
Params #:  220352
MACs:  20732096
 
[20:42:57] (INFO) Building library...
[20:42:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7388649680397728, 'median': 1.710205078125, 'mins': 1.6356201171875}
 
(14, 5, 2, 96, 720, 96)
Params #:  156240
MACs:  17816400
 
[20:43:14] (INFO) Building library...
[20:43:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8279951615767045, 'median': 1.81982421875, 'mins': 1.752197265625}
 
(14, 5, 2, 96, 720, 192)
Params #:  225360
MACs:  21203280
 
[20:43:32] (INFO) Building library...
[20:43:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2083773526278407, 'median': 2.0477294921875, 'mins': 1.7891845703125}
 
(14, 5, 2, 96, 736, 96)
Params #:  159712
MACs:  18212320
 
[20:43:49] (INFO) Building library...
[20:43:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.690433016690341, 'median': 1.680908203125, 'mins': 1.6029052734375}
 
(14, 5, 2, 96, 736, 192)
Params #:  230368
MACs:  21674464
 
[20:44:07] (INFO) Building library...
[20:44:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8920032848011363, 'median': 1.760498046875, 'mins': 1.6624755859375}
 
(14, 5, 2, 96, 752, 96)
Params #:  163184
MACs:  18608240
 
[20:44:25] (INFO) Building library...
[20:44:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7237448952414773, 'median': 1.5374755859375, 'mins': 1.442626953125}
 
(14, 5, 2, 96, 752, 192)
Params #:  235376
MACs:  22145648
 
[20:44:42] (INFO) Building library...
[20:44:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4944324840198864, 'median': 1.4852294921875, 'mins': 1.420654296875}
 
(14, 5, 2, 96, 768, 96)
Params #:  166656
MACs:  19004160
 
[20:45:00] (INFO) Building library...
[20:45:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8854703036221592, 'median': 1.856201171875, 'mins': 1.76806640625}
 
(14, 5, 2, 96, 768, 192)
Params #:  240384
MACs:  22616832
 
[20:45:17] (INFO) Building library...
[20:45:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.74324951171875, 'median': 1.712646484375, 'mins': 1.62451171875}
 
(14, 5, 2, 96, 784, 96)
Params #:  170128
MACs:  19400080
 
[20:45:35] (INFO) Building library...
[20:45:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7029518821022727, 'median': 1.6715087890625, 'mins': 1.6014404296875}
 
(14, 5, 2, 96, 784, 192)
Params #:  245392
MACs:  23088016
 
[20:45:53] (INFO) Building library...
[20:45:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8604148171164774, 'median': 1.8287353515625, 'mins': 1.746826171875}
 
(14, 5, 2, 96, 800, 96)
Params #:  173600
MACs:  19796000
 
[20:46:10] (INFO) Building library...
[20:46:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7311279296875, 'median': 1.6947021484375, 'mins': 1.6151123046875}
 
(14, 5, 2, 96, 800, 192)
Params #:  250400
MACs:  23559200
 
[20:46:28] (INFO) Building library...
[20:46:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0230446555397728, 'median': 1.988525390625, 'mins': 1.628173828125}
 
(14, 5, 2, 96, 816, 96)
Params #:  177072
MACs:  20191920
 
[20:46:45] (INFO) Building library...
[20:46:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8399036754261364, 'median': 1.822998046875, 'mins': 1.7479248046875}
 
(14, 5, 2, 96, 816, 192)
Params #:  255408
MACs:  24030384
 
[20:47:03] (INFO) Building library...
[20:47:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9030362215909091, 'median': 1.76513671875, 'mins': 1.664306640625}
 
(14, 5, 2, 96, 832, 96)
Params #:  180544
MACs:  20587840
 
[20:47:21] (INFO) Building library...
[20:47:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.88614501953125, 'median': 1.744873046875, 'mins': 1.6513671875}
 
(14, 5, 2, 96, 832, 192)
Params #:  260416
MACs:  24501568
 
[20:47:39] (INFO) Building library...
[20:47:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8398659446022727, 'median': 1.77197265625, 'mins': 1.66015625}
 
(14, 5, 2, 96, 848, 96)
Params #:  184016
MACs:  20983760
 
[20:47:57] (INFO) Building library...
[20:47:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6698919122869318, 'median': 1.6649169921875, 'mins': 1.593505859375}
 
(14, 5, 2, 96, 848, 192)
Params #:  265424
MACs:  24972752
 
[20:48:14] (INFO) Building library...
[20:48:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9593650124289772, 'median': 1.8995361328125, 'mins': 1.8031005859375}
 
(7, 5, 1, 192, 16, 96)
Params #:  5008
MACs:  245392
 
[20:48:32] (INFO) Building library...
[20:48:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6562855113636363, 'median': 1.621337890625, 'mins': 1.55029296875}
 
(7, 5, 1, 192, 16, 192)
Params #:  6544
MACs:  320656
 
[20:48:49] (INFO) Building library...
[20:48:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2818503639914773, 'median': 2.19677734375, 'mins': 2.110107421875}
 
(7, 5, 1, 192, 32, 96)
Params #:  10016
MACs:  490784
 
[20:49:08] (INFO) Building library...
[20:49:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4472256747159091, 'median': 1.4365234375, 'mins': 1.3717041015625}
 
(7, 5, 1, 192, 32, 192)
Params #:  13088
MACs:  641312
 
[20:49:25] (INFO) Building library...
[20:49:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3207053444602272, 'median': 2.292236328125, 'mins': 2.17578125}
 
(7, 5, 1, 192, 48, 96)
Params #:  15024
MACs:  736176
 
[20:49:44] (INFO) Building library...
[20:49:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5124334161931818, 'median': 1.4947509765625, 'mins': 1.426513671875}
 
(7, 5, 1, 192, 48, 192)
Params #:  19632
MACs:  961968
 
[20:50:01] (INFO) Building library...
[20:50:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.174658203125, 'median': 2.1507568359375, 'mins': 2.0770263671875}
 
(7, 5, 1, 192, 64, 96)
Params #:  20032
MACs:  981568
 
[20:50:20] (INFO) Building library...
[20:50:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.639899236505682, 'median': 1.6158447265625, 'mins': 1.541015625}
 
(7, 5, 1, 192, 64, 192)
Params #:  26176
MACs:  1282624
 
[20:50:37] (INFO) Building library...
[20:50:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6630748401988638, 'median': 2.6107177734375, 'mins': 2.504150390625}
 
(7, 5, 1, 192, 80, 96)
Params #:  25040
MACs:  1226960
 
[20:50:57] (INFO) Building library...
[20:50:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.46558837890625, 'median': 1.4564208984375, 'mins': 1.4029541015625}
 
(7, 5, 1, 192, 80, 192)
Params #:  32720
MACs:  1603280
 
[20:51:14] (INFO) Building library...
[20:51:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4345348011363637, 'median': 2.4039306640625, 'mins': 2.30029296875}
 
(7, 5, 1, 192, 96, 96)
Params #:  30048
MACs:  1472352
 
[20:51:33] (INFO) Building library...
[20:51:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5151622425426137, 'median': 1.4954833984375, 'mins': 1.4232177734375}
 
(7, 5, 1, 192, 96, 192)
Params #:  39264
MACs:  1923936
 
[20:51:50] (INFO) Building library...
[20:51:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1880149147727272, 'median': 2.1649169921875, 'mins': 2.07666015625}
 
(7, 5, 1, 192, 112, 96)
Params #:  35056
MACs:  1717744
 
[20:52:09] (INFO) Building library...
[20:52:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5525068803267046, 'median': 1.520263671875, 'mins': 1.45849609375}
 
(7, 5, 1, 192, 112, 192)
Params #:  45808
MACs:  2244592
 
[20:52:26] (INFO) Building library...
[20:52:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1526478160511364, 'median': 2.115478515625, 'mins': 2.0360107421875}
 
(7, 5, 1, 192, 128, 96)
Params #:  40064
MACs:  1963136
 
[20:52:45] (INFO) Building library...
[20:52:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6564042524857954, 'median': 1.640625, 'mins': 1.5787353515625}
 
(7, 5, 1, 192, 128, 192)
Params #:  52352
MACs:  2565248
 
[20:53:02] (INFO) Building library...
[20:53:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3037974964488637, 'median': 2.2947998046875, 'mins': 2.2152099609375}
 
(7, 5, 1, 192, 144, 96)
Params #:  45072
MACs:  2208528
 
[20:53:21] (INFO) Building library...
[20:53:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5155628551136364, 'median': 1.5059814453125, 'mins': 1.44921875}
 
(7, 5, 1, 192, 144, 192)
Params #:  58896
MACs:  2885904
 
[20:53:38] (INFO) Building library...
[20:53:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1643255060369317, 'median': 2.1556396484375, 'mins': 2.0701904296875}
 
(7, 5, 1, 192, 160, 96)
Params #:  50080
MACs:  2453920
 
[20:53:57] (INFO) Building library...
[20:53:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.59332275390625, 'median': 1.5626220703125, 'mins': 1.4720458984375}
 
(7, 5, 1, 192, 160, 192)
Params #:  65440
MACs:  3206560
 
[20:54:15] (INFO) Building library...
[20:54:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1722667347301137, 'median': 2.1444091796875, 'mins': 2.06689453125}
 
(7, 5, 1, 192, 176, 96)
Params #:  55088
MACs:  2699312
 
[20:54:34] (INFO) Building library...
[20:54:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5623002485795454, 'median': 1.5433349609375, 'mins': 1.465087890625}
 
(7, 5, 1, 192, 176, 192)
Params #:  71984
MACs:  3527216
 
[20:54:51] (INFO) Building library...
[20:54:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.439932528409091, 'median': 2.4224853515625, 'mins': 2.2252197265625}
 
(7, 5, 1, 192, 208, 96)
Params #:  65104
MACs:  3190096
 
[20:55:10] (INFO) Building library...
[20:55:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.537423428622159, 'median': 1.5242919921875, 'mins': 1.4683837890625}
 
(7, 5, 1, 192, 208, 192)
Params #:  85072
MACs:  4168528
 
[20:55:27] (INFO) Building library...
[20:55:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.224509499289773, 'median': 2.203369140625, 'mins': 2.1278076171875}
 
(7, 5, 1, 192, 224, 96)
Params #:  70112
MACs:  3435488
 
[20:55:46] (INFO) Building library...
[20:55:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5991987748579546, 'median': 1.5750732421875, 'mins': 1.4940185546875}
 
(7, 5, 1, 192, 224, 192)
Params #:  91616
MACs:  4489184
 
[20:56:04] (INFO) Building library...
[20:56:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1554820667613637, 'median': 2.1256103515625, 'mins': 2.0423583984375}
 
(7, 5, 1, 192, 240, 96)
Params #:  75120
MACs:  3680880
 
[20:56:23] (INFO) Building library...
[20:56:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5663962624289773, 'median': 1.5419921875, 'mins': 1.4600830078125}
 
(7, 5, 1, 192, 240, 192)
Params #:  98160
MACs:  4809840
 
[20:56:40] (INFO) Building library...
[20:56:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.290658291903409, 'median': 2.255615234375, 'mins': 2.156494140625}
 
(7, 5, 1, 192, 256, 96)
Params #:  80128
MACs:  3926272
 
[20:56:59] (INFO) Building library...
[20:56:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.460564630681818, 'median': 1.4481201171875, 'mins': 1.3856201171875}
 
(7, 5, 1, 192, 256, 192)
Params #:  104704
MACs:  5130496
 
[20:57:16] (INFO) Building library...
[20:57:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3206121271306817, 'median': 2.317626953125, 'mins': 2.2415771484375}
 
(7, 5, 1, 192, 272, 96)
Params #:  85136
MACs:  4171664
 
[20:57:35] (INFO) Building library...
[20:57:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4827869762073864, 'median': 1.477294921875, 'mins': 1.4140625}
 
(7, 5, 1, 192, 272, 192)
Params #:  111248
MACs:  5451152
 
[20:57:52] (INFO) Building library...
[20:57:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.461663263494318, 'median': 2.4344482421875, 'mins': 2.351806640625}
 
(7, 5, 1, 192, 288, 96)
Params #:  90144
MACs:  4417056
 
[20:58:11] (INFO) Building library...
[20:58:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5670965021306817, 'median': 1.543212890625, 'mins': 1.464599609375}
 
(7, 5, 1, 192, 288, 192)
Params #:  117792
MACs:  5771808
 
[20:58:29] (INFO) Building library...
[20:58:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.239137961647727, 'median': 2.2119140625, 'mins': 2.11279296875}
 
(7, 5, 1, 192, 304, 96)
Params #:  95152
MACs:  4662448
 
[20:58:48] (INFO) Building library...
[20:58:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6486605557528409, 'median': 1.549560546875, 'mins': 1.4442138671875}
 
(7, 5, 1, 192, 304, 192)
Params #:  124336
MACs:  6092464
 
[20:59:05] (INFO) Building library...
[20:59:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2134110884232956, 'median': 2.187255859375, 'mins': 2.1075439453125}
 
(7, 5, 1, 192, 320, 96)
Params #:  100160
MACs:  4907840
 
[20:59:24] (INFO) Building library...
[20:59:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.791000088778409, 'median': 1.753173828125, 'mins': 1.6318359375}
 
(7, 5, 1, 192, 320, 192)
Params #:  130880
MACs:  6413120
 
[20:59:42] (INFO) Building library...
[20:59:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1871348987926136, 'median': 2.1494140625, 'mins': 2.06103515625}
 
(7, 5, 1, 192, 336, 96)
Params #:  105168
MACs:  5153232
 
[21:00:01] (INFO) Building library...
[21:00:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4868674538352273, 'median': 1.4658203125, 'mins': 1.398681640625}
 
(7, 5, 1, 192, 336, 192)
Params #:  137424
MACs:  6733776
 
[21:00:18] (INFO) Building library...
[21:00:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5145640980113635, 'median': 2.46044921875, 'mins': 2.352294921875}
 
(7, 5, 1, 192, 352, 96)
Params #:  110176
MACs:  5398624
 
[21:00:37] (INFO) Building library...
[21:00:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6490944602272728, 'median': 1.6409912109375, 'mins': 1.5789794921875}
 
(7, 5, 1, 192, 352, 192)
Params #:  143968
MACs:  7054432
 
[21:00:54] (INFO) Building library...
[21:00:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.188888272372159, 'median': 2.167724609375, 'mins': 2.0892333984375}
 
(7, 5, 1, 192, 368, 96)
Params #:  115184
MACs:  5644016
 
[21:01:14] (INFO) Building library...
[21:01:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.661239346590909, 'median': 1.63720703125, 'mins': 1.563720703125}
 
(7, 5, 1, 192, 368, 192)
Params #:  150512
MACs:  7375088
 
[21:01:31] (INFO) Building library...
[21:01:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3647660688920453, 'median': 2.3262939453125, 'mins': 2.2264404296875}
 
(7, 5, 1, 192, 384, 96)
Params #:  120192
MACs:  5889408
 
[21:01:50] (INFO) Building library...
[21:01:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6665860262784091, 'median': 1.6558837890625, 'mins': 1.589599609375}
 
(7, 5, 1, 192, 384, 192)
Params #:  157056
MACs:  7695744
 
[21:02:07] (INFO) Building library...
[21:02:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4386973987926135, 'median': 2.4085693359375, 'mins': 2.32080078125}
 
(7, 5, 1, 192, 400, 96)
Params #:  125200
MACs:  6134800
 
[21:02:27] (INFO) Building library...
[21:02:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6657825816761365, 'median': 1.645263671875, 'mins': 1.573486328125}
 
(7, 5, 1, 192, 400, 192)
Params #:  163600
MACs:  8016400
 
[21:02:44] (INFO) Building library...
[21:02:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.367006613991477, 'median': 2.335205078125, 'mins': 2.2362060546875}
 
(7, 5, 1, 192, 416, 96)
Params #:  130208
MACs:  6380192
 
[21:03:03] (INFO) Building library...
[21:03:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6619495738636363, 'median': 1.656494140625, 'mins': 1.590576171875}
 
(7, 5, 1, 192, 416, 192)
Params #:  170144
MACs:  8337056
 
[21:03:21] (INFO) Building library...
[21:03:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4299460671164774, 'median': 2.3848876953125, 'mins': 2.28466796875}
 
(7, 5, 1, 192, 432, 96)
Params #:  135216
MACs:  6625584
 
[21:03:40] (INFO) Building library...
[21:03:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.682083407315341, 'median': 1.6517333984375, 'mins': 1.571533203125}
 
(7, 5, 1, 192, 432, 192)
Params #:  176688
MACs:  8657712
 
[21:03:57] (INFO) Building library...
[21:03:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.382644930752841, 'median': 2.372314453125, 'mins': 2.2930908203125}
 
(7, 5, 1, 192, 448, 96)
Params #:  140224
MACs:  6870976
 
[21:04:17] (INFO) Building library...
[21:04:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6555431019176137, 'median': 1.629638671875, 'mins': 1.5523681640625}
 
(7, 5, 1, 192, 448, 192)
Params #:  183232
MACs:  8978368
 
[21:04:34] (INFO) Building library...
[21:04:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5454767400568183, 'median': 2.5020751953125, 'mins': 2.405029296875}
 
(7, 5, 1, 192, 464, 96)
Params #:  145232
MACs:  7116368
 
[21:04:53] (INFO) Building library...
[21:04:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.466986638849432, 'median': 1.4510498046875, 'mins': 1.379150390625}
 
(7, 5, 1, 192, 464, 192)
Params #:  189776
MACs:  9299024
 
[21:05:10] (INFO) Building library...
[21:05:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.600613680752841, 'median': 2.552490234375, 'mins': 2.4320068359375}
 
(7, 5, 1, 192, 480, 96)
Params #:  150240
MACs:  7361760
 
[21:05:29] (INFO) Building library...
[21:05:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8334039861505682, 'median': 1.801025390625, 'mins': 1.720947265625}
 
(7, 5, 1, 192, 480, 192)
Params #:  196320
MACs:  9619680
 
[21:05:47] (INFO) Building library...
[21:05:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3456309925426138, 'median': 2.3145751953125, 'mins': 2.218994140625}
 
(7, 5, 1, 192, 496, 96)
Params #:  155248
MACs:  7607152
 
[21:06:06] (INFO) Building library...
[21:06:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5013993696732955, 'median': 1.4803466796875, 'mins': 1.4139404296875}
 
(7, 5, 1, 192, 496, 192)
Params #:  202864
MACs:  9940336
 
[21:06:23] (INFO) Building library...
[21:06:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2900346235795452, 'median': 2.2822265625, 'mins': 2.1949462890625}
 
(7, 5, 1, 192, 512, 96)
Params #:  160256
MACs:  7852544
 
[21:06:43] (INFO) Building library...
[21:06:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6620660955255682, 'median': 1.63037109375, 'mins': 1.5445556640625}
 
(7, 5, 1, 192, 512, 192)
Params #:  209408
MACs:  10260992
 
[21:07:00] (INFO) Building library...
[21:07:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1398415305397727, 'median': 2.118408203125, 'mins': 2.0367431640625}
 
(7, 5, 1, 192, 528, 96)
Params #:  165264
MACs:  8097936
 
[21:07:19] (INFO) Building library...
[21:07:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8978371360085227, 'median': 1.7916259765625, 'mins': 1.6748046875}
 
(7, 5, 1, 192, 528, 192)
Params #:  215952
MACs:  10581648
 
[21:07:37] (INFO) Building library...
[21:07:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5377163973721593, 'median': 2.50732421875, 'mins': 2.3875732421875}
 
(7, 5, 1, 192, 544, 96)
Params #:  170272
MACs:  8343328
 
[21:07:56] (INFO) Building library...
[21:07:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4440895774147726, 'median': 1.433837890625, 'mins': 1.3797607421875}
 
(7, 5, 1, 192, 544, 192)
Params #:  222496
MACs:  10902304
 
[21:08:14] (INFO) Building library...
[21:08:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.368935324928977, 'median': 2.3272705078125, 'mins': 2.2310791015625}
 
(7, 5, 1, 192, 560, 96)
Params #:  175280
MACs:  8588720
 
[21:08:33] (INFO) Building library...
[21:08:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8180675159801136, 'median': 1.810302734375, 'mins': 1.7381591796875}
 
(7, 5, 1, 192, 560, 192)
Params #:  229040
MACs:  11222960
 
[21:08:51] (INFO) Building library...
[21:08:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3299904563210228, 'median': 2.316650390625, 'mins': 2.2283935546875}
 
(7, 5, 1, 192, 576, 96)
Params #:  180288
MACs:  8834112
 
[21:09:10] (INFO) Building library...
[21:09:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9044089577414773, 'median': 1.7855224609375, 'mins': 1.6929931640625}
 
(7, 5, 1, 192, 576, 192)
Params #:  235584
MACs:  11543616
 
[21:09:28] (INFO) Building library...
[21:09:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.458387340198864, 'median': 2.447021484375, 'mins': 2.3721923828125}
 
(7, 5, 1, 192, 592, 96)
Params #:  185296
MACs:  9079504
 
[21:09:47] (INFO) Building library...
[21:09:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8046120383522728, 'median': 1.7703857421875, 'mins': 1.6845703125}
 
(7, 5, 1, 192, 592, 192)
Params #:  242128
MACs:  11864272
 
[21:10:04] (INFO) Building library...
[21:10:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.352141779119318, 'median': 2.335693359375, 'mins': 2.2388916015625}
 
(7, 5, 1, 192, 608, 96)
Params #:  190304
MACs:  9324896
 
[21:10:24] (INFO) Building library...
[21:10:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66019287109375, 'median': 1.6331787109375, 'mins': 1.5474853515625}
 
(7, 5, 1, 192, 608, 192)
Params #:  248672
MACs:  12184928
 
[21:10:41] (INFO) Building library...
[21:10:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1632490678267047, 'median': 2.13818359375, 'mins': 2.0477294921875}
 
(7, 5, 1, 192, 624, 96)
Params #:  195312
MACs:  9570288
 
[21:11:00] (INFO) Building library...
[21:11:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6490034623579546, 'median': 1.6431884765625, 'mins': 1.5694580078125}
 
(7, 5, 1, 192, 624, 192)
Params #:  255216
MACs:  12505584
 
[21:11:18] (INFO) Building library...
[21:11:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3801103071732954, 'median': 2.349365234375, 'mins': 2.261962890625}
 
(7, 5, 1, 192, 640, 96)
Params #:  200320
MACs:  9815680
 
[21:11:37] (INFO) Building library...
[21:11:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6781316583806818, 'median': 1.662353515625, 'mins': 1.5966796875}
 
(7, 5, 1, 192, 640, 192)
Params #:  261760
MACs:  12826240
 
[21:11:55] (INFO) Building library...
[21:11:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3821244673295454, 'median': 2.364013671875, 'mins': 2.26806640625}
 
(7, 5, 1, 192, 656, 96)
Params #:  205328
MACs:  10061072
 
[21:12:14] (INFO) Building library...
[21:12:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.65294189453125, 'median': 1.62841796875, 'mins': 1.5450439453125}
 
(7, 5, 1, 192, 656, 192)
Params #:  268304
MACs:  13146896
 
[21:12:31] (INFO) Building library...
[21:12:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4262351296164772, 'median': 2.388916015625, 'mins': 2.2977294921875}
 
(7, 5, 1, 192, 672, 96)
Params #:  210336
MACs:  10306464
 
[21:12:51] (INFO) Building library...
[21:12:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6830943714488635, 'median': 1.65966796875, 'mins': 1.58740234375}
 
(7, 5, 1, 192, 672, 192)
Params #:  274848
MACs:  13467552
 
[21:13:08] (INFO) Building library...
[21:13:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5505570845170453, 'median': 2.5169677734375, 'mins': 2.42822265625}
 
(7, 5, 1, 192, 688, 96)
Params #:  215344
MACs:  10551856
 
[21:13:28] (INFO) Building library...
[21:13:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6150190873579546, 'median': 1.6016845703125, 'mins': 1.5445556640625}
 
(7, 5, 1, 192, 688, 192)
Params #:  281392
MACs:  13788208
 
[21:13:45] (INFO) Building library...
[21:13:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.358294122869318, 'median': 2.3277587890625, 'mins': 2.23486328125}
 
(7, 5, 1, 192, 704, 96)
Params #:  220352
MACs:  10797248
 
[21:14:04] (INFO) Building library...
[21:14:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.784613591974432, 'median': 1.7645263671875, 'mins': 1.700439453125}
 
(7, 5, 1, 192, 704, 192)
Params #:  287936
MACs:  14108864
 
[21:14:22] (INFO) Building library...
[21:14:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5052745472301137, 'median': 2.4781494140625, 'mins': 2.3828125}
 
(7, 5, 1, 192, 720, 96)
Params #:  225360
MACs:  11042640
 
[21:14:41] (INFO) Building library...
[21:14:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6535955255681818, 'median': 1.6298828125, 'mins': 1.5504150390625}
 
(7, 5, 1, 192, 720, 192)
Params #:  294480
MACs:  14429520
 
[21:14:58] (INFO) Building library...
[21:14:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4466441761363638, 'median': 2.4169921875, 'mins': 2.3046875}
 
(7, 5, 1, 192, 736, 96)
Params #:  230368
MACs:  11288032
 
[21:15:18] (INFO) Building library...
[21:15:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8958263050426136, 'median': 1.845703125, 'mins': 1.7498779296875}
 
(7, 5, 1, 192, 736, 192)
Params #:  301024
MACs:  14750176
 
[21:15:35] (INFO) Building library...
[21:15:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3696244673295452, 'median': 2.33544921875, 'mins': 2.2388916015625}
 
(7, 5, 1, 192, 752, 96)
Params #:  235376
MACs:  11533424
 
[21:15:55] (INFO) Building library...
[21:15:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6953591086647728, 'median': 1.659912109375, 'mins': 1.574462890625}
 
(7, 5, 1, 192, 752, 192)
Params #:  307568
MACs:  15070832
 
[21:16:12] (INFO) Building library...
[21:16:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3325838955965907, 'median': 2.3187255859375, 'mins': 2.240234375}
 
(7, 5, 1, 192, 768, 96)
Params #:  240384
MACs:  11778816
 
[21:16:31] (INFO) Building library...
[21:16:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4789683948863637, 'median': 1.4566650390625, 'mins': 1.3797607421875}
 
(7, 5, 1, 192, 768, 192)
Params #:  314112
MACs:  15391488
 
[21:16:48] (INFO) Building library...
[21:16:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4734375, 'median': 2.429931640625, 'mins': 2.3472900390625}
 
(7, 5, 1, 192, 784, 96)
Params #:  245392
MACs:  12024208
 
[21:17:08] (INFO) Building library...
[21:17:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.760870916193182, 'median': 1.755126953125, 'mins': 1.6817626953125}
 
(7, 5, 1, 192, 784, 192)
Params #:  320656
MACs:  15712144
 
[21:17:25] (INFO) Building library...
[21:17:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.102586780894886, 'median': 2.09619140625, 'mins': 2.0167236328125}
 
(7, 5, 1, 192, 800, 96)
Params #:  250400
MACs:  12269600
 
[21:17:44] (INFO) Building library...
[21:17:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6779319069602272, 'median': 1.657470703125, 'mins': 1.578125}
 
(7, 5, 1, 192, 800, 192)
Params #:  327200
MACs:  16032800
 
[21:18:02] (INFO) Building library...
[21:18:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.514092462713068, 'median': 2.49072265625, 'mins': 2.408203125}
 
(7, 5, 1, 192, 816, 96)
Params #:  255408
MACs:  12514992
 
[21:18:21] (INFO) Building library...
[21:18:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4651378284801135, 'median': 1.4517822265625, 'mins': 1.3673095703125}
 
(7, 5, 1, 192, 816, 192)
Params #:  333744
MACs:  16353456
 
[21:18:38] (INFO) Building library...
[21:18:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3721280184659093, 'median': 2.34423828125, 'mins': 2.244140625}
 
(7, 5, 1, 192, 832, 96)
Params #:  260416
MACs:  12760384
 
[21:18:58] (INFO) Building library...
[21:18:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7639581853693183, 'median': 1.75244140625, 'mins': 1.67626953125}
 
(7, 5, 1, 192, 832, 192)
Params #:  340288
MACs:  16674112
 
[21:19:15] (INFO) Building library...
[21:19:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4377674449573865, 'median': 2.398681640625, 'mins': 2.2958984375}
 
(7, 5, 1, 192, 848, 96)
Params #:  265424
MACs:  13005776
 
[21:19:35] (INFO) Building library...
[21:19:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4553955078125, 'median': 1.4278564453125, 'mins': 1.3740234375}
 
(7, 5, 1, 192, 848, 192)
Params #:  346832
MACs:  16994768
 
[21:19:52] (INFO) Building library...
[21:19:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.474661532315341, 'median': 2.4632568359375, 'mins': 2.3707275390625}
 
(7, 5, 1, 192, 864, 96)
Params #:  270432
MACs:  13251168
 
[21:20:11] (INFO) Building library...
[21:20:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8018821022727274, 'median': 1.786865234375, 'mins': 1.6900634765625}
 
(7, 5, 1, 192, 864, 192)
Params #:  353376
MACs:  17315424
 
[21:20:29] (INFO) Building library...
[21:20:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.680533114346591, 'median': 2.622802734375, 'mins': 2.517822265625}
 
(7, 5, 1, 192, 880, 96)
Params #:  275440
MACs:  13496560
 
[21:20:48] (INFO) Building library...
[21:20:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.692257412997159, 'median': 1.6553955078125, 'mins': 1.5849609375}
 
(7, 5, 1, 192, 880, 192)
Params #:  359920
MACs:  17636080
 
[21:21:06] (INFO) Building library...
[21:21:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5197920365767046, 'median': 2.4814453125, 'mins': 2.3800048828125}
 
(7, 5, 1, 192, 896, 96)
Params #:  280448
MACs:  13741952
 
[21:21:25] (INFO) Building library...
[21:21:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.702206143465909, 'median': 1.6756591796875, 'mins': 1.5897216796875}
 
(7, 5, 1, 192, 896, 192)
Params #:  366464
MACs:  17956736
 
[21:21:43] (INFO) Building library...
[21:21:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.350729092684659, 'median': 2.330810546875, 'mins': 2.2645263671875}
 
(7, 5, 1, 192, 912, 96)
Params #:  285456
MACs:  13987344
 
[21:22:02] (INFO) Building library...
[21:22:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9430197975852272, 'median': 1.7811279296875, 'mins': 1.686767578125}
 
(7, 5, 1, 192, 912, 192)
Params #:  373008
MACs:  18277392
 
[21:22:20] (INFO) Building library...
[21:22:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3239779385653407, 'median': 2.315673828125, 'mins': 2.2208251953125}
 
(7, 5, 1, 192, 928, 96)
Params #:  290464
MACs:  14232736
 
[21:22:39] (INFO) Building library...
[21:22:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6725075461647727, 'median': 1.6473388671875, 'mins': 1.5745849609375}
 
(7, 5, 1, 192, 928, 192)
Params #:  379552
MACs:  18598048
 
[21:22:57] (INFO) Building library...
[21:22:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.134796697443182, 'median': 2.12353515625, 'mins': 2.0611572265625}
 
(7, 5, 1, 192, 944, 96)
Params #:  295472
MACs:  14478128
 
[21:23:16] (INFO) Building library...
[21:23:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8649636008522728, 'median': 1.843994140625, 'mins': 1.7452392578125}
 
(7, 5, 1, 192, 944, 192)
Params #:  386096
MACs:  18918704
 
[21:23:34] (INFO) Building library...
[21:23:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4569713245738636, 'median': 2.4273681640625, 'mins': 2.3262939453125}
 
(7, 5, 1, 192, 960, 96)
Params #:  300480
MACs:  14723520
 
[21:23:53] (INFO) Building library...
[21:23:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7060380415482954, 'median': 1.680908203125, 'mins': 1.600341796875}
 
(7, 5, 1, 192, 960, 192)
Params #:  392640
MACs:  19239360
 
[21:24:11] (INFO) Building library...
[21:24:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1020063920454546, 'median': 2.0970458984375, 'mins': 2.0303955078125}
 
(7, 5, 1, 192, 976, 96)
Params #:  305488
MACs:  14968912
 
[21:24:30] (INFO) Building library...
[21:24:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.930404385653409, 'median': 1.7916259765625, 'mins': 1.6990966796875}
 
(7, 5, 1, 192, 976, 192)
Params #:  399184
MACs:  19560016
 
[21:24:48] (INFO) Building library...
[21:24:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.301282848011364, 'median': 2.2900390625, 'mins': 2.2138671875}
 
(7, 5, 1, 192, 992, 96)
Params #:  310496
MACs:  15214304
 
[21:25:07] (INFO) Building library...
[21:25:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7971668590198864, 'median': 1.776123046875, 'mins': 1.6881103515625}
 
(7, 5, 1, 192, 992, 192)
Params #:  405728
MACs:  19880672
 
[21:25:25] (INFO) Building library...
[21:25:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3943647904829546, 'median': 2.376953125, 'mins': 2.298828125}
 
(7, 5, 1, 192, 1008, 96)
Params #:  315504
MACs:  15459696
 
[21:25:44] (INFO) Building library...
[21:25:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7163585316051135, 'median': 1.679931640625, 'mins': 1.598388671875}
 
(7, 5, 1, 192, 1008, 192)
Params #:  412272
MACs:  20201328
 
[21:26:02] (INFO) Building library...
[21:26:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4438820578835228, 'median': 2.41943359375, 'mins': 2.308837890625}
 
(7, 5, 1, 192, 1024, 96)
Params #:  320512
MACs:  15705088
 
[21:26:21] (INFO) Building library...
[21:26:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8365056818181817, 'median': 1.7841796875, 'mins': 1.6939697265625}
 
(7, 5, 1, 192, 1024, 192)
Params #:  418816
MACs:  20521984
 
[21:26:39] (INFO) Building library...
[21:26:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3837180397727273, 'median': 2.3441162109375, 'mins': 2.24853515625}
 
(7, 5, 1, 192, 1040, 96)
Params #:  325520
MACs:  15950480
 
[21:26:58] (INFO) Building library...
[21:26:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.627212801846591, 'median': 1.61767578125, 'mins': 1.55029296875}
 
(7, 5, 1, 192, 1040, 192)
Params #:  425360
MACs:  20842640
 
[21:27:16] (INFO) Building library...
[21:27:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4910966352982955, 'median': 2.453369140625, 'mins': 2.3658447265625}
 
(7, 5, 1, 192, 1056, 96)
Params #:  330528
MACs:  16195872
 
[21:27:35] (INFO) Building library...
[21:27:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9987371271306817, 'median': 1.9259033203125, 'mins': 1.85693359375}
 
(7, 5, 1, 192, 1056, 192)
Params #:  431904
MACs:  21163296
 
[21:27:52] (INFO) Building library...
[21:27:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3730280095880683, 'median': 2.3375244140625, 'mins': 2.2296142578125}
 
(7, 5, 1, 192, 1072, 96)
Params #:  335536
MACs:  16441264
 
[21:28:12] (INFO) Building library...
[21:28:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7897316672585226, 'median': 1.7750244140625, 'mins': 1.7052001953125}
 
(7, 5, 1, 192, 1072, 192)
Params #:  438448
MACs:  21483952
 
[21:28:30] (INFO) Building library...
[21:28:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3601063121448864, 'median': 2.32470703125, 'mins': 2.224853515625}
 
(7, 5, 1, 192, 1088, 96)
Params #:  340544
MACs:  16686656
 
[21:28:49] (INFO) Building library...
[21:28:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7408780184659092, 'median': 1.7357177734375, 'mins': 1.6640625}
 
(7, 5, 1, 192, 1088, 192)
Params #:  444992
MACs:  21804608
 
[21:29:07] (INFO) Building library...
[21:29:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.51385498046875, 'median': 2.4892578125, 'mins': 2.38623046875}
 
(7, 5, 1, 192, 1104, 96)
Params #:  345552
MACs:  16932048
 
[21:29:26] (INFO) Building library...
[21:29:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6690906871448863, 'median': 1.642578125, 'mins': 1.562744140625}
 
(7, 5, 1, 192, 1104, 192)
Params #:  451536
MACs:  22125264
 
[21:29:44] (INFO) Building library...
[21:29:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5451649058948864, 'median': 2.5145263671875, 'mins': 2.4161376953125}
 
(7, 5, 1, 192, 1120, 96)
Params #:  350560
MACs:  17177440
 
[21:30:03] (INFO) Building library...
[21:30:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8421275745738637, 'median': 1.8194580078125, 'mins': 1.728515625}
 
(7, 5, 1, 192, 1120, 192)
Params #:  458080
MACs:  22445920
 
[21:30:21] (INFO) Building library...
[21:30:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3476573597301136, 'median': 2.3182373046875, 'mins': 2.2225341796875}
 
(7, 5, 1, 192, 1136, 96)
Params #:  355568
MACs:  17422832
 
[21:30:40] (INFO) Building library...
[21:30:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7593949751420455, 'median': 1.739990234375, 'mins': 1.67138671875}
 
(7, 5, 1, 192, 1136, 192)
Params #:  464624
MACs:  22766576
 
[21:30:58] (INFO) Building library...
[21:30:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.487734153053977, 'median': 2.4527587890625, 'mins': 2.3492431640625}
 
(7, 5, 1, 192, 1152, 96)
Params #:  360576
MACs:  17668224
 
[21:31:17] (INFO) Building library...
[21:31:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7767988725142045, 'median': 1.763916015625, 'mins': 1.676513671875}
 
(7, 5, 1, 192, 1152, 192)
Params #:  471168
MACs:  23087232
 
[21:31:35] (INFO) Building library...
[21:31:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.473646129261364, 'median': 2.4674072265625, 'mins': 2.3848876953125}
 
(7, 5, 1, 192, 1168, 96)
Params #:  365584
MACs:  17913616
 
[21:31:55] (INFO) Building library...
[21:31:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7370683149857955, 'median': 1.7032470703125, 'mins': 1.619140625}
 
(7, 5, 1, 192, 1168, 192)
Params #:  477712
MACs:  23407888
 
[21:32:12] (INFO) Building library...
[21:32:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3658336292613638, 'median': 2.345458984375, 'mins': 2.244873046875}
 
(7, 5, 1, 192, 1184, 96)
Params #:  370592
MACs:  18159008
 
[21:32:32] (INFO) Building library...
[21:32:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8600230823863637, 'median': 1.8238525390625, 'mins': 1.7396240234375}
 
(7, 5, 1, 192, 1184, 192)
Params #:  484256
MACs:  23728544
 
[21:32:49] (INFO) Building library...
[21:32:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.522260076349432, 'median': 2.4739990234375, 'mins': 2.376953125}
 
(7, 5, 1, 192, 1200, 96)
Params #:  375600
MACs:  18404400
 
[21:33:09] (INFO) Building library...
[21:33:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7727616743607955, 'median': 1.7642822265625, 'mins': 1.7041015625}
 
(7, 5, 1, 192, 1200, 192)
Params #:  490800
MACs:  24049200
 
[21:33:27] (INFO) Building library...
[21:33:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.509491521661932, 'median': 2.4794921875, 'mins': 2.3731689453125}
 
(7, 5, 1, 192, 1216, 96)
Params #:  380608
MACs:  18649792
 
[21:33:46] (INFO) Building library...
[21:33:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7522383256392045, 'median': 1.748779296875, 'mins': 1.6729736328125}
 
(7, 5, 1, 192, 1216, 192)
Params #:  497344
MACs:  24369856
 
[21:34:04] (INFO) Building library...
[21:34:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.517792302911932, 'median': 2.49169921875, 'mins': 2.3759765625}
 
(7, 5, 1, 192, 1232, 96)
Params #:  385616
MACs:  18895184
 
[21:34:23] (INFO) Building library...
[21:34:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7552090731534091, 'median': 1.705810546875, 'mins': 1.5474853515625}
 
(7, 5, 1, 192, 1232, 192)
Params #:  503888
MACs:  24690512
 
[21:34:41] (INFO) Building library...
[21:34:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.528734241832386, 'median': 2.5010986328125, 'mins': 2.3968505859375}
 
(7, 5, 1, 192, 1248, 96)
Params #:  390624
MACs:  19140576
 
[21:35:00] (INFO) Building library...
[21:35:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.75712890625, 'median': 1.747802734375, 'mins': 1.6751708984375}
 
(7, 5, 1, 192, 1248, 192)
Params #:  510432
MACs:  25011168
 
[21:35:18] (INFO) Building library...
[21:35:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.523360928622159, 'median': 2.486328125, 'mins': 2.406005859375}
 
(7, 5, 1, 192, 1264, 96)
Params #:  395632
MACs:  19385968
 
[21:35:38] (INFO) Building library...
[21:35:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7907737038352274, 'median': 1.7569580078125, 'mins': 1.6708984375}
 
(7, 5, 1, 192, 1264, 192)
Params #:  516976
MACs:  25331824
 
[21:35:55] (INFO) Building library...
[21:35:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.528646573153409, 'median': 2.5010986328125, 'mins': 2.40478515625}
 
(7, 5, 1, 192, 1280, 96)
Params #:  400640
MACs:  19631360
 
[21:36:14] (INFO) Building library...
[21:36:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8064686168323865, 'median': 1.778564453125, 'mins': 1.6982421875}
 
(7, 5, 1, 192, 1280, 192)
Params #:  523520
MACs:  25652480
 
[21:36:32] (INFO) Building library...
[21:36:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.59718017578125, 'median': 2.5633544921875, 'mins': 2.4725341796875}
 
(7, 5, 1, 192, 1296, 96)
Params #:  405648
MACs:  19876752
 
[21:36:52] (INFO) Building library...
[21:36:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.799951171875, 'median': 1.77783203125, 'mins': 1.6873779296875}
 
(7, 5, 1, 192, 1296, 192)
Params #:  530064
MACs:  25973136
 
[21:37:09] (INFO) Building library...
[21:37:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5457563920454547, 'median': 2.5050048828125, 'mins': 2.4068603515625}
 
(7, 5, 1, 192, 1312, 96)
Params #:  410656
MACs:  20122144
 
[21:37:29] (INFO) Building library...
[21:37:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8169600053267045, 'median': 1.80712890625, 'mins': 1.6065673828125}
 
(7, 5, 1, 192, 1312, 192)
Params #:  536608
MACs:  26293792
 
[21:37:46] (INFO) Building library...
[21:37:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6588634144176138, 'median': 2.5472412109375, 'mins': 2.414794921875}
 
(7, 5, 1, 192, 1328, 96)
Params #:  415664
MACs:  20367536
 
[21:38:06] (INFO) Building library...
[21:38:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7987937233664772, 'median': 1.7711181640625, 'mins': 1.6907958984375}
 
(7, 5, 1, 192, 1328, 192)
Params #:  543152
MACs:  26614448
 
[21:38:23] (INFO) Building library...
[21:38:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.521206942471591, 'median': 2.47998046875, 'mins': 2.390869140625}
 
(7, 5, 1, 192, 1344, 96)
Params #:  420672
MACs:  20612928
 
[21:38:43] (INFO) Building library...
[21:38:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8418190696022727, 'median': 1.8111572265625, 'mins': 1.7347412109375}
 
(7, 5, 1, 192, 1344, 192)
Params #:  549696
MACs:  26935104
 
[21:39:01] (INFO) Building library...
[21:39:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.468374911221591, 'median': 2.4188232421875, 'mins': 2.3079833984375}
 
(7, 5, 1, 192, 1360, 96)
Params #:  425680
MACs:  20858320
 
[21:39:20] (INFO) Building library...
[21:39:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7498046875, 'median': 1.741455078125, 'mins': 1.664794921875}
 
(7, 5, 1, 192, 1360, 192)
Params #:  556240
MACs:  27255760
 
[21:39:38] (INFO) Building library...
[21:39:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5101584694602272, 'median': 2.466064453125, 'mins': 2.3792724609375}
 
(7, 5, 1, 192, 1376, 96)
Params #:  430688
MACs:  21103712
 
[21:39:58] (INFO) Building library...
[21:39:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7899336381392046, 'median': 1.7718505859375, 'mins': 1.690185546875}
 
(7, 5, 1, 192, 1376, 192)
Params #:  562784
MACs:  27576416
 
[21:40:15] (INFO) Building library...
[21:40:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.482919034090909, 'median': 2.44287109375, 'mins': 2.357421875}
 
(7, 5, 1, 192, 1392, 96)
Params #:  435696
MACs:  21349104
 
[21:40:35] (INFO) Building library...
[21:40:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8117276278409091, 'median': 1.774169921875, 'mins': 1.6934814453125}
 
(7, 5, 1, 192, 1392, 192)
Params #:  569328
MACs:  27897072
 
[21:40:52] (INFO) Building library...
[21:40:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5252607865767045, 'median': 2.496337890625, 'mins': 2.4091796875}
 
(7, 5, 1, 192, 1408, 96)
Params #:  440704
MACs:  21594496
 
[21:41:12] (INFO) Building library...
[21:41:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.84559326171875, 'median': 1.81591796875, 'mins': 1.72509765625}
 
(7, 5, 1, 192, 1408, 192)
Params #:  575872
MACs:  28217728
 
[21:41:29] (INFO) Building library...
[21:41:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4857677112926138, 'median': 2.468505859375, 'mins': 2.4013671875}
 
(7, 5, 1, 192, 1424, 96)
Params #:  445712
MACs:  21839888
 
[21:41:49] (INFO) Building library...
[21:41:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8035999644886365, 'median': 1.7703857421875, 'mins': 1.6859130859375}
 
(7, 5, 1, 192, 1424, 192)
Params #:  582416
MACs:  28538384
 
[21:42:06] (INFO) Building library...
[21:42:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4603315873579548, 'median': 2.4598388671875, 'mins': 2.3653564453125}
 
(7, 5, 1, 192, 1440, 96)
Params #:  450720
MACs:  22085280
 
[21:42:26] (INFO) Building library...
[21:42:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.796133700284091, 'median': 1.7720947265625, 'mins': 1.67919921875}
 
(7, 5, 1, 192, 1440, 192)
Params #:  588960
MACs:  28859040
 
[21:42:44] (INFO) Building library...
[21:42:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3860706676136365, 'median': 2.3582763671875, 'mins': 2.273681640625}
 
(7, 5, 1, 192, 1456, 96)
Params #:  455728
MACs:  22330672
 
[21:43:03] (INFO) Building library...
[21:43:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8274136629971591, 'median': 1.8009033203125, 'mins': 1.72265625}
 
(7, 5, 1, 192, 1456, 192)
Params #:  595504
MACs:  29179696
 
[21:43:21] (INFO) Building library...
[21:43:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.639208984375, 'median': 2.6405029296875, 'mins': 2.462890625}
 
(7, 5, 1, 192, 1472, 96)
Params #:  460736
MACs:  22576064
 
[21:43:40] (INFO) Building library...
[21:43:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7620439009232955, 'median': 1.7294921875, 'mins': 1.6588134765625}
 
(7, 5, 1, 192, 1472, 192)
Params #:  602048
MACs:  29500352
 
[21:43:58] (INFO) Building library...
[21:43:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.687576571377841, 'median': 2.6329345703125, 'mins': 2.50341796875}
 
(7, 5, 1, 192, 1488, 96)
Params #:  465744
MACs:  22821456
 
[21:44:17] (INFO) Building library...
[21:44:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8060857599431819, 'median': 1.7952880859375, 'mins': 1.725830078125}
 
(7, 5, 1, 192, 1488, 192)
Params #:  608592
MACs:  29821008
 
[21:44:35] (INFO) Building library...
[21:44:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.455487615411932, 'median': 2.408935546875, 'mins': 2.3270263671875}
 
(7, 5, 1, 192, 1504, 96)
Params #:  470752
MACs:  23066848
 
[21:44:55] (INFO) Building library...
[21:44:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6607710404829545, 'median': 1.6376953125, 'mins': 1.5665283203125}
 
(7, 5, 1, 192, 1504, 192)
Params #:  615136
MACs:  30141664
 
[21:45:12] (INFO) Building library...
[21:45:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4654862837357956, 'median': 2.4481201171875, 'mins': 2.37255859375}
 
(7, 5, 1, 192, 1520, 96)
Params #:  475760
MACs:  23312240
 
[21:45:32] (INFO) Building library...
[21:45:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7808116566051135, 'median': 1.75634765625, 'mins': 1.667724609375}
 
(7, 5, 1, 192, 1520, 192)
Params #:  621680
MACs:  30462320
 
[21:45:49] (INFO) Building library...
[21:45:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5309858842329547, 'median': 2.4974365234375, 'mins': 2.4083251953125}
 
(7, 5, 1, 192, 1536, 96)
Params #:  480768
MACs:  23557632
 
[21:46:09] (INFO) Building library...
[21:46:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8421985973011363, 'median': 1.822265625, 'mins': 1.7220458984375}
 
(7, 5, 1, 192, 1536, 192)
Params #:  628224
MACs:  30782976
 
[21:46:27] (INFO) Building library...
[21:46:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5655861594460228, 'median': 2.533935546875, 'mins': 2.4407958984375}
 
(7, 5, 1, 192, 1552, 96)
Params #:  485776
MACs:  23803024
 
[21:46:46] (INFO) Building library...
[21:46:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.916988858309659, 'median': 1.80859375, 'mins': 1.69677734375}
 
(7, 5, 1, 192, 1552, 192)
Params #:  634768
MACs:  31103632
 
[21:47:04] (INFO) Building library...
[21:47:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5567160866477274, 'median': 2.5181884765625, 'mins': 2.4346923828125}
 
(7, 5, 1, 192, 1568, 96)
Params #:  490784
MACs:  24048416
 
[21:47:24] (INFO) Building library...
[21:47:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.74556884765625, 'median': 1.73876953125, 'mins': 1.665771484375}
 
(7, 5, 1, 192, 1568, 192)
Params #:  641312
MACs:  31424288
 
[21:47:41] (INFO) Building library...
[21:47:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.661031827059659, 'median': 2.596923828125, 'mins': 2.48681640625}
 
(7, 5, 1, 192, 1584, 96)
Params #:  495792
MACs:  24293808
 
[21:48:01] (INFO) Building library...
[21:48:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7300015536221591, 'median': 1.6922607421875, 'mins': 1.6044921875}
 
(7, 5, 1, 192, 1584, 192)
Params #:  647856
MACs:  31744944
 
[21:48:19] (INFO) Building library...
[21:48:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4547540838068183, 'median': 2.451171875, 'mins': 2.3736572265625}
 
(7, 5, 1, 192, 1600, 96)
Params #:  500800
MACs:  24539200
 
[21:48:38] (INFO) Building library...
[21:48:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.633047762784091, 'median': 1.6102294921875, 'mins': 1.525390625}
 
(7, 5, 1, 192, 1600, 192)
Params #:  654400
MACs:  32065600
 
[21:48:56] (INFO) Building library...
[21:48:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.535452547940341, 'median': 2.501708984375, 'mins': 2.40771484375}
 
(7, 5, 1, 192, 1616, 96)
Params #:  505808
MACs:  24784592
 
[21:49:16] (INFO) Building library...
[21:49:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8910100763494317, 'median': 1.8271484375, 'mins': 1.7373046875}
 
(7, 5, 1, 192, 1616, 192)
Params #:  660944
MACs:  32386256
 
[21:49:33] (INFO) Building library...
[21:49:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5490434126420456, 'median': 2.513427734375, 'mins': 2.4075927734375}
 
(7, 5, 1, 192, 1632, 96)
Params #:  510816
MACs:  25029984
 
[21:49:53] (INFO) Building library...
[21:49:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8049105557528409, 'median': 1.80224609375, 'mins': 1.7283935546875}
 
(7, 5, 1, 192, 1632, 192)
Params #:  667488
MACs:  32706912
 
[21:50:10] (INFO) Building library...
[21:50:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5662309126420455, 'median': 2.5023193359375, 'mins': 2.40625}
 
(7, 5, 1, 192, 1648, 96)
Params #:  515824
MACs:  25275376
 
[21:50:30] (INFO) Building library...
[21:50:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6826637961647728, 'median': 1.6522216796875, 'mins': 1.552490234375}
 
(7, 5, 1, 192, 1648, 192)
Params #:  674032
MACs:  33027568
 
[21:50:48] (INFO) Building library...
[21:50:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4998401988636365, 'median': 2.468505859375, 'mins': 2.376953125}
 
(7, 5, 1, 192, 1664, 96)
Params #:  520832
MACs:  25520768
 
[21:51:07] (INFO) Building library...
[21:51:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.897833806818182, 'median': 1.870849609375, 'mins': 1.777099609375}
 
(7, 5, 1, 192, 1664, 192)
Params #:  680576
MACs:  33348224
 
[21:51:25] (INFO) Building library...
[21:51:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4640824751420456, 'median': 2.4580078125, 'mins': 2.373779296875}
 
(7, 5, 1, 192, 1680, 96)
Params #:  525840
MACs:  25766160
 
[21:51:44] (INFO) Building library...
[21:51:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6416936700994318, 'median': 1.6251220703125, 'mins': 1.54296875}
 
(7, 5, 1, 192, 1680, 192)
Params #:  687120
MACs:  33668880
 
[21:52:02] (INFO) Building library...
[21:52:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.486612215909091, 'median': 2.4635009765625, 'mins': 2.3671875}
 
(7, 5, 1, 192, 1696, 96)
Params #:  530848
MACs:  26011552
 
[21:52:22] (INFO) Building library...
[21:52:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6696466619318182, 'median': 1.6534423828125, 'mins': 1.5797119140625}
 
(7, 5, 1, 192, 1696, 192)
Params #:  693664
MACs:  33989536
 
[21:52:39] (INFO) Building library...
[21:52:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5181840376420452, 'median': 2.4970703125, 'mins': 2.4083251953125}
 
(7, 5, 1, 192, 1712, 96)
Params #:  535856
MACs:  26256944
 
[21:52:59] (INFO) Building library...
[21:52:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8535200639204545, 'median': 1.8248291015625, 'mins': 1.7412109375}
 
(7, 5, 1, 192, 1712, 192)
Params #:  700208
MACs:  34310192
 
[21:53:16] (INFO) Building library...
[21:53:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.547415438565341, 'median': 2.50634765625, 'mins': 2.3953857421875}
 
(7, 3, 1, 192, 16, 160)
Params #:  5776
MACs:  283024
 
[21:53:36] (INFO) Building library...
[21:53:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4896983753551136, 'median': 1.478759765625, 'mins': 1.41259765625}
 
(7, 3, 1, 192, 16, 320)
Params #:  8336
MACs:  408464
 
[21:53:53] (INFO) Building library...
[21:53:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4795576615767045, 'median': 1.4708251953125, 'mins': 1.402099609375}
 
(7, 3, 1, 192, 32, 160)
Params #:  11552
MACs:  566048
 
[21:54:10] (INFO) Building library...
[21:54:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.542544833096591, 'median': 1.5185546875, 'mins': 1.42578125}
 
(7, 3, 1, 192, 32, 320)
Params #:  16672
MACs:  816928
 
[21:54:27] (INFO) Building library...
[21:54:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5116532759232955, 'median': 1.489990234375, 'mins': 1.419677734375}
 
(7, 3, 1, 192, 48, 160)
Params #:  17328
MACs:  849072
 
[21:54:44] (INFO) Building library...
[21:54:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5136829723011365, 'median': 1.5035400390625, 'mins': 1.42529296875}
 
(7, 3, 1, 192, 48, 320)
Params #:  25008
MACs:  1225392
 
[21:55:01] (INFO) Building library...
[21:55:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5753806374289774, 'median': 1.53369140625, 'mins': 1.444091796875}
 
(7, 3, 1, 192, 64, 160)
Params #:  23104
MACs:  1132096
 
[21:55:18] (INFO) Building library...
[21:55:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6889548561789773, 'median': 1.65673828125, 'mins': 1.574462890625}
 
(7, 3, 1, 192, 64, 320)
Params #:  33344
MACs:  1633856
 
[21:55:35] (INFO) Building library...
[21:55:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.644173916903409, 'median': 1.6142578125, 'mins': 1.547607421875}
 
(7, 3, 1, 192, 80, 160)
Params #:  28880
MACs:  1415120
 
[21:55:52] (INFO) Building library...
[21:55:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5184248490767045, 'median': 1.5089111328125, 'mins': 1.4498291015625}
 
(7, 3, 1, 192, 80, 320)
Params #:  41680
MACs:  2042320
 
[21:56:10] (INFO) Building library...
[21:56:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.471869451349432, 'median': 1.451904296875, 'mins': 1.3819580078125}
 
(7, 3, 1, 192, 96, 160)
Params #:  34656
MACs:  1698144
 
[21:56:27] (INFO) Building library...
[21:56:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5217018821022728, 'median': 1.5050048828125, 'mins': 1.4329833984375}
 
(7, 3, 1, 192, 96, 320)
Params #:  50016
MACs:  2450784
 
[21:56:44] (INFO) Building library...
[21:56:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.63564453125, 'median': 1.62255859375, 'mins': 1.5609130859375}
 
(7, 3, 1, 192, 112, 160)
Params #:  40432
MACs:  1981168
 
[21:57:01] (INFO) Building library...
[21:57:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.503058416193182, 'median': 1.48876953125, 'mins': 1.41845703125}
 
(7, 3, 1, 192, 112, 320)
Params #:  58352
MACs:  2859248
 
[21:57:19] (INFO) Building library...
[21:57:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5181906960227274, 'median': 1.4967041015625, 'mins': 1.42626953125}
 
(7, 3, 1, 192, 128, 160)
Params #:  46208
MACs:  2264192
 
[21:57:36] (INFO) Building library...
[21:57:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6569846413352274, 'median': 1.648193359375, 'mins': 1.5794677734375}
 
(7, 3, 1, 192, 128, 320)
Params #:  66688
MACs:  3267712
 
[21:57:54] (INFO) Building library...
[21:57:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4920354669744318, 'median': 1.467529296875, 'mins': 1.3831787109375}
 
(7, 3, 1, 192, 144, 160)
Params #:  51984
MACs:  2547216
 
[21:58:11] (INFO) Building library...
[21:58:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7212923916903409, 'median': 1.7010498046875, 'mins': 1.63720703125}
 
(7, 3, 1, 192, 144, 320)
Params #:  75024
MACs:  3676176
 
[21:58:28] (INFO) Building library...
[21:58:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4561945134943182, 'median': 1.4532470703125, 'mins': 1.384033203125}
 
(7, 3, 1, 192, 160, 160)
Params #:  57760
MACs:  2830240
 
[21:58:46] (INFO) Building library...
[21:58:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5210316051136363, 'median': 1.510009765625, 'mins': 1.4527587890625}
 
(7, 3, 1, 192, 160, 320)
Params #:  83360
MACs:  4084640
 
[21:59:03] (INFO) Building library...
[21:59:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7523548473011363, 'median': 1.7164306640625, 'mins': 1.6224365234375}
 
(7, 3, 1, 192, 176, 160)
Params #:  63536
MACs:  3113264
 
[21:59:20] (INFO) Building library...
[21:59:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5437810724431817, 'median': 1.520751953125, 'mins': 1.43310546875}
 
(7, 3, 1, 192, 176, 320)
Params #:  91696
MACs:  4493104
 
[21:59:37] (INFO) Building library...
[21:59:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6930597478693181, 'median': 1.55810546875, 'mins': 1.4498291015625}
 
(7, 3, 1, 192, 208, 160)
Params #:  75088
MACs:  3679312
 
[21:59:54] (INFO) Building library...
[21:59:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.605362215909091, 'median': 1.5423583984375, 'mins': 1.45263671875}
 
(7, 3, 1, 192, 208, 320)
Params #:  108368
MACs:  5310032
 
[22:00:11] (INFO) Building library...
[22:00:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5749434037642045, 'median': 1.5362548828125, 'mins': 1.471923828125}
 
(7, 3, 1, 192, 224, 160)
Params #:  80864
MACs:  3962336
 
[22:00:28] (INFO) Building library...
[22:00:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5363303444602272, 'median': 1.51904296875, 'mins': 1.463134765625}
 
(7, 3, 1, 192, 224, 320)
Params #:  116704
MACs:  5718496
 
[22:00:45] (INFO) Building library...
[22:00:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.564651766690341, 'median': 1.55322265625, 'mins': 1.472412109375}
 
(7, 3, 1, 192, 240, 160)
Params #:  86640
MACs:  4245360
 
[22:01:03] (INFO) Building library...
[22:01:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5992531516335227, 'median': 1.5712890625, 'mins': 1.5145263671875}
 
(7, 3, 1, 192, 240, 320)
Params #:  125040
MACs:  6126960
 
[22:01:19] (INFO) Building library...
[22:01:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7366665926846592, 'median': 1.714599609375, 'mins': 1.6385498046875}
 
(7, 3, 1, 192, 256, 160)
Params #:  92416
MACs:  4528384
 
[22:01:36] (INFO) Building library...
[22:01:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4703935102982955, 'median': 1.4622802734375, 'mins': 1.402099609375}
 
(7, 3, 1, 192, 256, 320)
Params #:  133376
MACs:  6535424
 
[22:01:53] (INFO) Building library...
[22:01:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7822676225142045, 'median': 1.7587890625, 'mins': 1.68212890625}
 
(7, 3, 1, 192, 272, 160)
Params #:  98192
MACs:  4811408
 
[22:02:10] (INFO) Building library...
[22:02:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.69442138671875, 'median': 1.682373046875, 'mins': 1.6221923828125}
 
(7, 3, 1, 192, 272, 320)
Params #:  141712
MACs:  6943888
 
[22:02:27] (INFO) Building library...
[22:02:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5130237926136363, 'median': 1.495361328125, 'mins': 1.4423828125}
 
(7, 3, 1, 192, 288, 160)
Params #:  103968
MACs:  5094432
 
[22:02:43] (INFO) Building library...
[22:02:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6257346413352274, 'median': 1.55810546875, 'mins': 1.47119140625}
 
(7, 3, 1, 192, 288, 320)
Params #:  150048
MACs:  7352352
 
[22:03:01] (INFO) Building library...
[22:03:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5753995028409091, 'median': 1.5576171875, 'mins': 1.4627685546875}
 
(7, 3, 1, 192, 304, 160)
Params #:  109744
MACs:  5377456
 
[22:03:18] (INFO) Building library...
[22:03:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.709042080965909, 'median': 1.553955078125, 'mins': 1.471923828125}
 
(7, 3, 1, 192, 304, 320)
Params #:  158384
MACs:  7760816
 
[22:03:35] (INFO) Building library...
[22:03:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5473133433948865, 'median': 1.5167236328125, 'mins': 1.4451904296875}
 
(7, 3, 1, 192, 320, 160)
Params #:  115520
MACs:  5660480
 
[22:03:52] (INFO) Building library...
[22:03:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6948441938920455, 'median': 1.673828125, 'mins': 1.6041259765625}
 
(7, 3, 1, 192, 320, 320)
Params #:  166720
MACs:  8169280
 
[22:04:10] (INFO) Building library...
[22:04:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7762917258522728, 'median': 1.760986328125, 'mins': 1.6923828125}
 
(7, 3, 1, 192, 336, 160)
Params #:  121296
MACs:  5943504
 
[22:04:27] (INFO) Building library...
[22:04:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5583218661221592, 'median': 1.5421142578125, 'mins': 1.484130859375}
 
(7, 3, 1, 192, 336, 320)
Params #:  175056
MACs:  8577744
 
[22:04:44] (INFO) Building library...
[22:04:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.718185147372159, 'median': 1.706298828125, 'mins': 1.6461181640625}
 
(7, 3, 1, 192, 352, 160)
Params #:  127072
MACs:  6226528
 
[22:05:01] (INFO) Building library...
[22:05:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6872147993607955, 'median': 1.656005859375, 'mins': 1.56787109375}
 
(7, 3, 1, 192, 352, 320)
Params #:  183392
MACs:  8986208
 
[22:05:18] (INFO) Building library...
[22:05:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.469780939275568, 'median': 1.4547119140625, 'mins': 1.3802490234375}
 
(7, 3, 1, 192, 368, 160)
Params #:  132848
MACs:  6509552
 
[22:05:35] (INFO) Building library...
[22:05:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7170632102272727, 'median': 1.692138671875, 'mins': 1.60498046875}
 
(7, 3, 1, 192, 368, 320)
Params #:  191728
MACs:  9394672
 
[22:05:52] (INFO) Building library...
[22:05:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6701249556107955, 'median': 1.642578125, 'mins': 1.5687255859375}
 
(7, 3, 1, 192, 384, 160)
Params #:  138624
MACs:  6792576
 
[22:06:09] (INFO) Building library...
[22:06:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7199851296164772, 'median': 1.680908203125, 'mins': 1.599365234375}
 
(7, 3, 1, 192, 384, 320)
Params #:  200064
MACs:  9803136
 
[22:06:26] (INFO) Building library...
[22:06:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.62989501953125, 'median': 1.603759765625, 'mins': 1.53125}
 
(7, 3, 1, 192, 400, 160)
Params #:  144400
MACs:  7075600
 
[22:06:44] (INFO) Building library...
[22:06:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.673660555752841, 'median': 1.6600341796875, 'mins': 1.5972900390625}
 
(7, 3, 1, 192, 400, 320)
Params #:  208400
MACs:  10211600
 
[22:07:01] (INFO) Building library...
[22:07:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6485340465198863, 'median': 1.6279296875, 'mins': 1.5537109375}
 
(7, 3, 1, 192, 416, 160)
Params #:  150176
MACs:  7358624
 
[22:07:18] (INFO) Building library...
[22:07:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8804432262073865, 'median': 1.69091796875, 'mins': 1.5975341796875}
 
(7, 3, 1, 192, 416, 320)
Params #:  216736
MACs:  10620064
 
[22:07:36] (INFO) Building library...
[22:07:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4701582475142045, 'median': 1.452392578125, 'mins': 1.376708984375}
 
(7, 3, 1, 192, 432, 160)
Params #:  155952
MACs:  7641648
 
[22:07:52] (INFO) Building library...
[22:07:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4565329811789773, 'median': 1.432861328125, 'mins': 1.361083984375}
 
(7, 3, 1, 192, 432, 320)
Params #:  225072
MACs:  11028528
 
[22:08:10] (INFO) Building library...
[22:08:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6269697709517046, 'median': 1.4971923828125, 'mins': 1.3890380859375}
 
(7, 3, 1, 192, 448, 160)
Params #:  161728
MACs:  7924672
 
[22:08:27] (INFO) Building library...
[22:08:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9429676402698863, 'median': 1.9095458984375, 'mins': 1.565673828125}
 
(7, 3, 1, 192, 448, 320)
Params #:  233408
MACs:  11436992
 
[22:08:45] (INFO) Building library...
[22:08:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6667014382102272, 'median': 1.647705078125, 'mins': 1.58642578125}
 
(7, 3, 1, 192, 464, 160)
Params #:  167504
MACs:  8207696
 
[22:09:02] (INFO) Building library...
[22:09:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7081409801136365, 'median': 1.6917724609375, 'mins': 1.6231689453125}
 
(7, 3, 1, 192, 464, 320)
Params #:  241744
MACs:  11845456
 
[22:09:19] (INFO) Building library...
[22:09:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8089000355113636, 'median': 1.791748046875, 'mins': 1.7119140625}
 
(7, 3, 1, 192, 480, 160)
Params #:  173280
MACs:  8490720
 
[22:09:36] (INFO) Building library...
[22:09:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5158580433238635, 'median': 1.4830322265625, 'mins': 1.419677734375}
 
(7, 3, 1, 192, 480, 320)
Params #:  250080
MACs:  12253920
 
[22:09:53] (INFO) Building library...
[22:09:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4645075017755682, 'median': 1.45654296875, 'mins': 1.390625}
 
(7, 3, 1, 192, 496, 160)
Params #:  179056
MACs:  8773744
 
[22:10:10] (INFO) Building library...
[22:10:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6291981090198864, 'median': 1.613037109375, 'mins': 1.541015625}
 
(7, 3, 1, 192, 496, 320)
Params #:  258416
MACs:  12662384
 
[22:10:27] (INFO) Building library...
[22:10:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4647327769886365, 'median': 1.4453125, 'mins': 1.366455078125}
 
(7, 3, 1, 192, 512, 160)
Params #:  184832
MACs:  9056768
 
[22:10:44] (INFO) Building library...
[22:10:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.679684170809659, 'median': 1.6712646484375, 'mins': 1.603759765625}
 
(7, 3, 1, 192, 512, 320)
Params #:  266752
MACs:  13070848
 
[22:11:01] (INFO) Building library...
[22:11:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8001409357244318, 'median': 1.7794189453125, 'mins': 1.7103271484375}
 
(7, 3, 1, 192, 528, 160)
Params #:  190608
MACs:  9339792
 
[22:11:18] (INFO) Building library...
[22:11:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.749100008877841, 'median': 1.721435546875, 'mins': 1.536376953125}
 
(7, 3, 1, 192, 528, 320)
Params #:  275088
MACs:  13479312
 
[22:11:35] (INFO) Building library...
[22:11:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6500577059659092, 'median': 1.6224365234375, 'mins': 1.5513916015625}
 
(7, 3, 1, 192, 544, 160)
Params #:  196384
MACs:  9622816
 
[22:11:52] (INFO) Building library...
[22:11:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.551806640625, 'median': 1.523681640625, 'mins': 1.459228515625}
 
(7, 3, 1, 192, 544, 320)
Params #:  283424
MACs:  13887776
 
[22:12:09] (INFO) Building library...
[22:12:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4754461115056818, 'median': 1.458251953125, 'mins': 1.3753662109375}
 
(7, 3, 1, 192, 560, 160)
Params #:  202160
MACs:  9905840
 
[22:12:27] (INFO) Building library...
[22:12:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6590298739346592, 'median': 1.636962890625, 'mins': 1.55126953125}
 
(7, 3, 1, 192, 560, 320)
Params #:  291760
MACs:  14296240
 
[22:12:44] (INFO) Building library...
[22:12:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.655632990056818, 'median': 1.6463623046875, 'mins': 1.5804443359375}
 
(7, 3, 1, 192, 576, 160)
Params #:  207936
MACs:  10188864
 
[22:13:01] (INFO) Building library...
[22:13:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5241111061789774, 'median': 1.49365234375, 'mins': 1.4056396484375}
 
(7, 3, 1, 192, 576, 320)
Params #:  300096
MACs:  14704704
 
[22:13:18] (INFO) Building library...
[22:13:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7084761186079545, 'median': 1.6734619140625, 'mins': 1.5965576171875}
 
(7, 3, 1, 192, 592, 160)
Params #:  213712
MACs:  10471888
 
[22:13:36] (INFO) Building library...
[22:13:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7286554509943182, 'median': 1.707275390625, 'mins': 1.6339111328125}
 
(7, 3, 1, 192, 592, 320)
Params #:  308432
MACs:  15113168
 
[22:13:53] (INFO) Building library...
[22:13:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8277610085227274, 'median': 1.8009033203125, 'mins': 1.7196044921875}
 
(7, 3, 1, 192, 608, 160)
Params #:  219488
MACs:  10754912
 
[22:14:11] (INFO) Building library...
[22:14:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7200095436789773, 'median': 1.678466796875, 'mins': 1.61328125}
 
(7, 3, 1, 192, 608, 320)
Params #:  316768
MACs:  15521632
 
[22:14:28] (INFO) Building library...
[22:14:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6635242808948865, 'median': 1.65087890625, 'mins': 1.5833740234375}
 
(7, 3, 1, 192, 624, 160)
Params #:  225264
MACs:  11037936
 
[22:14:45] (INFO) Building library...
[22:14:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7195900656960228, 'median': 1.6866455078125, 'mins': 1.600830078125}
 
(7, 3, 1, 192, 624, 320)
Params #:  325104
MACs:  15930096
 
[22:15:03] (INFO) Building library...
[22:15:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8173051313920454, 'median': 1.7987060546875, 'mins': 1.717041015625}
 
(7, 3, 1, 192, 640, 160)
Params #:  231040
MACs:  11320960
 
[22:15:20] (INFO) Building library...
[22:15:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.68507080078125, 'median': 1.666015625, 'mins': 1.5958251953125}
 
(7, 3, 1, 192, 640, 320)
Params #:  333440
MACs:  16338560
 
[22:15:37] (INFO) Building library...
[22:15:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8719682173295455, 'median': 1.822509765625, 'mins': 1.7200927734375}
 
(7, 3, 1, 192, 656, 160)
Params #:  236816
MACs:  11603984
 
[22:15:54] (INFO) Building library...
[22:15:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9100508256392046, 'median': 1.732666015625, 'mins': 1.6229248046875}
 
(7, 3, 1, 192, 656, 320)
Params #:  341776
MACs:  16747024
 
[22:16:12] (INFO) Building library...
[22:16:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7891989968039772, 'median': 1.774169921875, 'mins': 1.7003173828125}
 
(7, 3, 1, 192, 672, 160)
Params #:  242592
MACs:  11887008
 
[22:16:29] (INFO) Building library...
[22:16:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6213356711647726, 'median': 1.605224609375, 'mins': 1.53515625}
 
(7, 3, 1, 192, 672, 320)
Params #:  350112
MACs:  17155488
 
[22:16:46] (INFO) Building library...
[22:16:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.79176025390625, 'median': 1.7708740234375, 'mins': 1.6888427734375}
 
(7, 3, 1, 192, 688, 160)
Params #:  248368
MACs:  12170032
 
[22:17:04] (INFO) Building library...
[22:17:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5657015713778408, 'median': 1.5465087890625, 'mins': 1.469970703125}
 
(7, 3, 1, 192, 688, 320)
Params #:  358448
MACs:  17563952
 
[22:17:21] (INFO) Building library...
[22:17:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.470211514559659, 'median': 1.4501953125, 'mins': 1.3870849609375}
 
(7, 3, 1, 192, 704, 160)
Params #:  254144
MACs:  12453056
 
[22:17:38] (INFO) Building library...
[22:17:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.639828213778409, 'median': 1.6192626953125, 'mins': 1.537841796875}
 
(7, 3, 1, 192, 704, 320)
Params #:  366784
MACs:  17972416
 
[22:17:55] (INFO) Building library...
[22:17:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7492620294744319, 'median': 1.74267578125, 'mins': 1.6719970703125}
 
(7, 3, 1, 192, 720, 160)
Params #:  259920
MACs:  12736080
 
[22:18:12] (INFO) Building library...
[22:18:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5503728693181817, 'median': 1.522705078125, 'mins': 1.4569091796875}
 
(7, 3, 1, 192, 720, 320)
Params #:  375120
MACs:  18380880
 
[22:18:30] (INFO) Building library...
[22:18:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8469382546164772, 'median': 1.8277587890625, 'mins': 1.7371826171875}
 
(7, 3, 1, 192, 736, 160)
Params #:  265696
MACs:  13019104
 
[22:18:47] (INFO) Building library...
[22:18:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8428300337357955, 'median': 1.694580078125, 'mins': 1.5885009765625}
 
(7, 3, 1, 192, 736, 320)
Params #:  383456
MACs:  18789344
 
[22:19:04] (INFO) Building library...
[22:19:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6601917613636363, 'median': 1.625244140625, 'mins': 1.536865234375}
 
(7, 3, 1, 192, 752, 160)
Params #:  271472
MACs:  13302128
 
[22:19:22] (INFO) Building library...
[22:19:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.699609375, 'median': 1.6712646484375, 'mins': 1.593017578125}
 
(7, 3, 1, 192, 752, 320)
Params #:  391792
MACs:  19197808
 
[22:19:39] (INFO) Building library...
[22:19:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.821670809659091, 'median': 1.7840576171875, 'mins': 1.6998291015625}
 
(7, 3, 1, 192, 768, 160)
Params #:  277248
MACs:  13585152
 
[22:19:56] (INFO) Building library...
[22:19:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7231722745028408, 'median': 1.6973876953125, 'mins': 1.61328125}
 
(7, 3, 1, 192, 768, 320)
Params #:  400128
MACs:  19606272
 
[22:20:14] (INFO) Building library...
[22:20:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6163596413352272, 'median': 1.6014404296875, 'mins': 1.5291748046875}
 
(7, 3, 1, 192, 784, 160)
Params #:  283024
MACs:  13868176
 
[22:20:32] (INFO) Building library...
[22:20:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6945501154119318, 'median': 1.671142578125, 'mins': 1.576416015625}
 
(7, 3, 1, 192, 784, 320)
Params #:  408464
MACs:  20014736
 
[22:20:49] (INFO) Building library...
[22:20:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8116843483664773, 'median': 1.783935546875, 'mins': 1.702392578125}
 
(7, 3, 1, 192, 800, 160)
Params #:  288800
MACs:  14151200
 
[22:21:06] (INFO) Building library...
[22:21:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.80438232421875, 'median': 1.7935791015625, 'mins': 1.723876953125}
 
(7, 3, 1, 192, 800, 320)
Params #:  416800
MACs:  20423200
 
[22:21:24] (INFO) Building library...
[22:21:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8219737659801136, 'median': 1.7998046875, 'mins': 1.709228515625}
 
(7, 3, 1, 192, 816, 160)
Params #:  294576
MACs:  14434224
 
[22:21:41] (INFO) Building library...
[22:21:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6822232333096592, 'median': 1.64892578125, 'mins': 1.5699462890625}
 
(7, 3, 1, 192, 816, 320)
Params #:  425136
MACs:  20831664
 
[22:21:58] (INFO) Building library...
[22:21:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7483920010653409, 'median': 1.74169921875, 'mins': 1.6737060546875}
 
(7, 3, 1, 192, 832, 160)
Params #:  300352
MACs:  14717248
 
[22:22:16] (INFO) Building library...
[22:22:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.63380126953125, 'median': 1.4656982421875, 'mins': 1.3751220703125}
 
(7, 3, 1, 192, 832, 320)
Params #:  433472
MACs:  21240128
 
[22:22:33] (INFO) Building library...
[22:22:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7983897816051135, 'median': 1.7703857421875, 'mins': 1.6883544921875}
 
(7, 3, 1, 192, 848, 160)
Params #:  306128
MACs:  15000272
 
[22:22:51] (INFO) Building library...
[22:22:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7927068536931818, 'median': 1.7752685546875, 'mins': 1.6943359375}
 
(7, 3, 1, 192, 848, 320)
Params #:  441808
MACs:  21648592
 
[22:23:08] (INFO) Building library...
[22:23:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6696300159801136, 'median': 1.644775390625, 'mins': 1.583251953125}
 
(7, 3, 1, 192, 864, 160)
Params #:  311904
MACs:  15283296
 
[22:23:25] (INFO) Building library...
[22:23:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6661876331676135, 'median': 1.65234375, 'mins': 1.58447265625}
 
(7, 3, 1, 192, 864, 320)
Params #:  450144
MACs:  22057056
 
[22:23:43] (INFO) Building library...
[22:23:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6848932439630682, 'median': 1.652099609375, 'mins': 1.572998046875}
 
(7, 3, 1, 192, 880, 160)
Params #:  317680
MACs:  15566320
 
[22:24:00] (INFO) Building library...
[22:24:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.664437588778409, 'median': 1.6405029296875, 'mins': 1.5618896484375}
 
(7, 3, 1, 192, 880, 320)
Params #:  458480
MACs:  22465520
 
[22:24:17] (INFO) Building library...
[22:24:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6756580699573864, 'median': 1.6678466796875, 'mins': 1.5994873046875}
 
(7, 3, 1, 192, 896, 160)
Params #:  323456
MACs:  15849344
 
[22:24:34] (INFO) Building library...
[22:24:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8052545720880682, 'median': 1.780517578125, 'mins': 1.7039794921875}
 
(7, 3, 1, 192, 896, 320)
Params #:  466816
MACs:  22873984
 
[22:24:52] (INFO) Building library...
[22:24:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.76793212890625, 'median': 1.760986328125, 'mins': 1.6976318359375}
 
(7, 3, 1, 192, 912, 160)
Params #:  329232
MACs:  16132368
 
[22:25:10] (INFO) Building library...
[22:25:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.701513671875, 'median': 1.673583984375, 'mins': 1.5830078125}
 
(7, 3, 1, 192, 912, 320)
Params #:  475152
MACs:  23282448
 
[22:25:27] (INFO) Building library...
[22:25:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6559326171875, 'median': 1.620849609375, 'mins': 1.5538330078125}
 
(7, 3, 1, 192, 928, 160)
Params #:  335008
MACs:  16415392
 
[22:25:44] (INFO) Building library...
[22:25:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.564620694247159, 'median': 1.5587158203125, 'mins': 1.3929443359375}
 
(7, 3, 1, 192, 928, 320)
Params #:  483488
MACs:  23690912
 
[22:26:02] (INFO) Building library...
[22:26:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8321588689630681, 'median': 1.6871337890625, 'mins': 1.57958984375}
 
(7, 3, 1, 192, 944, 160)
Params #:  340784
MACs:  16698416
 
[22:26:19] (INFO) Building library...
[22:26:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6203369140625, 'median': 1.61181640625, 'mins': 1.554931640625}
 
(7, 3, 1, 192, 944, 320)
Params #:  491824
MACs:  24099376
 
[22:26:37] (INFO) Building library...
[22:26:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6636130593039773, 'median': 1.635986328125, 'mins': 1.55517578125}
 
(7, 3, 1, 192, 960, 160)
Params #:  346560
MACs:  16981440
 
[22:26:54] (INFO) Building library...
[22:26:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4718283913352272, 'median': 1.4560546875, 'mins': 1.383056640625}
 
(7, 3, 1, 192, 960, 320)
Params #:  500160
MACs:  24507840
 
[22:27:11] (INFO) Building library...
[22:27:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5037708629261364, 'median': 1.4969482421875, 'mins': 1.42431640625}
 
(7, 3, 1, 192, 976, 160)
Params #:  352336
MACs:  17264464
 
[22:27:29] (INFO) Building library...
[22:27:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.664315518465909, 'median': 1.640625, 'mins': 1.5472412109375}
 
(7, 3, 1, 192, 976, 320)
Params #:  508496
MACs:  24916304
 
[22:27:46] (INFO) Building library...
[22:27:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8064630681818181, 'median': 1.774658203125, 'mins': 1.7056884765625}
 
(7, 3, 1, 192, 992, 160)
Params #:  358112
MACs:  17547488
 
[22:28:03] (INFO) Building library...
[22:28:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8192094282670455, 'median': 1.79541015625, 'mins': 1.7149658203125}
 
(7, 3, 1, 192, 992, 320)
Params #:  516832
MACs:  25324768
 
[22:28:21] (INFO) Building library...
[22:28:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.794986239346591, 'median': 1.7657470703125, 'mins': 1.5966796875}
 
(7, 3, 1, 192, 1008, 160)
Params #:  363888
MACs:  17830512
 
[22:28:38] (INFO) Building library...
[22:28:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7831243341619318, 'median': 1.7764892578125, 'mins': 1.700927734375}
 
(7, 3, 1, 192, 1008, 320)
Params #:  525168
MACs:  25733232
 
[22:28:56] (INFO) Building library...
[22:28:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.685726651278409, 'median': 1.66357421875, 'mins': 1.569580078125}
 
(7, 3, 1, 192, 1024, 160)
Params #:  369664
MACs:  18113536
 
[22:29:13] (INFO) Building library...
[22:29:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.803055087002841, 'median': 1.779541015625, 'mins': 1.706787109375}
 
(7, 3, 1, 192, 1024, 320)
Params #:  533504
MACs:  26141696
 
[22:29:30] (INFO) Building library...
[22:29:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8190873579545455, 'median': 1.780517578125, 'mins': 1.7059326171875}
 
(7, 3, 1, 192, 1040, 160)
Params #:  375440
MACs:  18396560
 
[22:29:47] (INFO) Building library...
[22:29:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6569202769886364, 'median': 1.6253662109375, 'mins': 1.5523681640625}
 
(7, 3, 1, 192, 1040, 320)
Params #:  541840
MACs:  26550160
 
[22:30:04] (INFO) Building library...
[22:30:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6577736594460226, 'median': 1.6304931640625, 'mins': 1.54052734375}
 
(7, 3, 1, 192, 1056, 160)
Params #:  381216
MACs:  18679584
 
[22:30:22] (INFO) Building library...
[22:30:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8698441938920454, 'median': 1.79736328125, 'mins': 1.708984375}
 
(7, 3, 1, 192, 1056, 320)
Params #:  550176
MACs:  26958624
 
[22:30:40] (INFO) Building library...
[22:30:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7503950639204546, 'median': 1.7447509765625, 'mins': 1.6722412109375}
 
(7, 3, 1, 192, 1072, 160)
Params #:  386992
MACs:  18962608
 
[22:30:58] (INFO) Building library...
[22:30:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9321366743607955, 'median': 1.7969970703125, 'mins': 1.716064453125}
 
(7, 3, 1, 192, 1072, 320)
Params #:  558512
MACs:  27367088
 
[22:31:16] (INFO) Building library...
[22:31:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.662789639559659, 'median': 1.6395263671875, 'mins': 1.551513671875}
 
(7, 3, 1, 192, 1088, 160)
Params #:  392768
MACs:  19245632
 
[22:31:33] (INFO) Building library...
[22:31:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7657049005681817, 'median': 1.742919921875, 'mins': 1.6611328125}
 
(7, 3, 1, 192, 1088, 320)
Params #:  566848
MACs:  27775552
 
[22:31:50] (INFO) Building library...
[22:31:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6734230735085227, 'median': 1.6690673828125, 'mins': 1.5875244140625}
 
(7, 3, 1, 192, 1104, 160)
Params #:  398544
MACs:  19528656
 
[22:32:08] (INFO) Building library...
[22:32:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.660897549715909, 'median': 1.6370849609375, 'mins': 1.5504150390625}
 
(7, 3, 1, 192, 1104, 320)
Params #:  575184
MACs:  28184016
 
[22:32:25] (INFO) Building library...
[22:32:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8352195046164772, 'median': 1.792236328125, 'mins': 1.7239990234375}
 
(7, 3, 1, 192, 1120, 160)
Params #:  404320
MACs:  19811680
 
[22:32:43] (INFO) Building library...
[22:32:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7820101651278408, 'median': 1.7640380859375, 'mins': 1.697021484375}
 
(7, 3, 1, 192, 1120, 320)
Params #:  583520
MACs:  28592480
 
[22:33:00] (INFO) Building library...
[22:33:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6576737837357955, 'median': 1.623291015625, 'mins': 1.53564453125}
 
(7, 3, 1, 192, 1136, 160)
Params #:  410096
MACs:  20094704
 
[22:33:17] (INFO) Building library...
[22:33:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6307117808948863, 'median': 1.62451171875, 'mins': 1.557861328125}
 
(7, 3, 1, 192, 1136, 320)
Params #:  591856
MACs:  29000944
 
[22:33:35] (INFO) Building library...
[22:33:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8047884854403409, 'median': 1.7808837890625, 'mins': 1.6890869140625}
 
(7, 3, 1, 192, 1152, 160)
Params #:  415872
MACs:  20377728
 
[22:33:52] (INFO) Building library...
[22:33:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.67413330078125, 'median': 1.6376953125, 'mins': 1.553955078125}
 
(7, 3, 1, 192, 1152, 320)
Params #:  600192
MACs:  29409408
 
[22:34:10] (INFO) Building library...
[22:34:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8294788707386365, 'median': 1.801025390625, 'mins': 1.719970703125}
 
(7, 3, 1, 192, 1168, 160)
Params #:  421648
MACs:  20660752
 
[22:34:27] (INFO) Building library...
[22:34:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8281649502840909, 'median': 1.7857666015625, 'mins': 1.7196044921875}
 
(7, 3, 1, 192, 1168, 320)
Params #:  608528
MACs:  29817872
 
[22:34:45] (INFO) Building library...
[22:34:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8249467329545455, 'median': 1.8070068359375, 'mins': 1.7296142578125}
 
(7, 3, 1, 192, 1184, 160)
Params #:  427424
MACs:  20943776
 
[22:35:02] (INFO) Building library...
[22:35:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.667557040127841, 'median': 1.6470947265625, 'mins': 1.558349609375}
 
(7, 3, 1, 192, 1184, 320)
Params #:  616864
MACs:  30226336
 
[22:35:19] (INFO) Building library...
[22:35:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7598976828835227, 'median': 1.7474365234375, 'mins': 1.6822509765625}
 
(7, 3, 1, 192, 1200, 160)
Params #:  433200
MACs:  21226800
 
[22:35:37] (INFO) Building library...
[22:35:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7888128107244319, 'median': 1.7637939453125, 'mins': 1.6776123046875}
 
(7, 3, 1, 192, 1200, 320)
Params #:  625200
MACs:  30634800
 
[22:35:54] (INFO) Building library...
[22:35:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8119118430397727, 'median': 1.76318359375, 'mins': 1.677734375}
 
(7, 3, 1, 192, 1216, 160)
Params #:  438976
MACs:  21509824
 
[22:36:12] (INFO) Building library...
[22:36:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7562233664772726, 'median': 1.75048828125, 'mins': 1.677978515625}
 
(7, 3, 1, 192, 1216, 320)
Params #:  633536
MACs:  31043264
 
[22:36:29] (INFO) Building library...
[22:36:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.776251775568182, 'median': 1.755126953125, 'mins': 1.66357421875}
 
(7, 3, 1, 192, 1232, 160)
Params #:  444752
MACs:  21792848
 
[22:36:46] (INFO) Building library...
[22:36:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6698441938920454, 'median': 1.651123046875, 'mins': 1.5625}
 
(7, 3, 1, 192, 1232, 320)
Params #:  641872
MACs:  31451728
 
[22:37:04] (INFO) Building library...
[22:37:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8812966086647727, 'median': 1.795654296875, 'mins': 1.7005615234375}
 
(7, 3, 1, 192, 1248, 160)
Params #:  450528
MACs:  22075872
 
[22:37:21] (INFO) Building library...
[22:37:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9577381480823863, 'median': 1.8170166015625, 'mins': 1.69091796875}
 
(7, 3, 1, 192, 1248, 320)
Params #:  650208
MACs:  31860192
 
[22:37:39] (INFO) Building library...
[22:37:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8021051580255683, 'median': 1.78271484375, 'mins': 1.697265625}
 
(7, 3, 1, 192, 1264, 160)
Params #:  456304
MACs:  22358896
 
[22:37:56] (INFO) Building library...
[22:37:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8366166548295455, 'median': 1.79638671875, 'mins': 1.71875}
 
(7, 3, 1, 192, 1264, 320)
Params #:  658544
MACs:  32268656
 
[22:38:14] (INFO) Building library...
[22:38:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7505814985795454, 'median': 1.7432861328125, 'mins': 1.6676025390625}
 
(7, 3, 1, 192, 1280, 160)
Params #:  462080
MACs:  22641920
 
[22:38:31] (INFO) Building library...
[22:38:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8009843306107955, 'median': 1.7852783203125, 'mins': 1.703857421875}
 
(7, 3, 1, 192, 1280, 320)
Params #:  666880
MACs:  32677120
 
[22:38:49] (INFO) Building library...
[22:38:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7518477006392046, 'median': 1.7396240234375, 'mins': 1.6671142578125}
 
(7, 3, 1, 192, 1296, 160)
Params #:  467856
MACs:  22924944
 
[22:39:06] (INFO) Building library...
[22:39:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8541437322443182, 'median': 1.825927734375, 'mins': 1.7464599609375}
 
(7, 3, 1, 192, 1296, 320)
Params #:  675216
MACs:  33085584
 
[22:39:24] (INFO) Building library...
[22:39:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8210660067471591, 'median': 1.806884765625, 'mins': 1.7333984375}
 
(7, 3, 1, 192, 1312, 160)
Params #:  473632
MACs:  23207968
 
[22:39:41] (INFO) Building library...
[22:39:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.80567626953125, 'median': 1.781494140625, 'mins': 1.6968994140625}
 
(7, 3, 1, 192, 1312, 320)
Params #:  683552
MACs:  33494048
 
[22:39:59] (INFO) Building library...
[22:39:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.781304376775568, 'median': 1.7664794921875, 'mins': 1.6905517578125}
 
(7, 3, 1, 192, 1328, 160)
Params #:  479408
MACs:  23490992
 
[22:40:16] (INFO) Building library...
[22:40:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7996160333806819, 'median': 1.781005859375, 'mins': 1.695556640625}
 
(7, 3, 1, 192, 1328, 320)
Params #:  691888
MACs:  33902512
 
[22:40:34] (INFO) Building library...
[22:40:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7613469904119319, 'median': 1.754150390625, 'mins': 1.6883544921875}
 
(7, 3, 1, 192, 1344, 160)
Params #:  485184
MACs:  23774016
 
[22:40:51] (INFO) Building library...
[22:40:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8675492720170455, 'median': 1.6866455078125, 'mins': 1.594482421875}
 
(7, 3, 1, 192, 1344, 320)
Params #:  700224
MACs:  34310976
 
[22:41:09] (INFO) Building library...
[22:41:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8141557173295455, 'median': 1.7877197265625, 'mins': 1.7059326171875}
 
(7, 3, 1, 192, 1360, 160)
Params #:  490960
MACs:  24057040
 
[22:41:27] (INFO) Building library...
[22:41:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6436168323863636, 'median': 1.612548828125, 'mins': 1.5467529296875}
 
(7, 3, 1, 192, 1360, 320)
Params #:  708560
MACs:  34719440
 
[22:41:44] (INFO) Building library...
[22:41:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8040749289772726, 'median': 1.77294921875, 'mins': 1.685791015625}
 
(7, 3, 1, 192, 1376, 160)
Params #:  496736
MACs:  24340064
 
[22:42:02] (INFO) Building library...
[22:42:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6738514293323863, 'median': 1.6522216796875, 'mins': 1.5877685546875}
 
(7, 3, 1, 192, 1376, 320)
Params #:  716896
MACs:  35127904
 
[22:42:19] (INFO) Building library...
[22:42:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6757302024147727, 'median': 1.6483154296875, 'mins': 1.5706787109375}
 
(7, 3, 1, 192, 1392, 160)
Params #:  502512
MACs:  24623088
 
[22:42:37] (INFO) Building library...
[22:42:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0073197798295452, 'median': 1.951416015625, 'mins': 1.86865234375}
 
(7, 3, 1, 192, 1392, 320)
Params #:  725232
MACs:  35536368
 
[22:42:54] (INFO) Building library...
[22:42:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7964166814630682, 'median': 1.7669677734375, 'mins': 1.6820068359375}
 
(7, 3, 1, 192, 1408, 160)
Params #:  508288
MACs:  24906112
 
[22:43:12] (INFO) Building library...
[22:43:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6790693803267045, 'median': 1.6488037109375, 'mins': 1.569091796875}
 
(7, 3, 1, 192, 1408, 320)
Params #:  733568
MACs:  35944832
 
[22:43:29] (INFO) Building library...
[22:43:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8402410333806818, 'median': 1.81103515625, 'mins': 1.7271728515625}
 
(7, 3, 1, 192, 1424, 160)
Params #:  514064
MACs:  25189136
 
[22:43:47] (INFO) Building library...
[22:43:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9130648526278409, 'median': 1.81201171875, 'mins': 1.710205078125}
 
(7, 3, 1, 192, 1424, 320)
Params #:  741904
MACs:  36353296
 
[22:44:04] (INFO) Building library...
[22:44:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7804643110795455, 'median': 1.7576904296875, 'mins': 1.668701171875}
 
(7, 3, 1, 192, 1440, 160)
Params #:  519840
MACs:  25472160
 
[22:44:22] (INFO) Building library...
[22:44:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7732266512784092, 'median': 1.7608642578125, 'mins': 1.689697265625}
 
(7, 3, 1, 192, 1440, 320)
Params #:  750240
MACs:  36761760
 
[22:44:39] (INFO) Building library...
[22:44:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.79521484375, 'median': 1.7659912109375, 'mins': 1.678955078125}
 
(7, 3, 1, 192, 1456, 160)
Params #:  525616
MACs:  25755184
 
[22:44:57] (INFO) Building library...
[22:44:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0932517311789773, 'median': 1.9508056640625, 'mins': 1.8592529296875}
 
(7, 3, 1, 192, 1456, 320)
Params #:  758576
MACs:  37170224
 
[22:45:14] (INFO) Building library...
[22:45:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9510009765625, 'median': 1.8182373046875, 'mins': 1.719970703125}
 
(7, 3, 1, 192, 1472, 160)
Params #:  531392
MACs:  26038208
 
[22:45:32] (INFO) Building library...
[22:45:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7916526100852272, 'median': 1.7674560546875, 'mins': 1.6890869140625}
 
(7, 3, 1, 192, 1472, 320)
Params #:  766912
MACs:  37578688
 
[22:45:50] (INFO) Building library...
[22:45:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7862903941761363, 'median': 1.7772216796875, 'mins': 1.709716796875}
 
(7, 3, 1, 192, 1488, 160)
Params #:  537168
MACs:  26321232
 
[22:46:07] (INFO) Building library...
[22:46:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.811831942471591, 'median': 1.7841796875, 'mins': 1.7041015625}
 
(7, 3, 1, 192, 1488, 320)
Params #:  775248
MACs:  37987152
 
[22:46:25] (INFO) Building library...
[22:46:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7769564541903409, 'median': 1.76708984375, 'mins': 1.69921875}
 
(7, 3, 1, 192, 1504, 160)
Params #:  542944
MACs:  26604256
 
[22:46:42] (INFO) Building library...
[22:46:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7851451526988635, 'median': 1.7593994140625, 'mins': 1.6807861328125}
 
(7, 3, 1, 192, 1504, 320)
Params #:  783584
MACs:  38395616
 
[22:47:00] (INFO) Building library...
[22:47:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6369318181818182, 'median': 1.6256103515625, 'mins': 1.5546875}
 
(7, 3, 1, 192, 1520, 160)
Params #:  548720
MACs:  26887280
 
[22:47:17] (INFO) Building library...
[22:47:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.82457275390625, 'median': 1.8076171875, 'mins': 1.7066650390625}
 
(7, 3, 1, 192, 1520, 320)
Params #:  791920
MACs:  38804080
 
[22:47:35] (INFO) Building library...
[22:47:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7498279918323865, 'median': 1.740966796875, 'mins': 1.6754150390625}
 
(7, 3, 1, 192, 1536, 160)
Params #:  554496
MACs:  27170304
 
[22:47:52] (INFO) Building library...
[22:47:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6704290216619317, 'median': 1.6373291015625, 'mins': 1.5501708984375}
 
(7, 3, 1, 192, 1536, 320)
Params #:  800256
MACs:  39212544
 
[22:48:10] (INFO) Building library...
[22:48:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6061734286221592, 'median': 1.593017578125, 'mins': 1.528076171875}
 
(7, 3, 1, 192, 1552, 160)
Params #:  560272
MACs:  27453328
 
[22:48:27] (INFO) Building library...
[22:48:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.808358487215909, 'median': 1.789794921875, 'mins': 1.6978759765625}
 
(7, 3, 1, 192, 1552, 320)
Params #:  808592
MACs:  39621008
 
[22:48:45] (INFO) Building library...
[22:48:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7592717950994319, 'median': 1.751220703125, 'mins': 1.675048828125}
 
(7, 3, 1, 192, 1568, 160)
Params #:  566048
MACs:  27736352
 
[22:49:02] (INFO) Building library...
[22:49:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9380060369318182, 'median': 1.8975830078125, 'mins': 1.69580078125}
 
(7, 3, 1, 192, 1568, 320)
Params #:  816928
MACs:  40029472
 
[22:49:20] (INFO) Building library...
[22:49:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7937932794744318, 'median': 1.77880859375, 'mins': 1.7022705078125}
 
(7, 3, 1, 192, 1584, 160)
Params #:  571824
MACs:  28019376
 
[22:49:38] (INFO) Building library...
[22:49:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8319324840198863, 'median': 1.78857421875, 'mins': 1.70654296875}
 
(7, 3, 1, 192, 1584, 320)
Params #:  825264
MACs:  40437936
 
[22:49:55] (INFO) Building library...
[22:49:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8586803089488637, 'median': 1.8392333984375, 'mins': 1.7496337890625}
 
(7, 3, 1, 192, 1600, 160)
Params #:  577600
MACs:  28302400
 
[22:50:13] (INFO) Building library...
[22:50:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.750439453125, 'median': 1.7386474609375, 'mins': 1.676025390625}
 
(7, 3, 1, 192, 1600, 320)
Params #:  833600
MACs:  40846400
 
[22:50:30] (INFO) Building library...
[22:50:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9564353249289772, 'median': 1.8355712890625, 'mins': 1.726318359375}
 
(7, 3, 1, 192, 1616, 160)
Params #:  583376
MACs:  28585424
 
[22:50:48] (INFO) Building library...
[22:50:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8242531516335228, 'median': 1.8260498046875, 'mins': 1.5999755859375}
 
(7, 3, 1, 192, 1616, 320)
Params #:  841936
MACs:  41254864
 
[22:51:06] (INFO) Building library...
[22:51:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7671852805397728, 'median': 1.756103515625, 'mins': 1.6925048828125}
 
(7, 3, 1, 192, 1632, 160)
Params #:  589152
MACs:  28868448
 
[22:51:23] (INFO) Building library...
[22:51:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8171309037642045, 'median': 1.802734375, 'mins': 1.6871337890625}
 
(7, 3, 1, 192, 1632, 320)
Params #:  850272
MACs:  41663328
 
[22:51:41] (INFO) Building library...
[22:51:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7765636097301136, 'median': 1.7686767578125, 'mins': 1.694091796875}
 
(7, 3, 1, 192, 1648, 160)
Params #:  594928
MACs:  29151472
 
[22:51:58] (INFO) Building library...
[22:51:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.818472567471591, 'median': 1.787841796875, 'mins': 1.697998046875}
 
(7, 3, 1, 192, 1648, 320)
Params #:  858608
MACs:  42071792
 
[22:52:16] (INFO) Building library...
[22:52:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7672230113636365, 'median': 1.7406005859375, 'mins': 1.6708984375}
 
(7, 3, 1, 192, 1664, 160)
Params #:  600704
MACs:  29434496
 
[22:52:33] (INFO) Building library...
[22:52:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9310591264204546, 'median': 1.9207763671875, 'mins': 1.7413330078125}
 
(7, 3, 1, 192, 1664, 320)
Params #:  866944
MACs:  42480256
 
[22:52:51] (INFO) Building library...
[22:52:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.734427157315341, 'median': 1.7318115234375, 'mins': 1.6649169921875}
 
(7, 3, 1, 192, 1680, 160)
Params #:  606480
MACs:  29717520
 
[22:53:09] (INFO) Building library...
[22:53:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7924416281960227, 'median': 1.774169921875, 'mins': 1.6854248046875}
 
(7, 3, 1, 192, 1680, 320)
Params #:  875280
MACs:  42888720
 
[22:53:26] (INFO) Building library...
[22:53:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7621293501420454, 'median': 1.756591796875, 'mins': 1.693115234375}
 
(7, 3, 1, 192, 1696, 160)
Params #:  612256
MACs:  30000544
 
[22:53:44] (INFO) Building library...
[22:53:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7878562233664772, 'median': 1.768798828125, 'mins': 1.692626953125}
 
(7, 3, 1, 192, 1696, 320)
Params #:  883616
MACs:  43297184
 
[22:54:01] (INFO) Building library...
[22:54:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7721846147017046, 'median': 1.759033203125, 'mins': 1.69580078125}
 
(7, 3, 1, 192, 1712, 160)
Params #:  618032
MACs:  30283568
 
[22:54:19] (INFO) Building library...
[22:54:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7116754705255681, 'median': 1.6885986328125, 'mins': 1.6024169921875}
 
(7, 3, 1, 192, 1712, 320)
Params #:  891952
MACs:  43705648
 
[22:54:36] (INFO) Building library...
[22:54:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7744884144176136, 'median': 1.756591796875, 'mins': 1.6829833984375}
 
(112, 5, 2, 16, 32, 16)
Params #:  1824
MACs:  10536960
 
[22:54:54] (INFO) Building library...
[22:54:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8295876242897726, 'median': 1.8184814453125, 'mins': 1.72802734375}
 
(112, 5, 2, 16, 32, 32)
Params #:  2336
MACs:  12142592
 
[22:55:13] (INFO) Building library...
[22:55:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6591397372159091, 'median': 1.6378173828125, 'mins': 1.5433349609375}
 
(112, 5, 2, 16, 48, 16)
Params #:  2736
MACs:  15805440
 
[22:55:32] (INFO) Building library...
[22:55:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.090667169744318, 'median': 2.0809326171875, 'mins': 1.9591064453125}
 
(112, 5, 2, 16, 48, 32)
Params #:  3504
MACs:  18213888
 
[22:55:50] (INFO) Building library...
[22:55:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.188759543678977, 'median': 2.1119384765625, 'mins': 2.0078125}
 
(112, 5, 2, 16, 64, 16)
Params #:  3648
MACs:  21073920
 
[22:56:08] (INFO) Building library...
[22:56:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.492403897372159, 'median': 2.47119140625, 'mins': 2.2889404296875}
 
(112, 5, 2, 16, 64, 32)
Params #:  4672
MACs:  24285184
 
[22:56:27] (INFO) Building library...
[22:56:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.425794566761364, 'median': 2.3720703125, 'mins': 2.242919921875}
 
(56, 3, 1, 32, 16, 16)
Params #:  912
MACs:  2860032
 
[22:56:46] (INFO) Building library...
[22:56:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.63193359375, 'median': 1.5899658203125, 'mins': 1.5067138671875}
 
(56, 3, 1, 32, 16, 32)
Params #:  1168
MACs:  3662848
 
[22:57:03] (INFO) Building library...
[22:57:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2712957208806817, 'median': 2.232421875, 'mins': 2.1251220703125}
 
(56, 3, 1, 32, 32, 16)
Params #:  1824
MACs:  5720064
 
[22:57:23] (INFO) Building library...
[22:57:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.9555763938210228, 'median': 0.9273681640625, 'mins': 0.8394775390625}
 
(56, 3, 1, 32, 32, 32)
Params #:  2336
MACs:  7325696
 
[22:57:38] (INFO) Building library...
[22:57:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4399724786931818, 'median': 1.425048828125, 'mins': 1.3466796875}
 
(56, 3, 1, 32, 48, 16)
Params #:  2736
MACs:  8580096
 
[22:57:55] (INFO) Building library...
[22:57:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7218705610795455, 'median': 1.694580078125, 'mins': 1.6134033203125}
 
(56, 3, 1, 32, 48, 32)
Params #:  3504
MACs:  10988544
 
[22:58:12] (INFO) Building library...
[22:58:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6387739701704547, 'median': 2.656982421875, 'mins': 2.307373046875}
 
(56, 3, 1, 32, 64, 16)
Params #:  3648
MACs:  11440128
 
[22:58:31] (INFO) Building library...
[22:58:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5234319513494319, 'median': 1.50048828125, 'mins': 1.4173583984375}
 
(56, 3, 1, 32, 64, 32)
Params #:  4672
MACs:  14651392
 
[22:58:49] (INFO) Building library...
[22:58:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3495827414772728, 'median': 2.3009033203125, 'mins': 2.20703125}
 
(56, 3, 1, 32, 80, 16)
Params #:  4560
MACs:  14300160
 
[22:59:09] (INFO) Building library...
[22:59:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.949935635653409, 'median': 1.90869140625, 'mins': 1.8193359375}
 
(56, 3, 1, 32, 80, 32)
Params #:  5840
MACs:  18314240
 
[22:59:27] (INFO) Building library...
[22:59:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8700117631392046, 'median': 2.8392333984375, 'mins': 2.7203369140625}
 
(56, 3, 1, 32, 96, 16)
Params #:  5472
MACs:  17160192
 
[22:59:47] (INFO) Building library...
[22:59:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0841264204545453, 'median': 1.968017578125, 'mins': 1.864013671875}
 
(56, 3, 1, 32, 96, 32)
Params #:  7008
MACs:  21977088
 
[23:00:05] (INFO) Building library...
[23:00:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.816657049005682, 'median': 2.7818603515625, 'mins': 2.6744384765625}
 
(56, 3, 1, 32, 112, 16)
Params #:  6384
MACs:  20020224
 
[23:00:26] (INFO) Building library...
[23:00:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7790971235795454, 'median': 1.746826171875, 'mins': 1.6622314453125}
 
(56, 3, 1, 32, 112, 32)
Params #:  8176
MACs:  25639936
 
[23:00:44] (INFO) Building library...
[23:00:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.600861150568182, 'median': 2.5557861328125, 'mins': 2.44921875}
 
(56, 3, 1, 32, 128, 16)
Params #:  7296
MACs:  22880256
 
[23:01:03] (INFO) Building library...
[23:01:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7259144176136363, 'median': 1.69677734375, 'mins': 1.603515625}
 
(56, 3, 1, 32, 128, 32)
Params #:  9344
MACs:  29302784
 
[23:01:21] (INFO) Building library...
[23:01:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.686159446022727, 'median': 2.6556396484375, 'mins': 2.5311279296875}
 
(56, 7, 2, 32, 16, 20)
Params #:  1616
MACs:  2471168
 
[23:01:40] (INFO) Building library...
[23:01:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6067737926136363, 'median': 1.58984375, 'mins': 1.4971923828125}
 
(56, 7, 2, 32, 16, 40)
Params #:  1936
MACs:  2722048
 
[23:01:58] (INFO) Building library...
[23:01:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.898245516690341, 'median': 1.880615234375, 'mins': 1.7672119140625}
 
(56, 7, 2, 32, 32, 20)
Params #:  3232
MACs:  4942336
 
[23:02:16] (INFO) Building library...
[23:02:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.9407259854403409, 'median': 0.9261474609375, 'mins': 0.84814453125}
 
(56, 7, 2, 32, 32, 40)
Params #:  3872
MACs:  5444096
 
[23:02:31] (INFO) Building library...
[23:02:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.95069580078125, 'median': 0.9154052734375, 'mins': 0.845947265625}
 
(56, 7, 2, 32, 48, 20)
Params #:  4848
MACs:  7413504
 
[23:02:46] (INFO) Building library...
[23:02:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9528542258522728, 'median': 1.9171142578125, 'mins': 1.82470703125}
 
(56, 7, 2, 32, 48, 40)
Params #:  5808
MACs:  8166144
 
[23:03:04] (INFO) Building library...
[23:03:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7525634765625, 'median': 1.7169189453125, 'mins': 1.627197265625}
 
(56, 7, 2, 32, 64, 20)
Params #:  6464
MACs:  9884672
 
[23:03:22] (INFO) Building library...
[23:03:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.74073486328125, 'median': 1.6849365234375, 'mins': 1.6024169921875}
 
(56, 7, 2, 32, 64, 40)
Params #:  7744
MACs:  10888192
 
[23:03:40] (INFO) Building library...
[23:03:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5989968039772726, 'median': 1.5711669921875, 'mins': 1.4986572265625}
 
(56, 7, 2, 32, 80, 20)
Params #:  8080
MACs:  12355840
 
[23:03:59] (INFO) Building library...
[23:03:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6357111150568182, 'median': 1.60986328125, 'mins': 1.5185546875}
 
(56, 7, 2, 32, 80, 40)
Params #:  9680
MACs:  13610240
 
[23:04:16] (INFO) Building library...
[23:04:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0890425248579545, 'median': 2.042724609375, 'mins': 1.9361572265625}
 
(56, 7, 2, 32, 96, 20)
Params #:  9696
MACs:  14827008
 
[23:04:34] (INFO) Building library...
[23:04:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.229657537286932, 'median': 2.1395263671875, 'mins': 2.0399169921875}
 
(56, 7, 2, 32, 96, 40)
Params #:  11616
MACs:  16332288
 
[23:04:53] (INFO) Building library...
[23:04:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.912002840909091, 'median': 1.796630859375, 'mins': 1.7081298828125}
 
(56, 7, 2, 32, 112, 20)
Params #:  11312
MACs:  17298176
 
[23:05:11] (INFO) Building library...
[23:05:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7684858842329545, 'median': 1.74169921875, 'mins': 1.6602783203125}
 
(56, 7, 2, 32, 112, 40)
Params #:  13552
MACs:  19054336
 
[23:05:29] (INFO) Building library...
[23:05:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.034140846946023, 'median': 1.9229736328125, 'mins': 1.8189697265625}
 
(56, 7, 2, 32, 128, 20)
Params #:  12928
MACs:  19769344
 
[23:05:47] (INFO) Building library...
[23:05:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8140070134943183, 'median': 1.712646484375, 'mins': 1.607177734375}
 
(56, 7, 2, 32, 128, 40)
Params #:  15488
MACs:  21776384
 
[23:06:05] (INFO) Building library...
[23:06:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8382390802556818, 'median': 1.731689453125, 'mins': 1.63525390625}
 
(28, 3, 1, 40, 16, 20)
Params #:  1104
MACs:  865536
 
[23:06:23] (INFO) Building library...
[23:06:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5299083362926136, 'median': 1.5028076171875, 'mins': 1.4251708984375}
 
(28, 3, 1, 40, 16, 40)
Params #:  1424
MACs:  1116416
 
[23:06:40] (INFO) Building library...
[23:06:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1925470525568183, 'median': 2.181640625, 'mins': 2.102294921875}
 
(28, 3, 1, 40, 32, 20)
Params #:  2208
MACs:  1731072
 
[23:06:59] (INFO) Building library...
[23:06:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.57138671875, 'median': 1.5458984375, 'mins': 1.4578857421875}
 
(28, 3, 1, 40, 32, 40)
Params #:  2848
MACs:  2232832
 
[23:07:16] (INFO) Building library...
[23:07:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.250265225497159, 'median': 2.22509765625, 'mins': 2.138427734375}
 
(28, 3, 1, 40, 48, 20)
Params #:  3312
MACs:  2596608
 
[23:07:35] (INFO) Building library...
[23:07:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.809933194247159, 'median': 1.682861328125, 'mins': 1.593017578125}
 
(28, 3, 1, 40, 48, 40)
Params #:  4272
MACs:  3349248
 
[23:07:52] (INFO) Building library...
[23:07:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3761696555397727, 'median': 2.343017578125, 'mins': 2.2257080078125}
 
(28, 3, 1, 40, 64, 20)
Params #:  4416
MACs:  3462144
 
[23:08:11] (INFO) Building library...
[23:08:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6357976740056819, 'median': 1.616455078125, 'mins': 1.5345458984375}
 
(28, 3, 1, 40, 64, 40)
Params #:  5696
MACs:  4465664
 
[23:08:28] (INFO) Building library...
[23:08:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.380038174715909, 'median': 2.3441162109375, 'mins': 2.2635498046875}
 
(28, 3, 1, 40, 80, 20)
Params #:  5520
MACs:  4327680
 
[23:08:47] (INFO) Building library...
[23:08:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7375377308238635, 'median': 1.71533203125, 'mins': 1.630126953125}
 
(28, 3, 1, 40, 80, 40)
Params #:  7120
MACs:  5582080
 
[23:09:04] (INFO) Building library...
[23:09:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.25518798828125, 'median': 2.2252197265625, 'mins': 2.1146240234375}
 
(28, 3, 1, 40, 96, 20)
Params #:  6624
MACs:  5193216
 
[23:09:23] (INFO) Building library...
[23:09:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5543079723011364, 'median': 1.533447265625, 'mins': 1.463134765625}
 
(28, 3, 1, 40, 96, 40)
Params #:  8544
MACs:  6698496
 
[23:09:40] (INFO) Building library...
[23:09:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.65286865234375, 'median': 2.64208984375, 'mins': 2.38671875}
 
(28, 3, 1, 40, 112, 20)
Params #:  7728
MACs:  6058752
 
[23:09:59] (INFO) Building library...
[23:09:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7087546608664772, 'median': 1.5501708984375, 'mins': 1.466064453125}
 
(28, 3, 1, 40, 112, 40)
Params #:  9968
MACs:  7814912
 
[23:10:16] (INFO) Building library...
[23:10:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5059614701704547, 'median': 2.4659423828125, 'mins': 2.3717041015625}
 
(28, 3, 1, 40, 128, 20)
Params #:  8832
MACs:  6924288
 
[23:10:36] (INFO) Building library...
[23:10:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5546098188920454, 'median': 1.5418701171875, 'mins': 1.458251953125}
 
(28, 3, 1, 40, 128, 40)
Params #:  11392
MACs:  8931328
 
[23:10:53] (INFO) Building library...
[23:10:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.328172718394886, 'median': 2.322509765625, 'mins': 2.2366943359375}
 
(28, 3, 1, 40, 144, 20)
Params #:  9936
MACs:  7789824
 
[23:11:11] (INFO) Building library...
[23:11:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6861128373579546, 'median': 1.68017578125, 'mins': 1.60400390625}
 
(28, 3, 1, 40, 144, 40)
Params #:  12816
MACs:  10047744
 
[23:11:28] (INFO) Building library...
[23:11:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.563077059659091, 'median': 2.52880859375, 'mins': 2.418212890625}
 
(28, 3, 1, 40, 160, 20)
Params #:  11040
MACs:  8655360
 
[23:11:47] (INFO) Building library...
[23:11:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9754139293323865, 'median': 1.966064453125, 'mins': 1.8709716796875}
 
(28, 3, 1, 40, 160, 40)
Params #:  14240
MACs:  11164160
 
[23:12:04] (INFO) Building library...
[23:12:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4975386186079547, 'median': 2.4715576171875, 'mins': 2.359619140625}
 
(28, 3, 1, 40, 176, 20)
Params #:  12144
MACs:  9520896
 
[23:12:24] (INFO) Building library...
[23:12:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.79813232421875, 'median': 1.772705078125, 'mins': 1.6854248046875}
 
(28, 3, 1, 40, 176, 40)
Params #:  15664
MACs:  12280576
 
[23:12:41] (INFO) Building library...
[23:12:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5020585493607954, 'median': 2.47021484375, 'mins': 2.3782958984375}
 
(28, 7, 2, 40, 16, 40)
Params #:  2064
MACs:  780864
 
[23:13:00] (INFO) Building library...
[23:13:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4389093572443181, 'median': 1.416015625, 'mins': 1.3419189453125}
 
(28, 7, 2, 40, 16, 80)
Params #:  2704
MACs:  906304
 
[23:13:18] (INFO) Building library...
[23:13:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5434348366477273, 'median': 1.5286865234375, 'mins': 1.4642333984375}
 
(28, 7, 2, 40, 32, 40)
Params #:  4128
MACs:  1561728
 
[23:13:35] (INFO) Building library...
[23:13:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6332608309659091, 'median': 1.592041015625, 'mins': 1.479736328125}
 
(28, 7, 2, 40, 32, 80)
Params #:  5408
MACs:  1812608
 
[23:13:53] (INFO) Building library...
[23:13:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.62086181640625, 'median': 1.580322265625, 'mins': 1.48291015625}
 
(28, 7, 2, 40, 48, 40)
Params #:  6192
MACs:  2342592
 
[23:14:11] (INFO) Building library...
[23:14:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5394753196022728, 'median': 1.5179443359375, 'mins': 1.447998046875}
 
(28, 7, 2, 40, 48, 80)
Params #:  8112
MACs:  2718912
 
[23:14:28] (INFO) Building library...
[23:14:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.556997958096591, 'median': 1.5250244140625, 'mins': 1.4327392578125}
 
(28, 7, 2, 40, 64, 40)
Params #:  8256
MACs:  3123456
 
[23:14:46] (INFO) Building library...
[23:14:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8114024769176136, 'median': 1.7042236328125, 'mins': 1.604248046875}
 
(28, 7, 2, 40, 64, 80)
Params #:  10816
MACs:  3625216
 
[23:15:04] (INFO) Building library...
[23:15:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7042735706676135, 'median': 1.68994140625, 'mins': 1.616943359375}
 
(28, 7, 2, 40, 80, 40)
Params #:  10320
MACs:  3904320
 
[23:15:22] (INFO) Building library...
[23:15:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6869861949573863, 'median': 1.6639404296875, 'mins': 1.58544921875}
 
(28, 7, 2, 40, 80, 80)
Params #:  13520
MACs:  4531520
 
[23:15:39] (INFO) Building library...
[23:15:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7039007013494318, 'median': 1.6910400390625, 'mins': 1.62158203125}
 
(28, 7, 2, 40, 96, 40)
Params #:  12384
MACs:  4685184
 
[23:15:56] (INFO) Building library...
[23:15:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7216752485795455, 'median': 1.6964111328125, 'mins': 1.6171875}
 
(28, 7, 2, 40, 96, 80)
Params #:  16224
MACs:  5437824
 
[23:16:14] (INFO) Building library...
[23:16:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6758744673295454, 'median': 1.6566162109375, 'mins': 1.5755615234375}
 
(28, 7, 2, 40, 112, 40)
Params #:  14448
MACs:  5466048
 
[23:16:32] (INFO) Building library...
[23:16:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4875732421875, 'median': 1.475341796875, 'mins': 1.416015625}
 
(28, 7, 2, 40, 112, 80)
Params #:  18928
MACs:  6344128
 
[23:16:49] (INFO) Building library...
[23:16:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.729546564275568, 'median': 1.7000732421875, 'mins': 1.5987548828125}
 
(28, 7, 2, 40, 128, 40)
Params #:  16512
MACs:  6246912
 
[23:17:07] (INFO) Building library...
[23:17:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5402488014914772, 'median': 1.531005859375, 'mins': 1.455078125}
 
(28, 7, 2, 40, 128, 80)
Params #:  21632
MACs:  7250432
 
[23:17:24] (INFO) Building library...
[23:17:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7543767755681818, 'median': 1.731201171875, 'mins': 1.6536865234375}
 
(28, 7, 2, 40, 144, 40)
Params #:  18576
MACs:  7027776
 
[23:17:42] (INFO) Building library...
[23:17:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7568303888494319, 'median': 1.7342529296875, 'mins': 1.64208984375}
 
(28, 7, 2, 40, 144, 80)
Params #:  24336
MACs:  8156736
 
[23:18:00] (INFO) Building library...
[23:18:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7886030717329546, 'median': 1.6590576171875, 'mins': 1.5616455078125}
 
(28, 7, 2, 40, 160, 40)
Params #:  20640
MACs:  7808640
 
[23:18:18] (INFO) Building library...
[23:18:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.731509676846591, 'median': 1.7069091796875, 'mins': 1.608642578125}
 
(28, 7, 2, 40, 160, 80)
Params #:  27040
MACs:  9063040
 
[23:18:35] (INFO) Building library...
[23:18:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7913307883522727, 'median': 1.7645263671875, 'mins': 1.646728515625}
 
(28, 7, 2, 40, 176, 40)
Params #:  22704
MACs:  8589504
 
[23:18:53] (INFO) Building library...
[23:18:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6846091530539773, 'median': 1.67333984375, 'mins': 1.597412109375}
 
(28, 7, 2, 40, 176, 80)
Params #:  29744
MACs:  9969344
 
[23:19:11] (INFO) Building library...
[23:19:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7545376864346591, 'median': 1.7108154296875, 'mins': 1.633056640625}
 
(28, 7, 2, 40, 192, 40)
Params #:  24768
MACs:  9370368
 
[23:19:28] (INFO) Building library...
[23:19:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8367098721590909, 'median': 1.7996826171875, 'mins': 1.701904296875}
 
(28, 7, 2, 40, 192, 80)
Params #:  32448
MACs:  10875648
 
[23:19:46] (INFO) Building library...
[23:19:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.677783203125, 'median': 1.6622314453125, 'mins': 1.5887451171875}
 
(28, 7, 2, 40, 208, 40)
Params #:  26832
MACs:  10151232
 
[23:20:04] (INFO) Building library...
[23:20:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.55240478515625, 'median': 1.533935546875, 'mins': 1.470947265625}
 
(28, 7, 2, 40, 208, 80)
Params #:  35152
MACs:  11781952
 
[23:20:22] (INFO) Building library...
[23:20:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.55145263671875, 'median': 1.5267333984375, 'mins': 1.447509765625}
 
(28, 7, 2, 40, 224, 40)
Params #:  28896
MACs:  10932096
 
[23:20:39] (INFO) Building library...
[23:20:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.727347079190341, 'median': 1.7020263671875, 'mins': 1.6246337890625}
 
(28, 7, 2, 40, 224, 80)
Params #:  37856
MACs:  12688256
 
[23:20:57] (INFO) Building library...
[23:20:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8227217240767046, 'median': 1.7027587890625, 'mins': 1.6092529296875}
 
(28, 7, 2, 40, 240, 40)
Params #:  30960
MACs:  11712960
 
[23:21:15] (INFO) Building library...
[23:21:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5224775834517046, 'median': 1.50244140625, 'mins': 1.427001953125}
 
(28, 7, 2, 40, 240, 80)
Params #:  40560
MACs:  13594560
 
[23:21:32] (INFO) Building library...
[23:21:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7330111416903409, 'median': 1.7083740234375, 'mins': 1.634765625}
 
(28, 7, 2, 40, 256, 40)
Params #:  33024
MACs:  12493824
 
[23:21:50] (INFO) Building library...
[23:21:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7078291459517045, 'median': 1.677001953125, 'mins': 1.590087890625}
 
(28, 7, 2, 40, 256, 80)
Params #:  43264
MACs:  14500864
 
[23:22:08] (INFO) Building library...
[23:22:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7409767844460227, 'median': 1.720703125, 'mins': 1.623779296875}
 
(28, 7, 2, 40, 272, 40)
Params #:  35088
MACs:  13274688
 
[23:22:25] (INFO) Building library...
[23:22:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.753167169744318, 'median': 1.732666015625, 'mins': 1.65380859375}
 
(28, 7, 2, 40, 272, 80)
Params #:  45968
MACs:  15407168
 
[23:22:43] (INFO) Building library...
[23:22:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7067693536931818, 'median': 1.6708984375, 'mins': 1.596435546875}
 
(28, 7, 2, 40, 288, 40)
Params #:  37152
MACs:  14055552
 
[23:23:00] (INFO) Building library...
[23:23:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7222079190340909, 'median': 1.709716796875, 'mins': 1.636962890625}
 
(28, 7, 2, 40, 288, 80)
Params #:  48672
MACs:  16313472
 
[23:23:18] (INFO) Building library...
[23:23:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.739501953125, 'median': 1.7080078125, 'mins': 1.630126953125}
 
(28, 7, 2, 40, 304, 40)
Params #:  39216
MACs:  14836416
 
[23:23:36] (INFO) Building library...
[23:23:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8967340642755681, 'median': 1.760498046875, 'mins': 1.65478515625}
 
(28, 7, 2, 40, 304, 80)
Params #:  51376
MACs:  17219776
 
[23:23:54] (INFO) Building library...
[23:23:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.726620205965909, 'median': 1.6978759765625, 'mins': 1.6116943359375}
 
(28, 7, 2, 40, 320, 40)
Params #:  41280
MACs:  15617280
 
[23:24:11] (INFO) Building library...
[23:24:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7719460227272728, 'median': 1.74462890625, 'mins': 1.6585693359375}
 
(28, 7, 2, 40, 320, 80)
Params #:  54080
MACs:  18126080
 
[23:24:29] (INFO) Building library...
[23:24:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6678644353693182, 'median': 1.656494140625, 'mins': 1.5880126953125}
 
(28, 7, 2, 40, 336, 40)
Params #:  43344
MACs:  16398144
 
[23:24:47] (INFO) Building library...
[23:24:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8912630948153408, 'median': 1.72021484375, 'mins': 1.63671875}
 
(28, 7, 2, 40, 336, 80)
Params #:  56784
MACs:  19032384
 
[23:25:05] (INFO) Building library...
[23:25:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.900521573153409, 'median': 1.741455078125, 'mins': 1.658447265625}
 
(28, 7, 2, 40, 352, 40)
Params #:  45408
MACs:  17179008
 
[23:25:23] (INFO) Building library...
[23:25:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.708984375, 'median': 1.548583984375, 'mins': 1.4698486328125}
 
(28, 7, 2, 40, 352, 80)
Params #:  59488
MACs:  19938688
 
[23:25:41] (INFO) Building library...
[23:25:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8356023615056818, 'median': 1.798583984375, 'mins': 1.6954345703125}
 
(14, 5, 1, 80, 16, 48)
Params #:  2448
MACs:  479808
 
[23:25:59] (INFO) Building library...
[23:25:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5134488192471591, 'median': 1.48974609375, 'mins': 1.432373046875}
 
(14, 5, 1, 80, 16, 96)
Params #:  3216
MACs:  630336
 
[23:26:16] (INFO) Building library...
[23:26:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4703557794744317, 'median': 1.3671875, 'mins': 1.294189453125}
 
(14, 5, 1, 80, 32, 48)
Params #:  4896
MACs:  959616
 
[23:26:34] (INFO) Building library...
[23:26:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.515618341619318, 'median': 1.485595703125, 'mins': 1.40283203125}
 
(14, 5, 1, 80, 32, 96)
Params #:  6432
MACs:  1260672
 
[23:26:51] (INFO) Building library...
[23:26:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6493485884232955, 'median': 1.509521484375, 'mins': 1.41748046875}
 
(14, 5, 1, 80, 48, 48)
Params #:  7344
MACs:  1439424
 
[23:27:09] (INFO) Building library...
[23:27:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5261518998579546, 'median': 1.50146484375, 'mins': 1.436279296875}
 
(14, 5, 1, 80, 48, 96)
Params #:  9648
MACs:  1891008
 
[23:27:26] (INFO) Building library...
[23:27:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4410999644886364, 'median': 1.432373046875, 'mins': 1.365234375}
 
(14, 5, 1, 80, 64, 48)
Params #:  9792
MACs:  1919232
 
[23:27:43] (INFO) Building library...
[23:27:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6443215110085228, 'median': 1.613037109375, 'mins': 1.533203125}
 
(14, 5, 1, 80, 64, 96)
Params #:  12864
MACs:  2521344
 
[23:28:00] (INFO) Building library...
[23:28:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6289439808238637, 'median': 1.598388671875, 'mins': 1.5338134765625}
 
(14, 5, 1, 80, 96, 48)
Params #:  14688
MACs:  2878848
 
[23:28:17] (INFO) Building library...
[23:28:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4853826349431818, 'median': 1.4674072265625, 'mins': 1.402587890625}
 
(14, 5, 1, 80, 96, 96)
Params #:  19296
MACs:  3782016
 
[23:28:34] (INFO) Building library...
[23:28:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4969648881392046, 'median': 1.4764404296875, 'mins': 1.4071044921875}
 
(14, 5, 1, 80, 112, 48)
Params #:  17136
MACs:  3358656
 
[23:28:51] (INFO) Building library...
[23:28:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.650094327059659, 'median': 1.624267578125, 'mins': 1.5439453125}
 
(14, 5, 1, 80, 112, 96)
Params #:  22512
MACs:  4412352
 
[23:29:09] (INFO) Building library...
[23:29:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6582785866477272, 'median': 1.624755859375, 'mins': 1.5445556640625}
 
(14, 5, 1, 80, 128, 48)
Params #:  19584
MACs:  3838464
 
[23:29:26] (INFO) Building library...
[23:29:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6219216086647728, 'median': 1.611328125, 'mins': 1.54296875}
 
(14, 5, 1, 80, 128, 96)
Params #:  25728
MACs:  5042688
 
[23:29:43] (INFO) Building library...
[23:29:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7216996626420455, 'median': 1.7012939453125, 'mins': 1.620849609375}
 
(14, 5, 1, 80, 144, 48)
Params #:  22032
MACs:  4318272
 
[23:30:00] (INFO) Building library...
[23:30:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.346734064275568, 'median': 1.3221435546875, 'mins': 1.2574462890625}
 
(14, 5, 1, 80, 144, 96)
Params #:  28944
MACs:  5673024
 
[23:30:18] (INFO) Building library...
[23:30:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6409656871448863, 'median': 1.620849609375, 'mins': 1.5289306640625}
 
(14, 5, 1, 80, 160, 48)
Params #:  24480
MACs:  4798080
 
[23:30:36] (INFO) Building library...
[23:30:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4928855202414772, 'median': 1.4769287109375, 'mins': 1.397705078125}
 
(14, 5, 1, 80, 160, 96)
Params #:  32160
MACs:  6303360
 
[23:30:53] (INFO) Building library...
[23:30:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7219027432528409, 'median': 1.684326171875, 'mins': 1.6048583984375}
 
(14, 5, 1, 80, 176, 48)
Params #:  26928
MACs:  5277888
 
[23:31:10] (INFO) Building library...
[23:31:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.643561345880682, 'median': 1.617919921875, 'mins': 1.5283203125}
 
(14, 5, 1, 80, 176, 96)
Params #:  35376
MACs:  6933696
 
[23:31:27] (INFO) Building library...
[23:31:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7742453835227272, 'median': 1.7427978515625, 'mins': 1.651123046875}
 
(14, 5, 1, 80, 192, 48)
Params #:  29376
MACs:  5757696
 
[23:31:44] (INFO) Building library...
[23:31:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5261496803977272, 'median': 1.5042724609375, 'mins': 1.4329833984375}
 
(14, 5, 1, 80, 192, 96)
Params #:  38592
MACs:  7564032
 
[23:32:02] (INFO) Building library...
[23:32:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6291648171164772, 'median': 1.6077880859375, 'mins': 1.5234375}
 
(14, 5, 1, 80, 208, 48)
Params #:  31824
MACs:  6237504
 
[23:32:19] (INFO) Building library...
[23:32:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6723444158380683, 'median': 1.65087890625, 'mins': 1.5654296875}
 
(14, 5, 1, 80, 208, 96)
Params #:  41808
MACs:  8194368
 
[23:32:36] (INFO) Building library...
[23:32:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5174394087357954, 'median': 1.4979248046875, 'mins': 1.421875}
 
(14, 5, 1, 80, 224, 48)
Params #:  34272
MACs:  6717312
 
[23:32:53] (INFO) Building library...
[23:32:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6256025834517045, 'median': 1.594970703125, 'mins': 1.51220703125}
 
(14, 5, 1, 80, 224, 96)
Params #:  45024
MACs:  8824704
 
[23:33:10] (INFO) Building library...
[23:33:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6951083096590909, 'median': 1.680419921875, 'mins': 1.61181640625}
 
(14, 5, 1, 80, 240, 48)
Params #:  36720
MACs:  7197120
 
[23:33:27] (INFO) Building library...
[23:33:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6620705344460227, 'median': 1.6346435546875, 'mins': 1.541015625}
 
(14, 5, 1, 80, 240, 96)
Params #:  48240
MACs:  9455040
 
[23:33:44] (INFO) Building library...
[23:33:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7355113636363637, 'median': 1.7109375, 'mins': 1.628662109375}
 
(14, 5, 1, 80, 256, 48)
Params #:  39168
MACs:  7676928
 
[23:34:02] (INFO) Building library...
[23:34:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.65889892578125, 'median': 1.6363525390625, 'mins': 1.56298828125}
 
(14, 5, 1, 80, 256, 96)
Params #:  51456
MACs:  10085376
 
[23:34:19] (INFO) Building library...
[23:34:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6995439009232955, 'median': 1.6688232421875, 'mins': 1.5826416015625}
 
(14, 5, 1, 80, 272, 48)
Params #:  41616
MACs:  8156736
 
[23:34:36] (INFO) Building library...
[23:34:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6643266157670455, 'median': 1.638427734375, 'mins': 1.5509033203125}
 
(14, 5, 1, 80, 272, 96)
Params #:  54672
MACs:  10715712
 
[23:34:54] (INFO) Building library...
[23:34:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6884687943892045, 'median': 1.6649169921875, 'mins': 1.574951171875}
 
(14, 5, 1, 80, 288, 48)
Params #:  44064
MACs:  8636544
 
[23:35:11] (INFO) Building library...
[23:35:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.592452725497159, 'median': 1.4666748046875, 'mins': 1.3905029296875}
 
(14, 5, 1, 80, 288, 96)
Params #:  57888
MACs:  11346048
 
[23:35:29] (INFO) Building library...
[23:35:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7492897727272727, 'median': 1.7200927734375, 'mins': 1.624267578125}
 
(14, 5, 1, 80, 304, 48)
Params #:  46512
MACs:  9116352
 
[23:35:47] (INFO) Building library...
[23:35:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5368441495028409, 'median': 1.4969482421875, 'mins': 1.43212890625}
 
(14, 5, 1, 80, 304, 96)
Params #:  61104
MACs:  11976384
 
[23:36:04] (INFO) Building library...
[23:36:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7889825994318183, 'median': 1.6923828125, 'mins': 1.5989990234375}
 
(14, 5, 1, 80, 320, 48)
Params #:  48960
MACs:  9596160
 
[23:36:22] (INFO) Building library...
[23:36:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5425392844460226, 'median': 1.520263671875, 'mins': 1.4447021484375}
 
(14, 5, 1, 80, 320, 96)
Params #:  64320
MACs:  12606720
 
[23:36:39] (INFO) Building library...
[23:36:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7204390092329545, 'median': 1.6798095703125, 'mins': 1.6092529296875}
 
(14, 5, 1, 80, 336, 48)
Params #:  51408
MACs:  10075968
 
[23:36:56] (INFO) Building library...
[23:36:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7202170632102274, 'median': 1.68505859375, 'mins': 1.60498046875}
 
(14, 5, 1, 80, 336, 96)
Params #:  67536
MACs:  13237056
 
[23:37:14] (INFO) Building library...
[23:37:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7871171431107955, 'median': 1.6669921875, 'mins': 1.5626220703125}
 
(14, 5, 1, 80, 352, 48)
Params #:  53856
MACs:  10555776
 
[23:37:31] (INFO) Building library...
[23:37:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4972523082386364, 'median': 1.471923828125, 'mins': 1.3973388671875}
 
(14, 5, 1, 80, 352, 96)
Params #:  70752
MACs:  13867392
 
[23:37:49] (INFO) Building library...
[23:37:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6808660333806817, 'median': 1.6595458984375, 'mins': 1.587158203125}
 
(14, 5, 1, 80, 368, 48)
Params #:  56304
MACs:  11035584
 
[23:38:06] (INFO) Building library...
[23:38:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6878429066051137, 'median': 1.66650390625, 'mins': 1.588623046875}
 
(14, 5, 1, 80, 368, 96)
Params #:  73968
MACs:  14497728
 
[23:38:23] (INFO) Building library...
[23:38:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6175703568892046, 'median': 1.4915771484375, 'mins': 1.4102783203125}
 
(14, 5, 1, 80, 384, 48)
Params #:  58752
MACs:  11515392
 
[23:38:40] (INFO) Building library...
[23:38:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.69769287109375, 'median': 1.66845703125, 'mins': 1.5819091796875}
 
(14, 5, 1, 80, 384, 96)
Params #:  77184
MACs:  15128064
 
[23:38:58] (INFO) Building library...
[23:38:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6242542613636364, 'median': 1.607666015625, 'mins': 1.536865234375}
 
(14, 5, 1, 80, 400, 48)
Params #:  61200
MACs:  11995200
 
[23:39:15] (INFO) Building library...
[23:39:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7488136985085228, 'median': 1.7099609375, 'mins': 1.629638671875}
 
(14, 5, 1, 80, 400, 96)
Params #:  80400
MACs:  15758400
 
[23:39:33] (INFO) Building library...
[23:39:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7196444424715909, 'median': 1.6802978515625, 'mins': 1.6065673828125}
 
(14, 5, 1, 80, 416, 48)
Params #:  63648
MACs:  12475008
 
[23:39:50] (INFO) Building library...
[23:39:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7251598011363636, 'median': 1.68798828125, 'mins': 1.6109619140625}
 
(14, 5, 1, 80, 416, 96)
Params #:  83616
MACs:  16388736
 
[23:40:08] (INFO) Building library...
[23:40:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6947887073863637, 'median': 1.6624755859375, 'mins': 1.5810546875}
 
(14, 5, 1, 80, 432, 48)
Params #:  66096
MACs:  12954816
 
[23:40:25] (INFO) Building library...
[23:40:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7035833185369318, 'median': 1.6900634765625, 'mins': 1.62060546875}
 
(14, 5, 1, 80, 432, 96)
Params #:  86832
MACs:  17019072
 
[23:40:43] (INFO) Building library...
[23:40:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9343827681107955, 'median': 1.6966552734375, 'mins': 1.590576171875}
 
(14, 5, 1, 80, 448, 48)
Params #:  68544
MACs:  13434624
 
[23:41:00] (INFO) Building library...
[23:41:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.71514892578125, 'median': 1.687255859375, 'mins': 1.5972900390625}
 
(14, 5, 1, 80, 448, 96)
Params #:  90048
MACs:  17649408
 
[23:41:18] (INFO) Building library...
[23:41:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.724853515625, 'median': 1.698486328125, 'mins': 1.60107421875}
 
(14, 5, 1, 80, 464, 48)
Params #:  70992
MACs:  13914432
 
[23:41:35] (INFO) Building library...
[23:41:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7058615944602273, 'median': 1.67626953125, 'mins': 1.607666015625}
 
(14, 5, 1, 80, 464, 96)
Params #:  93264
MACs:  18279744
 
[23:41:52] (INFO) Building library...
[23:41:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6992742365056819, 'median': 1.6771240234375, 'mins': 1.614990234375}
 
(14, 5, 1, 80, 480, 48)
Params #:  73440
MACs:  14394240
 
[23:42:10] (INFO) Building library...
[23:42:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7341930042613636, 'median': 1.704833984375, 'mins': 1.62060546875}
 
(14, 5, 1, 80, 480, 96)
Params #:  96480
MACs:  18910080
 
[23:42:27] (INFO) Building library...
[23:42:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8424793590198865, 'median': 1.816650390625, 'mins': 1.721435546875}
 
(14, 5, 1, 80, 496, 48)
Params #:  75888
MACs:  14874048
 
[23:42:44] (INFO) Building library...
[23:42:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6824529474431817, 'median': 1.6690673828125, 'mins': 1.5963134765625}
 
(14, 5, 1, 80, 496, 96)
Params #:  99696
MACs:  19540416
 
[23:43:02] (INFO) Building library...
[23:43:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7197620738636363, 'median': 1.69921875, 'mins': 1.604736328125}
 
(14, 5, 1, 80, 512, 48)
Params #:  78336
MACs:  15353856
 
[23:43:19] (INFO) Building library...
[23:43:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.759228515625, 'median': 1.72607421875, 'mins': 1.6435546875}
 
(14, 5, 1, 80, 512, 96)
Params #:  102912
MACs:  20170752
 
[23:43:36] (INFO) Building library...
[23:43:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7010997425426135, 'median': 1.689697265625, 'mins': 1.6138916015625}
 
(14, 5, 1, 80, 528, 48)
Params #:  80784
MACs:  15833664
 
[23:43:53] (INFO) Building library...
[23:43:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6844105113636363, 'median': 1.6629638671875, 'mins': 1.589111328125}
 
(14, 5, 1, 80, 528, 96)
Params #:  106128
MACs:  20801088
 
[23:44:11] (INFO) Building library...
[23:44:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8735628995028408, 'median': 1.8502197265625, 'mins': 1.758056640625}
 
(14, 5, 1, 80, 544, 48)
Params #:  83232
MACs:  16313472
 
[23:44:28] (INFO) Building library...
[23:44:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8941572709517045, 'median': 1.7706298828125, 'mins': 1.68359375}
 
(14, 5, 1, 80, 544, 96)
Params #:  109344
MACs:  21431424
 
[23:44:46] (INFO) Building library...
[23:44:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.001161887428977, 'median': 1.8526611328125, 'mins': 1.760498046875}
 
(14, 5, 1, 80, 560, 48)
Params #:  85680
MACs:  16793280
 
[23:45:03] (INFO) Building library...
[23:45:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.711312588778409, 'median': 1.7034912109375, 'mins': 1.6341552734375}
 
(14, 5, 1, 80, 560, 96)
Params #:  112560
MACs:  22061760
 
[23:45:21] (INFO) Building library...
[23:45:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8972645152698864, 'median': 1.7210693359375, 'mins': 1.62939453125}
 
(14, 5, 1, 80, 576, 48)
Params #:  88128
MACs:  17273088
 
[23:45:38] (INFO) Building library...
[23:45:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.704117098721591, 'median': 1.681640625, 'mins': 1.6109619140625}
 
(14, 5, 1, 80, 576, 96)
Params #:  115776
MACs:  22692096
 
[23:45:56] (INFO) Building library...
[23:45:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5212679776278408, 'median': 1.4906005859375, 'mins': 1.4193115234375}
 
(14, 5, 1, 80, 592, 48)
Params #:  90576
MACs:  17752896
 
[23:46:13] (INFO) Building library...
[23:46:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6587324662642045, 'median': 1.6435546875, 'mins': 1.572265625}
 
(14, 5, 1, 80, 592, 96)
Params #:  118992
MACs:  23322432
 
[23:46:31] (INFO) Building library...
[23:46:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8602217240767045, 'median': 1.8363037109375, 'mins': 1.7342529296875}
 
(14, 5, 1, 80, 608, 48)
Params #:  93024
MACs:  18232704
 
[23:46:48] (INFO) Building library...
[23:46:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5258522727272728, 'median': 1.501220703125, 'mins': 1.442626953125}
 
(14, 5, 1, 80, 608, 96)
Params #:  122208
MACs:  23952768
 
[23:47:05] (INFO) Building library...
[23:47:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4927390358664774, 'median': 1.48486328125, 'mins': 1.421875}
 
(14, 5, 1, 80, 624, 48)
Params #:  95472
MACs:  18712512
 
[23:47:23] (INFO) Building library...
[23:47:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9191561612215908, 'median': 1.7607421875, 'mins': 1.675048828125}
 
(14, 5, 1, 80, 624, 96)
Params #:  125424
MACs:  24583104
 
[23:47:40] (INFO) Building library...
[23:47:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6941950017755683, 'median': 1.689208984375, 'mins': 1.6168212890625}
 
(14, 5, 1, 80, 640, 48)
Params #:  97920
MACs:  19192320
 
[23:47:58] (INFO) Building library...
[23:47:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6136973987926135, 'median': 1.5860595703125, 'mins': 1.5130615234375}
 
(14, 5, 1, 80, 640, 96)
Params #:  128640
MACs:  25213440
 
[23:48:15] (INFO) Building library...
[23:48:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.020575506036932, 'median': 1.8837890625, 'mins': 1.791259765625}
 
(14, 5, 1, 80, 656, 48)
Params #:  100368
MACs:  19672128
 
[23:48:33] (INFO) Building library...
[23:48:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9803300337357954, 'median': 1.9410400390625, 'mins': 1.8485107421875}
 
(14, 5, 1, 80, 656, 96)
Params #:  131856
MACs:  25843776
 
[23:48:50] (INFO) Building library...
[23:48:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8492132013494318, 'median': 1.8338623046875, 'mins': 1.755126953125}
 
(14, 5, 1, 80, 672, 48)
Params #:  102816
MACs:  20151936
 
[23:49:08] (INFO) Building library...
[23:49:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7135642311789774, 'median': 1.677490234375, 'mins': 1.603759765625}
 
(14, 5, 1, 80, 672, 96)
Params #:  135072
MACs:  26474112
 
[23:49:25] (INFO) Building library...
[23:49:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.730472079190341, 'median': 1.7021484375, 'mins': 1.6201171875}
 
(14, 5, 1, 80, 688, 48)
Params #:  105264
MACs:  20631744
 
[23:49:42] (INFO) Building library...
[23:49:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7412098277698864, 'median': 1.6947021484375, 'mins': 1.623291015625}
 
(14, 5, 1, 80, 688, 96)
Params #:  138288
MACs:  27104448
 
[23:50:00] (INFO) Building library...
[23:50:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7285344904119317, 'median': 1.722900390625, 'mins': 1.654052734375}
 
(14, 5, 1, 80, 704, 48)
Params #:  107712
MACs:  21111552
 
[23:50:17] (INFO) Building library...
[23:50:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8299072265625, 'median': 1.8033447265625, 'mins': 1.712646484375}
 
(14, 5, 1, 80, 704, 96)
Params #:  141504
MACs:  27734784
 
[23:50:34] (INFO) Building library...
[23:50:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8554787375710227, 'median': 1.7513427734375, 'mins': 1.65234375}
 
(14, 5, 1, 96, 16, 48)
Params #:  2704
MACs:  529984
 
[23:50:52] (INFO) Building library...
[23:50:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4826382723721592, 'median': 1.416015625, 'mins': 1.3031005859375}
 
(14, 5, 1, 96, 16, 96)
Params #:  3472
MACs:  680512
 
[23:51:09] (INFO) Building library...
[23:51:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7431451970880683, 'median': 1.734375, 'mins': 1.6661376953125}
 
(14, 5, 1, 96, 32, 48)
Params #:  5408
MACs:  1059968
 
[23:51:29] (INFO) Building library...
[23:51:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3663496537642046, 'median': 1.3450927734375, 'mins': 1.2659912109375}
 
(14, 5, 1, 96, 32, 96)
Params #:  6944
MACs:  1361024
 
[23:51:46] (INFO) Building library...
[23:51:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0135242808948863, 'median': 1.9674072265625, 'mins': 1.88134765625}
 
(14, 5, 1, 96, 48, 48)
Params #:  8112
MACs:  1589952
 
[23:52:05] (INFO) Building library...
[23:52:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5160999644886364, 'median': 1.4892578125, 'mins': 1.4091796875}
 
(14, 5, 1, 96, 48, 96)
Params #:  10416
MACs:  2041536
 
[23:52:22] (INFO) Building library...
[23:52:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.178281471946023, 'median': 2.14892578125, 'mins': 2.056884765625}
 
(14, 5, 1, 96, 64, 48)
Params #:  10816
MACs:  2119936
 
[23:52:41] (INFO) Building library...
[23:52:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6539106889204545, 'median': 1.635009765625, 'mins': 1.556640625}
 
(14, 5, 1, 96, 64, 96)
Params #:  13888
MACs:  2722048
 
[23:52:58] (INFO) Building library...
[23:52:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3717296253551137, 'median': 2.3377685546875, 'mins': 2.2342529296875}
 
(14, 5, 1, 96, 80, 48)
Params #:  13520
MACs:  2649920
 
[23:53:17] (INFO) Building library...
[23:53:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5172740589488636, 'median': 1.50146484375, 'mins': 1.4342041015625}
 
(14, 5, 1, 96, 80, 96)
Params #:  17360
MACs:  3402560
 
[23:53:34] (INFO) Building library...
[23:53:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1856545188210226, 'median': 2.156005859375, 'mins': 2.0771484375}
 
(14, 5, 1, 96, 112, 48)
Params #:  18928
MACs:  3709888
 
[23:53:53] (INFO) Building library...
[23:53:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6865256569602274, 'median': 1.665283203125, 'mins': 1.575927734375}
 
(14, 5, 1, 96, 112, 96)
Params #:  24304
MACs:  4763584
 
[23:54:10] (INFO) Building library...
[23:54:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4097556374289772, 'median': 2.3760986328125, 'mins': 2.27197265625}
 
(14, 5, 1, 96, 128, 48)
Params #:  21632
MACs:  4239872
 
[23:54:30] (INFO) Building library...
[23:54:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8504438920454545, 'median': 1.7305908203125, 'mins': 1.604248046875}
 
(14, 5, 1, 96, 128, 96)
Params #:  27776
MACs:  5444096
 
[23:54:47] (INFO) Building library...
[23:54:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1875810102982953, 'median': 2.163330078125, 'mins': 2.05078125}
 
(14, 5, 1, 96, 144, 48)
Params #:  24336
MACs:  4769856
 
[23:55:07] (INFO) Building library...
[23:55:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4969060724431817, 'median': 1.475830078125, 'mins': 1.3955078125}
 
(14, 5, 1, 96, 144, 96)
Params #:  31248
MACs:  6124608
 
[23:55:25] (INFO) Building library...
[23:55:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4163629705255683, 'median': 2.3897705078125, 'mins': 2.28759765625}
 
(14, 5, 1, 96, 160, 48)
Params #:  27040
MACs:  5299840
 
[23:55:44] (INFO) Building library...
[23:55:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6735107421875, 'median': 1.6627197265625, 'mins': 1.5992431640625}
 
(14, 5, 1, 96, 160, 96)
Params #:  34720
MACs:  6805120
 
[23:56:01] (INFO) Building library...
[23:56:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.222719504616477, 'median': 2.1922607421875, 'mins': 2.116943359375}
 
(14, 5, 1, 96, 176, 48)
Params #:  29744
MACs:  5829824
 
[23:56:20] (INFO) Building library...
[23:56:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.937493341619318, 'median': 1.7081298828125, 'mins': 1.6156005859375}
 
(14, 5, 1, 96, 176, 96)
Params #:  38192
MACs:  7485632
 
[23:56:38] (INFO) Building library...
[23:56:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1943714488636363, 'median': 2.187255859375, 'mins': 2.1151123046875}
 
(14, 5, 1, 96, 192, 48)
Params #:  32448
MACs:  6359808
 
[23:56:57] (INFO) Building library...
[23:56:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5181662819602273, 'median': 1.4964599609375, 'mins': 1.4202880859375}
 
(14, 5, 1, 96, 192, 96)
Params #:  41664
MACs:  8166144
 
[23:57:15] (INFO) Building library...
[23:57:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.455660733309659, 'median': 2.4151611328125, 'mins': 2.330078125}
 
(14, 5, 1, 96, 208, 48)
Params #:  35152
MACs:  6889792
 
[23:57:34] (INFO) Building library...
[23:57:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6791936700994319, 'median': 1.6546630859375, 'mins': 1.583984375}
 
(14, 5, 1, 96, 208, 96)
Params #:  45136
MACs:  8846656
 
[23:57:51] (INFO) Building library...
[23:57:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4348100142045452, 'median': 2.3863525390625, 'mins': 2.3013916015625}
 
(14, 5, 1, 96, 224, 48)
Params #:  37856
MACs:  7419776
 
[23:58:10] (INFO) Building library...
[23:58:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66884765625, 'median': 1.6456298828125, 'mins': 1.56494140625}
 
(14, 5, 1, 96, 224, 96)
Params #:  48608
MACs:  9527168
 
[23:58:28] (INFO) Building library...
[23:58:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4303733132102274, 'median': 2.3988037109375, 'mins': 2.2928466796875}
 
(14, 5, 1, 96, 240, 48)
Params #:  40560
MACs:  7949760
 
[23:58:47] (INFO) Building library...
[23:58:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7711958451704546, 'median': 1.74658203125, 'mins': 1.5872802734375}
 
(14, 5, 1, 96, 240, 96)
Params #:  52080
MACs:  10207680
 
[23:59:04] (INFO) Building library...
[23:59:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.434343927556818, 'median': 2.397705078125, 'mins': 2.29931640625}
 
(14, 5, 1, 96, 256, 48)
Params #:  43264
MACs:  8479744
 
[23:59:23] (INFO) Building library...
[23:59:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7226140802556817, 'median': 1.695556640625, 'mins': 1.6312255859375}
 
(14, 5, 1, 96, 256, 96)
Params #:  55552
MACs:  10888192
 
[23:59:41] (INFO) Building library...
[23:59:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3982688210227274, 'median': 2.3583984375, 'mins': 2.25830078125}
 
(14, 5, 1, 96, 272, 48)
Params #:  45968
MACs:  9009728
 
[00:00:00] (INFO) Building library...
[00:00:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.702385919744318, 'median': 1.678955078125, 'mins': 1.5986328125}
 
(14, 5, 1, 96, 272, 96)
Params #:  59024
MACs:  11568704
 
[00:00:17] (INFO) Building library...
[00:00:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5465531782670454, 'median': 2.4522705078125, 'mins': 2.3056640625}
 
(14, 5, 1, 96, 288, 48)
Params #:  48672
MACs:  9539712
 
[00:00:36] (INFO) Building library...
[00:00:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4968372691761365, 'median': 1.477294921875, 'mins': 1.3936767578125}
 
(14, 5, 1, 96, 288, 96)
Params #:  62496
MACs:  12249216
 
[00:00:54] (INFO) Building library...
[00:00:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4711869673295452, 'median': 2.4263916015625, 'mins': 2.32177734375}
 
(14, 5, 1, 96, 304, 48)
Params #:  51376
MACs:  10069696
 
[00:01:13] (INFO) Building library...
[00:01:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4822432084517045, 'median': 1.468994140625, 'mins': 1.40771484375}
 
(14, 5, 1, 96, 304, 96)
Params #:  65968
MACs:  12929728
 
[00:01:30] (INFO) Building library...
[00:01:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.652890846946023, 'median': 2.6334228515625, 'mins': 2.4190673828125}
 
(14, 5, 1, 96, 320, 48)
Params #:  54080
MACs:  10599680
 
[00:01:50] (INFO) Building library...
[00:01:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4817205255681818, 'median': 1.460205078125, 'mins': 1.3936767578125}
 
(14, 5, 1, 96, 320, 96)
Params #:  69440
MACs:  13610240
 
[00:02:07] (INFO) Building library...
[00:02:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.716064453125, 'median': 2.666259765625, 'mins': 2.557373046875}
 
(14, 5, 1, 96, 336, 48)
Params #:  56784
MACs:  11129664
 
[00:02:26] (INFO) Building library...
[00:02:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7278076171875, 'median': 1.7047119140625, 'mins': 1.6317138671875}
 
(14, 5, 1, 96, 336, 96)
Params #:  72912
MACs:  14290752
 
[00:02:43] (INFO) Building library...
[00:02:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4143576882102273, 'median': 2.3768310546875, 'mins': 2.2801513671875}
 
(14, 5, 1, 96, 352, 48)
Params #:  59488
MACs:  11659648
 
[00:03:03] (INFO) Building library...
[00:03:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7149269797585227, 'median': 1.6932373046875, 'mins': 1.605712890625}
 
(14, 5, 1, 96, 352, 96)
Params #:  76384
MACs:  14971264
 
[00:03:20] (INFO) Building library...
[00:03:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.166924493963068, 'median': 2.1341552734375, 'mins': 2.034423828125}
 
(14, 5, 1, 96, 368, 48)
Params #:  62192
MACs:  12189632
 
[00:03:39] (INFO) Building library...
[00:03:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6289506392045454, 'median': 1.620849609375, 'mins': 1.5482177734375}
 
(14, 5, 1, 96, 368, 96)
Params #:  79856
MACs:  15651776
 
[00:03:56] (INFO) Building library...
[00:03:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4648271040482954, 'median': 2.421875, 'mins': 2.326904296875}
 
(14, 5, 1, 96, 384, 48)
Params #:  64896
MACs:  12719616
 
[00:04:16] (INFO) Building library...
[00:04:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6784190784801136, 'median': 1.6483154296875, 'mins': 1.55712890625}
 
(14, 5, 1, 96, 384, 96)
Params #:  83328
MACs:  16332288
 
[00:04:33] (INFO) Building library...
[00:04:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1554620916193183, 'median': 2.1407470703125, 'mins': 2.0693359375}
 
(14, 5, 1, 96, 400, 48)
Params #:  67600
MACs:  13249600
 
[00:04:53] (INFO) Building library...
[00:04:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.71929931640625, 'median': 1.6937255859375, 'mins': 1.6158447265625}
 
(14, 5, 1, 96, 400, 96)
Params #:  86800
MACs:  17012800
 
[00:05:10] (INFO) Building library...
[00:05:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.404507723721591, 'median': 2.3765869140625, 'mins': 2.2811279296875}
 
(14, 5, 1, 96, 416, 48)
Params #:  70304
MACs:  13779584
 
[00:05:29] (INFO) Building library...
[00:05:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.88311767578125, 'median': 1.8653564453125, 'mins': 1.7777099609375}
 
(14, 5, 1, 96, 416, 96)
Params #:  90272
MACs:  17693312
 
[00:05:47] (INFO) Building library...
[00:05:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3903098366477273, 'median': 2.353271484375, 'mins': 2.263427734375}
 
(14, 7, 2, 96, 16, 96)
Params #:  3856
MACs:  414736
 
[00:06:06] (INFO) Building library...
[00:06:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3201571377840908, 'median': 1.3094482421875, 'mins': 1.2554931640625}
 
(14, 7, 2, 96, 16, 192)
Params #:  5392
MACs:  490000
 
[00:06:23] (INFO) Building library...
[00:06:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.349114435369318, 'median': 1.3358154296875, 'mins': 1.2547607421875}
 
(14, 7, 2, 96, 32, 96)
Params #:  7712
MACs:  829472
 
[00:06:41] (INFO) Building library...
[00:06:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:06:49] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(56) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  float conv2d_nchw[16];
  __shared__ float pad_temp_shared[336];
  __shared__ float placeholder_shared[384];
  for (int ff_init = 0; ff_init < 2; ++ff_init) {
    conv2d_nchw[ff_init] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 4)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 8)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 12)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 2)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 6)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 10)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 14)] = 0.000000e+00f;
  }
  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {
    __syncthreads();
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {
      pad_temp_shared[((((((int)threadIdx.z) * 84) + (((int)threadIdx.y) * 42)) + (((int)threadIdx.x) * 6)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = placeholder[((((((rc_outer * 2352) + (((int)threadIdx.z) * 588)) + ((((((int)threadIdx.y) * 3) + (((((int)threadIdx.x) * 3) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner >> 1)) / 7)) >> 1) * 196)) + (((int)blockIdx.y) * 28)) + ((((((((int)threadIdx.x) * 3) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner >> 1)) / 7) + ((int)threadIdx.y)) & 1) * 14)) + (((((int)threadIdx.x) * 6) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 14))];
    }
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {
      if (((((((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 48) + ((int)threadIdx.y)) >> 1) + ((int)threadIdx.z)) < 4) {
        if (((((((int)threadIdx.z) * 96) + (((int)threadIdx.y) * 48)) + (((int)threadIdx.x) * 7)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) < 384) {
          if (((((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 48) + ((int)threadIdx.y)) < 2) {
            if (((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) < 48) {
              placeholder_shared[((((((int)threadIdx.z) * 96) + (((int)threadIdx.y) * 48)) + (((int)threadIdx.x) * 7)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1)] = placeholder1[(((((((int)threadIdx.z) * 768) + (((int)threadIdx.y) * 384)) + ((((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 12) * 96)) + (rc_outer * 12)) + (((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) % 12))];
            }
          }
        }
      }
    }
    __syncthreads();
    for (int rc_inner = 0; rc_inner < 12; ++rc_inner) {
      for (int ff = 0; ff < 2; ++ff) {
        conv2d_nchw[ff] = (conv2d_nchw[ff] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner)]));
        conv2d_nchw[(ff + 4)] = (conv2d_nchw[(ff + 4)] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 96)]));
        conv2d_nchw[(ff + 8)] = (conv2d_nchw[(ff + 8)] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 192)]));
        conv2d_nchw[(ff + 12)] = (conv2d_nchw[(ff + 12)] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 288)]));
        conv2d_nchw[(ff + 2)] = (conv2d_nchw[(ff + 2)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[(((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner)]));
        conv2d_nchw[(ff + 6)] = (conv2d_nchw[(ff + 6)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 96)]));
        conv2d_nchw[(ff + 10)] = (conv2d_nchw[(ff + 10)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 192)]));
        conv2d_nchw[(ff + 14)] = (conv2d_nchw[(ff + 14)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 288)]));
      }
    }
  }
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] = (conv2d_nchw[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) * placeholder3[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) + placeholder4[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 1568)] = (conv2d_nchw[(ax1_inner_inner_inner + 4)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 3136)] = (conv2d_nchw[(ax1_inner_inner_inner + 8)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 4704)] = (conv2d_nchw[(ax1_inner_inner_inner + 12)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] = (conv2d_nchw[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) * placeholder3[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) + placeholder4[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 1575)] = (conv2d_nchw[(ax1_inner_inner_inner + 6)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 3143)] = (conv2d_nchw[(ax1_inner_inner_inner + 10)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 4711)] = (conv2d_nchw[(ax1_inner_inner_inner + 14)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[((((((int)threadIdx.z) * 98) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) * placeholder3[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) + placeholder4[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]));
    T_add[(((((((int)threadIdx.z) * 98) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5))]);
}

extern "C" __global__ void __launch_bounds__(32) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:06:49] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 32, 192)
Params #:  10784
MACs:  980000
 
[00:06:52] (INFO) Building library...
[00:06:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:07:00] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5))]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[((((((int)threadIdx.z) * 98) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) * placeholder3[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) + placeholder4[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]));
    T_add[(((((((int)threadIdx.z) * 98) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(32) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(56) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  float conv2d_nchw[16];
  __shared__ float pad_temp_shared[336];
  __shared__ float placeholder_shared[384];
  for (int ff_init = 0; ff_init < 2; ++ff_init) {
    conv2d_nchw[ff_init] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 4)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 8)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 12)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 2)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 6)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 10)] = 0.000000e+00f;
    conv2d_nchw[(ff_init + 14)] = 0.000000e+00f;
  }
  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {
    __syncthreads();
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {
      pad_temp_shared[((((((int)threadIdx.z) * 84) + (((int)threadIdx.y) * 42)) + (((int)threadIdx.x) * 6)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = placeholder[((((((rc_outer * 2352) + (((int)threadIdx.z) * 588)) + ((((((int)threadIdx.y) * 3) + (((((int)threadIdx.x) * 3) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner >> 1)) / 7)) >> 1) * 196)) + (((int)blockIdx.y) * 28)) + ((((((((int)threadIdx.x) * 3) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner >> 1)) / 7) + ((int)threadIdx.y)) & 1) * 14)) + (((((int)threadIdx.x) * 6) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 14))];
    }
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {
      if (((((((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 48) + ((int)threadIdx.y)) >> 1) + ((int)threadIdx.z)) < 4) {
        if (((((((int)threadIdx.z) * 96) + (((int)threadIdx.y) * 48)) + (((int)threadIdx.x) * 7)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) < 384) {
          if (((((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 48) + ((int)threadIdx.y)) < 2) {
            if (((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) < 48) {
              placeholder_shared[((((((int)threadIdx.z) * 96) + (((int)threadIdx.y) * 48)) + (((int)threadIdx.x) * 7)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1)] = placeholder1[(((((((int)threadIdx.z) * 768) + (((int)threadIdx.y) * 384)) + ((((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 12) * 96)) + (rc_outer * 12)) + (((((int)threadIdx.x) * 7) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) % 12))];
            }
          }
        }
      }
    }
    __syncthreads();
    for (int rc_inner = 0; rc_inner < 12; ++rc_inner) {
      for (int ff = 0; ff < 2; ++ff) {
        conv2d_nchw[ff] = (conv2d_nchw[ff] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner)]));
        conv2d_nchw[(ff + 4)] = (conv2d_nchw[(ff + 4)] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 96)]));
        conv2d_nchw[(ff + 8)] = (conv2d_nchw[(ff + 8)] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 192)]));
        conv2d_nchw[(ff + 12)] = (conv2d_nchw[(ff + 12)] + (pad_temp_shared[(((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 288)]));
        conv2d_nchw[(ff + 2)] = (conv2d_nchw[(ff + 2)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[(((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner)]));
        conv2d_nchw[(ff + 6)] = (conv2d_nchw[(ff + 6)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 96)]));
        conv2d_nchw[(ff + 10)] = (conv2d_nchw[(ff + 10)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 192)]));
        conv2d_nchw[(ff + 14)] = (conv2d_nchw[(ff + 14)] + (pad_temp_shared[((((rc_inner * 28) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] * placeholder_shared[((((((int)threadIdx.z) * 24) + (ff * 12)) + rc_inner) + 288)]));
      }
    }
  }
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x))] = (conv2d_nchw[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) * placeholder3[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) + placeholder4[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 1568)] = (conv2d_nchw[(ax1_inner_inner_inner + 4)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 3136)] = (conv2d_nchw[(ax1_inner_inner_inner + 8)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 4704)] = (conv2d_nchw[(ax1_inner_inner_inner + 12)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 7)] = (conv2d_nchw[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) * placeholder3[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]) + placeholder4[((((int)threadIdx.z) * 2) + ax1_inner_inner_inner)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 1575)] = (conv2d_nchw[(ax1_inner_inner_inner + 6)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 8)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 3143)] = (conv2d_nchw[(ax1_inner_inner_inner + 10)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 16)]));
    T_add[((((((((int)threadIdx.z) * 392) + (ax1_inner_inner_inner * 196)) + (((int)blockIdx.y) * 28)) + (((int)threadIdx.y) * 14)) + ((int)threadIdx.x)) + 4711)] = (conv2d_nchw[(ax1_inner_inner_inner + 14)] + (((0.000000e+00f - placeholder2[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) * placeholder3[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]) + placeholder4[(((((int)threadIdx.z) * 2) + ax1_inner_inner_inner) + 24)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:07:00] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 48, 96)
Params #:  11568
MACs:  1244208
 
[00:07:03] (INFO) Building library...
[00:07:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4630482066761363, 'median': 1.455322265625, 'mins': 1.3890380859375}
 
(14, 7, 2, 96, 48, 192)
Params #:  16176
MACs:  1470000
 
[00:07:21] (INFO) Building library...
[00:07:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4841708096590909, 'median': 1.46435546875, 'mins': 1.386474609375}
 
(14, 7, 2, 96, 64, 96)
Params #:  15424
MACs:  1658944
 
[00:07:38] (INFO) Building library...
[00:07:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:07:46] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(64) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 49) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6))]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:07:46] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 64, 192)
Params #:  21568
MACs:  1960000
 
[00:07:49] (INFO) Building library...
[00:07:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:07:57] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6))]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 49) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(64) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:07:57] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 80, 96)
Params #:  19280
MACs:  2073680
 
[00:08:00] (INFO) Building library...
[00:08:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.48118896484375, 'median': 1.4654541015625, 'mins': 1.3751220703125}
 
(14, 7, 2, 96, 80, 192)
Params #:  26960
MACs:  2450000
 
[00:08:17] (INFO) Building library...
[00:08:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4801214044744317, 'median': 1.4637451171875, 'mins': 1.396484375}
 
(14, 7, 2, 96, 112, 96)
Params #:  26992
MACs:  2903152
 
[00:08:34] (INFO) Building library...
[00:08:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5501697887073864, 'median': 1.517822265625, 'mins': 1.4423828125}
 
(14, 7, 2, 96, 112, 192)
Params #:  37744
MACs:  3430000
 
[00:08:51] (INFO) Building library...
[00:08:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6184858842329546, 'median': 1.61083984375, 'mins': 1.5316162109375}
 
(14, 7, 2, 96, 128, 96)
Params #:  30848
MACs:  3317888
 
[00:09:09] (INFO) Building library...
[00:09:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:09:17] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(128) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7))]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 49) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:09:17] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 128, 192)
Params #:  43136
MACs:  3920000
 
[00:09:20] (INFO) Building library...
[00:09:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:09:27] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 49) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(128) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7))]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:09:27] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 144, 96)
Params #:  34704
MACs:  3732624
 
[00:09:31] (INFO) Building library...
[00:09:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7419111772017046, 'median': 1.713134765625, 'mins': 1.6212158203125}
 
(14, 7, 2, 96, 144, 192)
Params #:  48528
MACs:  4410000
 
[00:09:48] (INFO) Building library...
[00:09:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6553011807528408, 'median': 1.6343994140625, 'mins': 1.5504150390625}
 
(14, 7, 2, 96, 160, 96)
Params #:  38560
MACs:  4147360
 
[00:10:05] (INFO) Building library...
[00:10:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:10:13] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(160) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 245) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 5)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 245) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:10:13] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 160, 192)
Params #:  53920
MACs:  4900000
 
[00:10:16] (INFO) Building library...
[00:10:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:10:24] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 5)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 245) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 245) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(160) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:10:24] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 176, 96)
Params #:  42416
MACs:  4562096
 
[00:10:27] (INFO) Building library...
[00:10:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.654595392400568, 'median': 1.6357421875, 'mins': 1.549560546875}
 
(14, 7, 2, 96, 176, 192)
Params #:  59312
MACs:  5390000
 
[00:10:44] (INFO) Building library...
[00:10:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4594404740767046, 'median': 1.45458984375, 'mins': 1.3846435546875}
 
(14, 7, 2, 96, 192, 96)
Params #:  46272
MACs:  4976832
 
[00:11:01] (INFO) Building library...
[00:11:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:11:09] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:11:09] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 192, 192)
Params #:  64704
MACs:  5880000
 
[00:11:12] (INFO) Building library...
[00:11:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:11:20] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:11:20] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 208, 96)
Params #:  50128
MACs:  5391568
 
[00:11:23] (INFO) Building library...
[00:11:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7344560102982955, 'median': 1.703857421875, 'mins': 1.6195068359375}
 
(14, 7, 2, 96, 208, 192)
Params #:  70096
MACs:  6370000
 
[00:11:41] (INFO) Building library...
[00:11:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6649247602982955, 'median': 1.635009765625, 'mins': 1.564453125}
 
(14, 7, 2, 96, 224, 96)
Params #:  53984
MACs:  5806304
 
[00:11:58] (INFO) Building library...
[00:11:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:12:06] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(224) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 343) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 7)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 343) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:12:06] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 224, 192)
Params #:  75488
MACs:  6860000
 
[00:12:09] (INFO) Building library...
[00:12:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:12:17] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 7)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 343) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 343) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(224) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:12:17] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 240, 96)
Params #:  57840
MACs:  6221040
 
[00:12:20] (INFO) Building library...
[00:12:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.479476651278409, 'median': 1.4696044921875, 'mins': 1.4129638671875}
 
(14, 7, 2, 96, 240, 192)
Params #:  80880
MACs:  7350000
 
[00:12:37] (INFO) Building library...
[00:12:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.000017755681818, 'median': 1.9156494140625, 'mins': 1.7559814453125}
 
(14, 7, 2, 96, 256, 96)
Params #:  61696
MACs:  6635776
 
[00:12:55] (INFO) Building library...
[00:12:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:13:03] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8))]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(256) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:13:03] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 256, 192)
Params #:  86272
MACs:  7840000
 
[00:13:06] (INFO) Building library...
[00:13:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:13:14] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8))]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(256) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:13:14] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 272, 96)
Params #:  65552
MACs:  7050512
 
[00:13:17] (INFO) Building library...
[00:13:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5266035600142045, 'median': 1.502197265625, 'mins': 1.4312744140625}
 
(14, 7, 2, 96, 272, 192)
Params #:  91664
MACs:  8330000
 
[00:13:34] (INFO) Building library...
[00:13:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8452858664772727, 'median': 1.8177490234375, 'mins': 1.7320556640625}
 
(14, 7, 2, 96, 288, 96)
Params #:  69408
MACs:  7465248
 
[00:13:52] (INFO) Building library...
[00:13:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:14:00] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 441) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 9)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(288) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 441) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:14:00] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 288, 192)
Params #:  97056
MACs:  8820000
 
[00:14:03] (INFO) Building library...
[00:14:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:14:11] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 9)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 441) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(288) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 441) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:14:11] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 304, 96)
Params #:  73264
MACs:  7879984
 
[00:14:14] (INFO) Building library...
[00:14:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.717217462713068, 'median': 1.6951904296875, 'mins': 1.6002197265625}
 
(14, 7, 2, 96, 304, 192)
Params #:  102448
MACs:  9310000
 
[00:14:31] (INFO) Building library...
[00:14:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66732177734375, 'median': 1.6534423828125, 'mins': 1.5838623046875}
 
(14, 7, 2, 96, 320, 96)
Params #:  77120
MACs:  8294720
 
[00:14:49] (INFO) Building library...
[00:14:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:14:56] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 5)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 245) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 245) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(320) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:14:56] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 320, 192)
Params #:  107840
MACs:  9800000
 
[00:15:00] (INFO) Building library...
[00:15:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:15:07] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 245) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 245) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 5)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(320) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:15:07] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 336, 96)
Params #:  80976
MACs:  8709456
 
[00:15:10] (INFO) Building library...
[00:15:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6842640269886364, 'median': 1.639404296875, 'mins': 1.5521240234375}
 
(14, 7, 2, 96, 336, 192)
Params #:  113232
MACs:  10290000
 
[00:15:28] (INFO) Building library...
[00:15:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.705389959161932, 'median': 1.6822509765625, 'mins': 1.605224609375}
 
(14, 7, 2, 96, 352, 96)
Params #:  84832
MACs:  9124192
 
[00:15:45] (INFO) Building library...
[00:15:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:15:53] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 539) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(352) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 11)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 539) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:15:53] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 352, 192)
Params #:  118624
MACs:  10780000
 
[00:15:56] (INFO) Building library...
[00:15:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:16:04] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 11)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 539) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(352) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 539) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:16:04] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 368, 96)
Params #:  88688
MACs:  9538928
 
[00:16:07] (INFO) Building library...
[00:16:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5155162464488636, 'median': 1.4874267578125, 'mins': 1.411865234375}
 
(14, 7, 2, 96, 368, 192)
Params #:  124016
MACs:  11270000
 
[00:16:24] (INFO) Building library...
[00:16:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6601185191761363, 'median': 1.6512451171875, 'mins': 1.583740234375}
 
(14, 7, 2, 96, 384, 96)
Params #:  92544
MACs:  9953664
 
[00:16:42] (INFO) Building library...
[00:16:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:16:50] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(384) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 147) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:16:50] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 384, 192)
Params #:  129408
MACs:  11760000
 
[00:16:53] (INFO) Building library...
[00:16:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:17:01] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(384) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 147) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:17:01] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 400, 96)
Params #:  96400
MACs:  10368400
 
[00:17:04] (INFO) Building library...
[00:17:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6906050248579545, 'median': 1.6678466796875, 'mins': 1.5908203125}
 
(14, 7, 2, 96, 400, 192)
Params #:  134800
MACs:  12250000
 
[00:17:21] (INFO) Building library...
[00:17:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4819446910511365, 'median': 1.4609375, 'mins': 1.405517578125}
 
(14, 7, 2, 96, 416, 96)
Params #:  100256
MACs:  10783136
 
[00:17:39] (INFO) Building library...
[00:17:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:17:46] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 13)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 637) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 637) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(416) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:17:46] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 416, 192)
Params #:  140192
MACs:  12740000
 
[00:17:49] (INFO) Building library...
[00:17:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:17:57] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 13)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 637) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 637) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(416) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:17:57] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 432, 96)
Params #:  104112
MACs:  11197872
 
[00:18:00] (INFO) Building library...
[00:18:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6920021750710228, 'median': 1.682861328125, 'mins': 1.6025390625}
 
(14, 7, 2, 96, 432, 192)
Params #:  145584
MACs:  13230000
 
[00:18:18] (INFO) Building library...
[00:18:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7040605024857955, 'median': 1.6717529296875, 'mins': 1.5958251953125}
 
(14, 7, 2, 96, 448, 96)
Params #:  107968
MACs:  11612608
 
[00:18:35] (INFO) Building library...
[00:18:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:18:43] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 343) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 7)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(448) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 343) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:18:43] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 448, 192)
Params #:  150976
MACs:  13720000
 
[00:18:46] (INFO) Building library...
[00:18:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:18:54] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 7)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 343) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(448) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 343) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:18:54] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 464, 96)
Params #:  111824
MACs:  12027344
 
[00:18:57] (INFO) Building library...
[00:18:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5827048561789774, 'median': 1.5594482421875, 'mins': 1.48046875}
 
(14, 7, 2, 96, 464, 192)
Params #:  156368
MACs:  14210000
 
[00:19:14] (INFO) Building library...
[00:19:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7031194513494319, 'median': 1.6922607421875, 'mins': 1.61962890625}
 
(14, 7, 2, 96, 480, 96)
Params #:  115680
MACs:  12442080
 
[00:19:32] (INFO) Building library...
[00:19:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:19:40] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 735) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(480) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 735) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 15)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:19:40] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 480, 192)
Params #:  161760
MACs:  14700000
 
[00:19:43] (INFO) Building library...
[00:19:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:19:51] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 735) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 735) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(480) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 15)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:19:51] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 496, 96)
Params #:  119536
MACs:  12856816
 
[00:19:54] (INFO) Building library...
[00:19:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8501276189630682, 'median': 1.813232421875, 'mins': 1.7276611328125}
 
(14, 7, 2, 96, 496, 192)
Params #:  167152
MACs:  15190000
 
[00:20:11] (INFO) Building library...
[00:20:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.733536044034091, 'median': 1.7142333984375, 'mins': 1.623779296875}
 
(14, 7, 2, 96, 512, 96)
Params #:  123392
MACs:  13271552
 
[00:20:28] (INFO) Building library...
[00:20:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:20:36] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9))]);
}

extern "C" __global__ void __launch_bounds__(512) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:20:36] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 512, 192)
Params #:  172544
MACs:  15680000
 
[00:20:39] (INFO) Building library...
[00:20:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:20:47] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 49) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(512) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9))]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:20:47] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 528, 96)
Params #:  127248
MACs:  13686288
 
[00:20:50] (INFO) Building library...
[00:20:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8486472389914772, 'median': 1.8289794921875, 'mins': 1.758544921875}
 
(14, 7, 2, 96, 528, 192)
Params #:  177936
MACs:  16170000
 
[00:21:08] (INFO) Building library...
[00:21:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8948697176846592, 'median': 1.8748779296875, 'mins': 1.7086181640625}
 
(14, 7, 2, 96, 544, 96)
Params #:  131104
MACs:  14101024
 
[00:21:25] (INFO) Building library...
[00:21:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:21:33] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 17)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 833) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(544) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 833) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:21:33] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 544, 192)
Params #:  183328
MACs:  16660000
 
[00:21:36] (INFO) Building library...
[00:21:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:21:44] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(544) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 833) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 17)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 833) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:21:44] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 560, 96)
Params #:  134960
MACs:  14515760
 
[00:21:47] (INFO) Building library...
[00:21:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8520840731534092, 'median': 1.824462890625, 'mins': 1.7489013671875}
 
(14, 7, 2, 96, 560, 192)
Params #:  188720
MACs:  17150000
 
[00:22:05] (INFO) Building library...
[00:22:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9050603693181818, 'median': 1.87841796875, 'mins': 1.7872314453125}
 
(14, 7, 2, 96, 576, 96)
Params #:  138816
MACs:  14930496
 
[00:22:22] (INFO) Building library...
[00:22:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:22:30] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 9)]);
}

extern "C" __global__ void __launch_bounds__(576) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 441) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(56) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  float conv2d_nchw[18];
  __shared__ float pad_temp_shared[168];
  __shared__ float placeholder_shared[216];
  conv2d_nchw[0] = 0.000000e+00f;
  conv2d_nchw[2] = 0.000000e+00f;
  conv2d_nchw[4] = 0.000000e+00f;
  conv2d_nchw[6] = 0.000000e+00f;
  conv2d_nchw[8] = 0.000000e+00f;
  conv2d_nchw[10] = 0.000000e+00f;
  conv2d_nchw[12] = 0.000000e+00f;
  conv2d_nchw[14] = 0.000000e+00f;
  conv2d_nchw[16] = 0.000000e+00f;
  conv2d_nchw[1] = 0.000000e+00f;
  conv2d_nchw[3] = 0.000000e+00f;
  conv2d_nchw[5] = 0.000000e+00f;
  conv2d_nchw[7] = 0.000000e+00f;
  conv2d_nchw[9] = 0.000000e+00f;
  conv2d_nchw[11] = 0.000000e+00f;
  conv2d_nchw[13] = 0.000000e+00f;
  conv2d_nchw[15] = 0.000000e+00f;
  conv2d_nchw[17] = 0.000000e+00f;
  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {
    __syncthreads();
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {
      pad_temp_shared[(((((int)threadIdx.z) * 42) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = placeholder[(((((rc_outer * 1176) + ((((((int)threadIdx.z) * 3) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) / 14)) >> 1) * 196)) + (((int)blockIdx.y) * 28)) + ((((((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) / 14) + ((int)threadIdx.z)) & 1) * 14)) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 14))];
    }
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {
      if (((((((int)threadIdx.x) * 2) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 >> 1)) / 27) + ((int)threadIdx.z)) < 4) {
        if (((((int)threadIdx.x) * 2) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 >> 1)) < 27) {
          placeholder_shared[(((((int)threadIdx.z) * 54) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1)] = placeholder1[(((((((int)blockIdx.z) * 3456) + (((int)threadIdx.z) * 864)) + ((((((int)threadIdx.x) * 2) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 >> 1)) / 3) * 96)) + (rc_outer * 6)) + (((((int)threadIdx.x) * 4) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) % 6))];
        }
      }
    }
    __syncthreads();
    for (int rc_inner = 0; rc_inner < 6; ++rc_inner) {
      conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[((((int)threadIdx.z) * 6) + rc_inner)]));
      conv2d_nchw[2] = (conv2d_nchw[2] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 24)]));
      conv2d_nchw[4] = (conv2d_nchw[4] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 48)]));
      conv2d_nchw[6] = (conv2d_nchw[6] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 72)]));
      conv2d_nchw[8] = (conv2d_nchw[8] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 96)]));
      conv2d_nchw[10] = (conv2d_nchw[10] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 120)]));
      conv2d_nchw[12] = (conv2d_nchw[12] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 144)]));
      conv2d_nchw[14] = (conv2d_nchw[14] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 168)]));
      conv2d_nchw[16] = (conv2d_nchw[16] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 192)]));
      conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[((((int)threadIdx.z) * 6) + rc_inner)]));
      conv2d_nchw[3] = (conv2d_nchw[3] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 24)]));
      conv2d_nchw[5] = (conv2d_nchw[5] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 48)]));
      conv2d_nchw[7] = (conv2d_nchw[7] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 72)]));
      conv2d_nchw[9] = (conv2d_nchw[9] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 96)]));
      conv2d_nchw[11] = (conv2d_nchw[11] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 120)]));
      conv2d_nchw[13] = (conv2d_nchw[13] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 144)]));
      conv2d_nchw[15] = (conv2d_nchw[15] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 168)]));
      conv2d_nchw[17] = (conv2d_nchw[17] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 192)]));
    }
  }
  T_add[((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x))] = (conv2d_nchw[0] + (((0.000000e+00f - placeholder2[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) * placeholder3[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) + placeholder4[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 784)] = (conv2d_nchw[2] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 1568)] = (conv2d_nchw[4] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 2352)] = (conv2d_nchw[6] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3136)] = (conv2d_nchw[8] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3920)] = (conv2d_nchw[10] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 4704)] = (conv2d_nchw[12] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 5488)] = (conv2d_nchw[14] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 6272)] = (conv2d_nchw[16] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 14)] = (conv2d_nchw[1] + (((0.000000e+00f - placeholder2[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) * placeholder3[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) + placeholder4[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 798)] = (conv2d_nchw[3] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 1582)] = (conv2d_nchw[5] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 2366)] = (conv2d_nchw[7] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3150)] = (conv2d_nchw[9] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3934)] = (conv2d_nchw[11] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 4718)] = (conv2d_nchw[13] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 5502)] = (conv2d_nchw[15] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 6286)] = (conv2d_nchw[17] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]));
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:22:30] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 576, 192)
Params #:  194112
MACs:  17640000
 
[00:22:34] (INFO) Building library...
[00:22:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:22:42] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(56) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  float conv2d_nchw[18];
  __shared__ float pad_temp_shared[168];
  __shared__ float placeholder_shared[216];
  conv2d_nchw[0] = 0.000000e+00f;
  conv2d_nchw[2] = 0.000000e+00f;
  conv2d_nchw[4] = 0.000000e+00f;
  conv2d_nchw[6] = 0.000000e+00f;
  conv2d_nchw[8] = 0.000000e+00f;
  conv2d_nchw[10] = 0.000000e+00f;
  conv2d_nchw[12] = 0.000000e+00f;
  conv2d_nchw[14] = 0.000000e+00f;
  conv2d_nchw[16] = 0.000000e+00f;
  conv2d_nchw[1] = 0.000000e+00f;
  conv2d_nchw[3] = 0.000000e+00f;
  conv2d_nchw[5] = 0.000000e+00f;
  conv2d_nchw[7] = 0.000000e+00f;
  conv2d_nchw[9] = 0.000000e+00f;
  conv2d_nchw[11] = 0.000000e+00f;
  conv2d_nchw[13] = 0.000000e+00f;
  conv2d_nchw[15] = 0.000000e+00f;
  conv2d_nchw[17] = 0.000000e+00f;
  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {
    __syncthreads();
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {
      pad_temp_shared[(((((int)threadIdx.z) * 42) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = placeholder[(((((rc_outer * 1176) + ((((((int)threadIdx.z) * 3) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) / 14)) >> 1) * 196)) + (((int)blockIdx.y) * 28)) + ((((((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) / 14) + ((int)threadIdx.z)) & 1) * 14)) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 14))];
    }
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {
      if (((((((int)threadIdx.x) * 2) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 >> 1)) / 27) + ((int)threadIdx.z)) < 4) {
        if (((((int)threadIdx.x) * 2) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 >> 1)) < 27) {
          placeholder_shared[(((((int)threadIdx.z) * 54) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1)] = placeholder1[(((((((int)blockIdx.z) * 3456) + (((int)threadIdx.z) * 864)) + ((((((int)threadIdx.x) * 2) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 >> 1)) / 3) * 96)) + (rc_outer * 6)) + (((((int)threadIdx.x) * 4) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) % 6))];
        }
      }
    }
    __syncthreads();
    for (int rc_inner = 0; rc_inner < 6; ++rc_inner) {
      conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[((((int)threadIdx.z) * 6) + rc_inner)]));
      conv2d_nchw[2] = (conv2d_nchw[2] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 24)]));
      conv2d_nchw[4] = (conv2d_nchw[4] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 48)]));
      conv2d_nchw[6] = (conv2d_nchw[6] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 72)]));
      conv2d_nchw[8] = (conv2d_nchw[8] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 96)]));
      conv2d_nchw[10] = (conv2d_nchw[10] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 120)]));
      conv2d_nchw[12] = (conv2d_nchw[12] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 144)]));
      conv2d_nchw[14] = (conv2d_nchw[14] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 168)]));
      conv2d_nchw[16] = (conv2d_nchw[16] + (pad_temp_shared[((rc_inner * 28) + ((int)threadIdx.x))] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 192)]));
      conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[((((int)threadIdx.z) * 6) + rc_inner)]));
      conv2d_nchw[3] = (conv2d_nchw[3] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 24)]));
      conv2d_nchw[5] = (conv2d_nchw[5] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 48)]));
      conv2d_nchw[7] = (conv2d_nchw[7] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 72)]));
      conv2d_nchw[9] = (conv2d_nchw[9] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 96)]));
      conv2d_nchw[11] = (conv2d_nchw[11] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 120)]));
      conv2d_nchw[13] = (conv2d_nchw[13] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 144)]));
      conv2d_nchw[15] = (conv2d_nchw[15] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 168)]));
      conv2d_nchw[17] = (conv2d_nchw[17] + (pad_temp_shared[(((rc_inner * 28) + ((int)threadIdx.x)) + 14)] * placeholder_shared[(((((int)threadIdx.z) * 6) + rc_inner) + 192)]));
    }
  }
  T_add[((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x))] = (conv2d_nchw[0] + (((0.000000e+00f - placeholder2[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) * placeholder3[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) + placeholder4[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 784)] = (conv2d_nchw[2] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 1568)] = (conv2d_nchw[4] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 2352)] = (conv2d_nchw[6] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3136)] = (conv2d_nchw[8] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3920)] = (conv2d_nchw[10] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 4704)] = (conv2d_nchw[12] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 5488)] = (conv2d_nchw[14] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 6272)] = (conv2d_nchw[16] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 14)] = (conv2d_nchw[1] + (((0.000000e+00f - placeholder2[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) * placeholder3[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]) + placeholder4[((((int)blockIdx.z) * 36) + ((int)threadIdx.z))]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 798)] = (conv2d_nchw[3] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 4)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 1582)] = (conv2d_nchw[5] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 8)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 2366)] = (conv2d_nchw[7] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 12)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3150)] = (conv2d_nchw[9] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 16)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 3934)] = (conv2d_nchw[11] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 20)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 4718)] = (conv2d_nchw[13] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 24)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 5502)] = (conv2d_nchw[15] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 28)]));
  T_add[(((((((int)blockIdx.z) * 7056) + (((int)threadIdx.z) * 196)) + (((int)blockIdx.y) * 28)) + ((int)threadIdx.x)) + 6286)] = (conv2d_nchw[17] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) * placeholder3[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]) + placeholder4[(((((int)blockIdx.z) * 36) + ((int)threadIdx.z)) + 32)]));
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 9)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 441) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(576) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:22:42] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 592, 96)
Params #:  142672
MACs:  15345232
 
[00:22:45] (INFO) Building library...
[00:22:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.873245516690341, 'median': 1.8489990234375, 'mins': 1.7685546875}
 
(14, 7, 2, 96, 592, 192)
Params #:  199504
MACs:  18130000
 
[00:23:02] (INFO) Building library...
[00:23:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7695168235085228, 'median': 1.7186279296875, 'mins': 1.6455078125}
 
(14, 7, 2, 96, 608, 96)
Params #:  146528
MACs:  15759968
 
[00:23:20] (INFO) Building library...
[00:23:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:23:28] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 19)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 931) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 931) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(608) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:23:28] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 608, 192)
Params #:  204896
MACs:  18620000
 
[00:23:31] (INFO) Building library...
[00:23:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:23:39] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 931) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 931) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(608) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 19)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:23:39] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 624, 96)
Params #:  150384
MACs:  16174704
 
[00:23:42] (INFO) Building library...
[00:23:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7186612215909092, 'median': 1.689697265625, 'mins': 1.58837890625}
 
(14, 7, 2, 96, 624, 192)
Params #:  210288
MACs:  19110000
 
[00:23:59] (INFO) Building library...
[00:23:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7996171431107955, 'median': 1.76318359375, 'mins': 1.6761474609375}
 
(14, 7, 2, 96, 640, 96)
Params #:  154240
MACs:  16589440
 
[00:24:17] (INFO) Building library...
[00:24:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:24:25] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(640) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 245) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 245) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 5)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:24:25] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 640, 192)
Params #:  215680
MACs:  19600000
 
[00:24:28] (INFO) Building library...
[00:24:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:24:36] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 5)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 245) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(640) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 245) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:24:36] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 656, 96)
Params #:  158096
MACs:  17004176
 
[00:24:39] (INFO) Building library...
[00:24:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6782603870738637, 'median': 1.6719970703125, 'mins': 1.597900390625}
 
(14, 7, 2, 96, 656, 192)
Params #:  221072
MACs:  20090000
 
[00:24:56] (INFO) Building library...
[00:24:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9512007279829546, 'median': 1.781494140625, 'mins': 1.6871337890625}
 
(14, 7, 2, 96, 672, 96)
Params #:  161952
MACs:  17418912
 
[00:25:14] (INFO) Building library...
[00:25:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:25:22] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 1029) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 1029) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(672) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 21)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:25:22] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 672, 192)
Params #:  226464
MACs:  20580000
 
[00:25:25] (INFO) Building library...
[00:25:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:25:33] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 21)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 1029) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(672) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 1029) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:25:33] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 688, 96)
Params #:  165808
MACs:  17833648
 
[00:25:36] (INFO) Building library...
[00:25:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7404640891335228, 'median': 1.72265625, 'mins': 1.639892578125}
 
(14, 7, 2, 96, 688, 192)
Params #:  231856
MACs:  21070000
 
[00:25:53] (INFO) Building library...
[00:25:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5065629438920454, 'median': 1.47216796875, 'mins': 1.4071044921875}
 
(14, 7, 2, 96, 704, 96)
Params #:  169664
MACs:  18248384
 
[00:26:11] (INFO) Building library...
[00:26:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:26:19] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(704) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 539) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 539) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 11)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:26:19] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 704, 192)
Params #:  237248
MACs:  21560000
 
[00:26:22] (INFO) Building library...
[00:26:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:26:30] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 11)]);
}

extern "C" __global__ void __launch_bounds__(704) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 539) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 539) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:26:30] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 720, 96)
Params #:  173520
MACs:  18663120
 
[00:26:33] (INFO) Building library...
[00:26:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.77899169921875, 'median': 1.7420654296875, 'mins': 1.6690673828125}
 
(14, 7, 2, 96, 720, 192)
Params #:  242640
MACs:  22050000
 
[00:26:51] (INFO) Building library...
[00:26:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.160840953480114, 'median': 2.145263671875, 'mins': 1.795654296875}
 
(14, 7, 2, 96, 736, 96)
Params #:  177376
MACs:  19077856
 
[00:27:09] (INFO) Building library...
[00:27:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:27:17] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 1127) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 23)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 1127) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(736) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:27:17] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 736, 192)
Params #:  248032
MACs:  22540000
 
[00:27:20] (INFO) Building library...
[00:27:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:27:28] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 1127) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(736) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 1127) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 23)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:27:28] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 752, 96)
Params #:  181232
MACs:  19492592
 
[00:27:31] (INFO) Building library...
[00:27:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9662952769886364, 'median': 1.8956298828125, 'mins': 1.6883544921875}
 
(14, 7, 2, 96, 752, 192)
Params #:  253424
MACs:  23030000
 
[00:27:49] (INFO) Building library...
[00:27:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7489990234375, 'median': 1.7279052734375, 'mins': 1.6541748046875}
 
(14, 7, 2, 96, 768, 96)
Params #:  185088
MACs:  19907328
 
[00:28:06] (INFO) Building library...
[00:28:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:28:14] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
}

extern "C" __global__ void __launch_bounds__(768) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 147) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:28:14] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 768, 192)
Params #:  258816
MACs:  23520000
 
[00:28:17] (INFO) Building library...
[00:28:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:28:25] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(768) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 147) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:28:25] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 784, 96)
Params #:  188944
MACs:  20322064
 
[00:28:29] (INFO) Building library...
[00:28:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6407049005681817, 'median': 1.632568359375, 'mins': 1.5621337890625}
 
(14, 7, 2, 96, 784, 192)
Params #:  264208
MACs:  24010000
 
[00:28:46] (INFO) Building library...
[00:28:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7164195667613635, 'median': 1.694091796875, 'mins': 1.609130859375}
 
(14, 7, 2, 96, 800, 96)
Params #:  192800
MACs:  20736800
 
[00:29:04] (INFO) Building library...
[00:29:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:29:11] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 1225) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 1225) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(800) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 25)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:29:11] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 800, 192)
Params #:  269600
MACs:  24500000
 
[00:29:14] (INFO) Building library...
[00:29:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:29:22] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 1225) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 25)]);
}

extern "C" __global__ void __launch_bounds__(800) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 1225) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:29:22] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 816, 96)
Params #:  196656
MACs:  21151536
 
[00:29:25] (INFO) Building library...
[00:29:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7157248757102272, 'median': 1.6925048828125, 'mins': 1.6024169921875}
 
(14, 7, 2, 96, 816, 192)
Params #:  274992
MACs:  24990000
 
[00:29:43] (INFO) Building library...
[00:29:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6941428444602273, 'median': 1.67822265625, 'mins': 1.6026611328125}
 
(14, 7, 2, 96, 832, 96)
Params #:  200512
MACs:  21566272
 
[00:30:00] (INFO) Building library...
[00:30:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:30:08] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(96) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(832) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 637) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 637) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 13)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:30:08] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 832, 192)
Params #:  280384
MACs:  25480000
 
[00:30:11] (INFO) Building library...
[00:30:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
[00:30:19] (ERROR) Traceback (most recent call last):
  8: TVMFuncCall
  7: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16P
  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 189, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/jd/workspace/tvm-v0.9.0/python/tvm/contrib/nvcc.py", line 113, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: 
#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(832) tvmgen_default_fused_add_sqrt_divide_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 3)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_2_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 147) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]));
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_2_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 13)]);
}

extern "C" __global__ void __launch_bounds__(392) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2, float* __restrict__ placeholder3, float* __restrict__ placeholder4) {
  __shared__ float PaddedInput_shared[11552];
  __shared__ float placeholder_shared[1568];
  float PaddedInput_shared_local[196];
  float placeholder_shared_local[196];
  float DepthwiseConv2d[4];
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer) {
    if (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) < 11552) {
      if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 56) + (((int)threadIdx.z) * 7)) + ((int)threadIdx.y)) < 1651) {
        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 2) + (((int)threadIdx.z) >> 2)) < 59) {
          PaddedInput_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (((((57 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361)) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) < 323)) && (3 <= (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19) < 17)) ? placeholder[(((((((int)blockIdx.z) * 6272) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) / 361) * 196)) + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 361) / 19) * 14)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) % 19)) - 45)] : 0.000000e+00f);
        }
      }
    }
  }
  #pragma unroll
  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1) {
    placeholder_shared[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = placeholder1[(((((((int)blockIdx.z) * 1568) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_outer1 * 392)) + (((int)threadIdx.z) * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))];
  }
  __syncthreads();
  #pragma unroll
  for (int ax1 = 0; ax1 < 2; ++ax1) {
    #pragma unroll
    for (int ax2 = 0; ax2 < 7; ++ax2) {
      #pragma unroll
      for (int ax3 = 0; ax3 < 7; ++ax3) {
        PaddedInput_shared_local[(((ax1 * 49) + (ax2 * 7)) + ax3)] = PaddedInput_shared[((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3)];
        PaddedInput_shared_local[((((ax1 * 49) + (ax2 * 7)) + ax3) + 98)] = PaddedInput_shared[(((((((((int)threadIdx.z) * 722) + (ax1 * 361)) + (((int)threadIdx.y) * 38)) + (ax2 * 19)) + (((int)threadIdx.x) * 2)) + ax3) + 5776)];
      }
    }
  }
  #pragma unroll
  for (int ax0 = 0; ax0 < 2; ++ax0) {
    #pragma unroll
    for (int ax21 = 0; ax21 < 7; ++ax21) {
      #pragma unroll
      for (int ax31 = 0; ax31 < 7; ++ax31) {
        placeholder_shared_local[(((ax0 * 49) + (ax21 * 7)) + ax31)] = placeholder_shared[((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31)];
        placeholder_shared_local[((((ax0 * 49) + (ax21 * 7)) + ax31) + 98)] = placeholder_shared[(((((((int)threadIdx.z) * 98) + (ax0 * 49)) + (ax21 * 7)) + ax31) + 784)];
      }
    }
  }
  #pragma unroll
  for (int c = 0; c < 2; ++c) {
    DepthwiseConv2d[c] = 0.000000e+00f;
    DepthwiseConv2d[(c + 2)] = 0.000000e+00f;
    #pragma unroll
    for (int di = 0; di < 7; ++di) {
      #pragma unroll
      for (int dj = 0; dj < 7; ++dj) {
        DepthwiseConv2d[c] = (DepthwiseConv2d[c] + (PaddedInput_shared_local[(((c * 49) + (di * 7)) + dj)] * placeholder_shared_local[(((c * 49) + (di * 7)) + dj)]));
        DepthwiseConv2d[(c + 2)] = (DepthwiseConv2d[(c + 2)] + (PaddedInput_shared_local[((((c * 49) + (di * 7)) + dj) + 98)] * placeholder_shared_local[((((c * 49) + (di * 7)) + dj) + 98)]));
      }
    }
  }
  #pragma unroll
  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 2; ++ax1_inner_inner_inner) {
    T_add[(((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x))] = (DepthwiseConv2d[ax1_inner_inner_inner] + (((0.000000e+00f - placeholder2[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) * placeholder3[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]) + placeholder4[(((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner)]));
    T_add[((((((((int)blockIdx.z) * 1568) + (((int)threadIdx.z) * 98)) + (ax1_inner_inner_inner * 49)) + (((int)threadIdx.y) * 7)) + ((int)threadIdx.x)) + 784)] = (DepthwiseConv2d[(ax1_inner_inner_inner + 2)] + (((0.000000e+00f - placeholder2[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) * placeholder3[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]) + placeholder4[((((((int)blockIdx.z) * 32) + (((int)threadIdx.z) * 2)) + ax1_inner_inner_inner) + 16)]));
  }
}

extern "C" __global__ void __launch_bounds__(192) tvmgen_default_fused_add_sqrt_divide_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  T_multiply[((int)threadIdx.x)] = ((1.000000e+00f / sqrtf((placeholder[((int)threadIdx.x)] + 1.000000e-05f))) * placeholder1[((int)threadIdx.x)]);
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_expand_dims_squeeze_expand_dims_multiply_1_kernel0(float* __restrict__ T_multiply, float* __restrict__ placeholder, float* __restrict__ placeholder1) {
  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 637) {
    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) / 49)]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_kernel0(float* __restrict__ T_add, float* __restrict__ y, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 637) {
    T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (y[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (((0.000000e+00f - placeholder[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) * placeholder1[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]) + placeholder2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 49)]));
  }
}


Compilation error:
ptxas error   : Entry function 'tvmgen_default_fused_nn_conv2d_negative_multiply_add_expand_dims_expand_dims_add_1_kernel0' uses too much shared data (0xcd00 bytes, 0xc000 max)

[00:30:19] (ERROR) Error occurred while loading device module.
 
(14, 7, 2, 96, 848, 96)
Params #:  204368
MACs:  21981008
 
[00:30:22] (INFO) Building library...
[00:30:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7328180486505682, 'median': 1.7066650390625, 'mins': 1.632568359375}
 
(14, 7, 2, 96, 848, 192)
Params #:  285776
MACs:  25970000
 
[00:30:40] (INFO) Building library...
[00:30:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0203147194602273, 'median': 1.9124755859375, 'mins': 1.747314453125}
 
(7, 7, 1, 192, 16, 96)
Params #:  5392
MACs:  264208
 
[00:30:58] (INFO) Building library...
[00:30:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5140613902698863, 'median': 1.487548828125, 'mins': 1.409912109375}
 
(7, 7, 1, 192, 16, 192)
Params #:  6928
MACs:  339472
 
[00:31:15] (INFO) Building library...
[00:31:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1900146484375, 'median': 2.166015625, 'mins': 2.0657958984375}
 
(7, 7, 1, 192, 32, 96)
Params #:  10784
MACs:  528416
 
[00:31:34] (INFO) Building library...
[00:31:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4711936257102274, 'median': 1.451416015625, 'mins': 1.378662109375}
 
(7, 7, 1, 192, 32, 192)
Params #:  13856
MACs:  678944
 
[00:31:52] (INFO) Building library...
[00:31:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.161355868252841, 'median': 2.1396484375, 'mins': 2.0655517578125}
 
(7, 7, 1, 192, 48, 96)
Params #:  16176
MACs:  792624
 
[00:32:11] (INFO) Building library...
[00:32:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5710582386363636, 'median': 1.5596923828125, 'mins': 1.4813232421875}
 
(7, 7, 1, 192, 48, 192)
Params #:  20784
MACs:  1018416
 
[00:32:29] (INFO) Building library...
[00:32:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1197543057528407, 'median': 2.1102294921875, 'mins': 2.031005859375}
 
(7, 7, 1, 192, 64, 96)
Params #:  21568
MACs:  1056832
 
[00:32:47] (INFO) Building library...
[00:32:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.629830655184659, 'median': 1.586181640625, 'mins': 1.5260009765625}
 
(7, 7, 1, 192, 64, 192)
Params #:  27712
MACs:  1357888
 
[00:33:05] (INFO) Building library...
[00:33:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3421775124289774, 'median': 2.318603515625, 'mins': 2.2249755859375}
 
(7, 7, 1, 192, 80, 96)
Params #:  26960
MACs:  1321040
 
[00:33:25] (INFO) Building library...
[00:33:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6191606001420455, 'median': 1.6043701171875, 'mins': 1.5301513671875}
 
(7, 7, 1, 192, 80, 192)
Params #:  34640
MACs:  1697360
 
[00:33:42] (INFO) Building library...
[00:33:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3511929598721593, 'median': 2.341796875, 'mins': 2.2618408203125}
 
(7, 7, 1, 192, 96, 96)
Params #:  32352
MACs:  1585248
 
[00:34:00] (INFO) Building library...
[00:34:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4840487393465909, 'median': 1.4674072265625, 'mins': 1.3896484375}
 
(7, 7, 1, 192, 96, 192)
Params #:  41568
MACs:  2036832
 
[00:34:18] (INFO) Building library...
[00:34:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3389970259232955, 'median': 2.328125, 'mins': 2.2464599609375}
 
(7, 7, 1, 192, 112, 96)
Params #:  37744
MACs:  1849456
 
[00:34:37] (INFO) Building library...
[00:34:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.447146883877841, 'median': 1.437255859375, 'mins': 1.3818359375}
 
(7, 7, 1, 192, 112, 192)
Params #:  48496
MACs:  2376304
 
[00:34:55] (INFO) Building library...
[00:34:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2046519886363636, 'median': 2.1923828125, 'mins': 2.126953125}
 
(7, 7, 1, 192, 128, 96)
Params #:  43136
MACs:  2113664
 
[00:35:14] (INFO) Building library...
[00:35:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4851218483664772, 'median': 1.475341796875, 'mins': 1.4140625}
 
(7, 7, 1, 192, 128, 192)
Params #:  55424
MACs:  2715776
 
[00:35:32] (INFO) Building library...
[00:35:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2040660511363637, 'median': 2.16748046875, 'mins': 2.0677490234375}
 
(7, 7, 1, 192, 144, 96)
Params #:  48528
MACs:  2377872
 
[00:35:51] (INFO) Building library...
[00:35:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.653130548650568, 'median': 1.5250244140625, 'mins': 1.4185791015625}
 
(7, 7, 1, 192, 144, 192)
Params #:  62352
MACs:  3055248
 
[00:36:08] (INFO) Building library...
[00:36:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.173845880681818, 'median': 2.16796875, 'mins': 2.0831298828125}
 
(7, 7, 1, 192, 160, 96)
Params #:  53920
MACs:  2642080
 
[00:36:28] (INFO) Building library...
[00:36:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.571585360440341, 'median': 1.5545654296875, 'mins': 1.488037109375}
 
(7, 7, 1, 192, 160, 192)
Params #:  69280
MACs:  3394720
 
[00:36:45] (INFO) Building library...
[00:36:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2218505859375, 'median': 2.208984375, 'mins': 2.1263427734375}
 
(7, 7, 1, 192, 176, 96)
Params #:  59312
MACs:  2906288
 
[00:37:05] (INFO) Building library...
[00:37:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4472867098721591, 'median': 1.439208984375, 'mins': 1.3807373046875}
 
(7, 7, 1, 192, 176, 192)
Params #:  76208
MACs:  3734192
 
[00:37:22] (INFO) Building library...
[00:37:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1432151100852272, 'median': 2.131591796875, 'mins': 2.0501708984375}
 
(7, 7, 1, 192, 208, 96)
Params #:  70096
MACs:  3434704
 
[00:37:40] (INFO) Building library...
[00:37:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5261152787642045, 'median': 1.519287109375, 'mins': 1.4437255859375}
 
(7, 7, 1, 192, 208, 192)
Params #:  90064
MACs:  4413136
 
[00:37:58] (INFO) Building library...
[00:37:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2489202325994317, 'median': 2.218994140625, 'mins': 2.1214599609375}
 
(7, 7, 1, 192, 224, 96)
Params #:  75488
MACs:  3698912
 
[00:38:17] (INFO) Building library...
[00:38:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.55870361328125, 'median': 1.549560546875, 'mins': 1.4892578125}
 
(7, 7, 1, 192, 224, 192)
Params #:  96992
MACs:  4752608
 
[00:38:35] (INFO) Building library...
[00:38:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2857832475142046, 'median': 2.1810302734375, 'mins': 2.06005859375}
 
(7, 7, 1, 192, 240, 96)
Params #:  80880
MACs:  3963120
 
[00:38:54] (INFO) Building library...
[00:38:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5595259232954546, 'median': 1.538818359375, 'mins': 1.46923828125}
 
(7, 7, 1, 192, 240, 192)
Params #:  103920
MACs:  5092080
 
[00:39:11] (INFO) Building library...
[00:39:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1171886097301136, 'median': 2.10888671875, 'mins': 2.0391845703125}
 
(7, 7, 1, 192, 256, 96)
Params #:  86272
MACs:  4227328
 
[00:39:31] (INFO) Building library...
[00:39:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6433049982244319, 'median': 1.636474609375, 'mins': 1.56982421875}
 
(7, 7, 1, 192, 256, 192)
Params #:  110848
MACs:  5431552
 
[00:39:49] (INFO) Building library...
[00:39:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4044022993607954, 'median': 2.3687744140625, 'mins': 2.2938232421875}
 
(7, 7, 1, 192, 272, 96)
Params #:  91664
MACs:  4491536
 
[00:40:09] (INFO) Building library...
[00:40:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5521928267045455, 'median': 1.53466796875, 'mins': 1.4718017578125}
 
(7, 7, 1, 192, 272, 192)
Params #:  117776
MACs:  5771024
 
[00:40:26] (INFO) Building library...
[00:40:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.230118075284091, 'median': 2.2005615234375, 'mins': 2.113525390625}
 
(7, 7, 1, 192, 288, 96)
Params #:  97056
MACs:  4755744
 
[00:40:45] (INFO) Building library...
[00:40:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4939885919744318, 'median': 1.4827880859375, 'mins': 1.4189453125}
 
(7, 7, 1, 192, 288, 192)
Params #:  124704
MACs:  6110496
 
[00:41:03] (INFO) Building library...
[00:41:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2453724254261362, 'median': 2.1947021484375, 'mins': 2.1240234375}
 
(7, 7, 1, 192, 304, 96)
Params #:  102448
MACs:  5019952
 
[00:41:22] (INFO) Building library...
[00:41:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5293778852982955, 'median': 1.5228271484375, 'mins': 1.4466552734375}
 
(7, 7, 1, 192, 304, 192)
Params #:  131632
MACs:  6449968
 
[00:41:39] (INFO) Building library...
[00:41:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.178017356178977, 'median': 2.16796875, 'mins': 2.092041015625}
 
(7, 7, 1, 192, 320, 96)
Params #:  107840
MACs:  5284160
 
[00:41:59] (INFO) Building library...
[00:41:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4792913263494318, 'median': 1.45654296875, 'mins': 1.3836669921875}
 
(7, 7, 1, 192, 320, 192)
Params #:  138560
MACs:  6789440
 
[00:42:16] (INFO) Building library...
[00:42:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.65120849609375, 'median': 2.586181640625, 'mins': 2.490478515625}
 
(7, 7, 1, 192, 336, 96)
Params #:  113232
MACs:  5548368
 
[00:42:36] (INFO) Building library...
[00:42:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.593621271306818, 'median': 1.570068359375, 'mins': 1.4759521484375}
 
(7, 7, 1, 192, 336, 192)
Params #:  145488
MACs:  7128912
 
[00:42:53] (INFO) Building library...
[00:42:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.214708362926136, 'median': 2.177001953125, 'mins': 2.0986328125}
 
(7, 7, 1, 192, 352, 96)
Params #:  118624
MACs:  5812576
 
[00:43:13] (INFO) Building library...
[00:43:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5162275834517045, 'median': 1.4771728515625, 'mins': 1.399658203125}
 
(7, 7, 1, 192, 352, 192)
Params #:  152416
MACs:  7468384
 
[00:43:30] (INFO) Building library...
[00:43:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1003651012073865, 'median': 2.0916748046875, 'mins': 2.0325927734375}
 
(7, 7, 1, 192, 368, 96)
Params #:  124016
MACs:  6076784
 
[00:43:50] (INFO) Building library...
[00:43:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.68876953125, 'median': 1.659912109375, 'mins': 1.5855712890625}
 
(7, 7, 1, 192, 368, 192)
Params #:  159344
MACs:  7807856
 
[00:44:07] (INFO) Building library...
[00:44:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.364438698508523, 'median': 2.328857421875, 'mins': 2.2421875}
 
(7, 7, 1, 192, 384, 96)
Params #:  129408
MACs:  6340992
 
[00:44:27] (INFO) Building library...
[00:44:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7986172762784092, 'median': 1.797119140625, 'mins': 1.6300048828125}
 
(7, 7, 1, 192, 384, 192)
Params #:  166272
MACs:  8147328
 
[00:44:44] (INFO) Building library...
[00:44:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4062755237926137, 'median': 2.364990234375, 'mins': 2.281005859375}
 
(7, 7, 1, 192, 400, 96)
Params #:  134800
MACs:  6605200
 
[00:45:04] (INFO) Building library...
[00:45:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6456554066051137, 'median': 1.6165771484375, 'mins': 1.5325927734375}
 
(7, 7, 1, 192, 400, 192)
Params #:  173200
MACs:  8486800
 
[00:45:21] (INFO) Building library...
[00:45:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4417402787642044, 'median': 2.4066162109375, 'mins': 2.30517578125}
 
(7, 7, 1, 192, 416, 96)
Params #:  140192
MACs:  6869408
 
[00:45:41] (INFO) Building library...
[00:45:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7632046786221591, 'median': 1.652099609375, 'mins': 1.564208984375}
 
(7, 7, 1, 192, 416, 192)
Params #:  180128
MACs:  8826272
 
[00:45:59] (INFO) Building library...
[00:45:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3681895862926137, 'median': 2.3323974609375, 'mins': 2.233642578125}
 
(7, 7, 1, 192, 432, 96)
Params #:  145584
MACs:  7133616
 
[00:46:18] (INFO) Building library...
[00:46:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6372025923295455, 'median': 1.6337890625, 'mins': 1.5672607421875}
 
(7, 7, 1, 192, 432, 192)
Params #:  187056
MACs:  9165744
 
[00:46:36] (INFO) Building library...
[00:46:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3640713778409093, 'median': 2.34521484375, 'mins': 2.249755859375}
 
(7, 7, 1, 192, 448, 96)
Params #:  150976
MACs:  7397824
 
[00:46:55] (INFO) Building library...
[00:46:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.43638916015625, 'median': 1.4195556640625, 'mins': 1.3499755859375}
 
(7, 7, 1, 192, 448, 192)
Params #:  193984
MACs:  9505216
 
[00:47:13] (INFO) Building library...
[00:47:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.387236993963068, 'median': 2.350341796875, 'mins': 2.2509765625}
 
(7, 7, 1, 192, 464, 96)
Params #:  156368
MACs:  7662032
 
[00:47:32] (INFO) Building library...
[00:47:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9121914950284091, 'median': 1.861572265625, 'mins': 1.7525634765625}
 
(7, 7, 1, 192, 464, 192)
Params #:  200912
MACs:  9844688
 
[00:47:49] (INFO) Building library...
[00:47:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.380514248934659, 'median': 2.341796875, 'mins': 2.2518310546875}
 
(7, 7, 1, 192, 480, 96)
Params #:  161760
MACs:  7926240
 
[00:48:08] (INFO) Building library...
[00:48:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6004594282670455, 'median': 1.5887451171875, 'mins': 1.525390625}
 
(7, 7, 1, 192, 480, 192)
Params #:  207840
MACs:  10184160
 
[00:48:26] (INFO) Building library...
[00:48:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.370328036221591, 'median': 2.3310546875, 'mins': 2.2391357421875}
 
(7, 7, 1, 192, 496, 96)
Params #:  167152
MACs:  8190448
 
[00:48:46] (INFO) Building library...
[00:48:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4635842063210227, 'median': 1.436279296875, 'mins': 1.3724365234375}
 
(7, 7, 1, 192, 496, 192)
Params #:  214768
MACs:  10523632
 
[00:49:03] (INFO) Building library...
[00:49:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3752019708806817, 'median': 2.3563232421875, 'mins': 2.2679443359375}
 
(7, 7, 1, 192, 512, 96)
Params #:  172544
MACs:  8454656
 
[00:49:22] (INFO) Building library...
[00:49:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6651744495738636, 'median': 1.642333984375, 'mins': 1.5616455078125}
 
(7, 7, 1, 192, 512, 192)
Params #:  221696
MACs:  10863104
 
[00:49:40] (INFO) Building library...
[00:49:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.533562677556818, 'median': 2.49853515625, 'mins': 2.4013671875}
 
(7, 7, 1, 192, 528, 96)
Params #:  177936
MACs:  8718864
 
[00:49:59] (INFO) Building library...
[00:49:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.84039306640625, 'median': 1.7935791015625, 'mins': 1.73095703125}
 
(7, 7, 1, 192, 528, 192)
Params #:  228624
MACs:  11202576
 
[00:50:17] (INFO) Building library...
[00:50:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2946877219460227, 'median': 2.28662109375, 'mins': 2.2039794921875}
 
(7, 7, 1, 192, 544, 96)
Params #:  183328
MACs:  8983072
 
[00:50:36] (INFO) Building library...
[00:50:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.797509765625, 'median': 1.7740478515625, 'mins': 1.6881103515625}
 
(7, 7, 1, 192, 544, 192)
Params #:  235552
MACs:  11542048
 
[00:50:54] (INFO) Building library...
[00:50:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1821866122159093, 'median': 2.16064453125, 'mins': 2.0921630859375}
 
(7, 7, 1, 192, 560, 96)
Params #:  188720
MACs:  9247280
 
[00:51:13] (INFO) Building library...
[00:51:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6849265358664773, 'median': 1.6575927734375, 'mins': 1.582763671875}
 
(7, 7, 1, 192, 560, 192)
Params #:  242480
MACs:  11881520
 
[00:51:30] (INFO) Building library...
[00:51:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3820390181107953, 'median': 2.3387451171875, 'mins': 2.25390625}
 
(7, 7, 1, 192, 576, 96)
Params #:  194112
MACs:  9511488
 
[00:51:50] (INFO) Building library...
[00:51:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7287675337357955, 'median': 1.6968994140625, 'mins': 1.622802734375}
 
(7, 7, 1, 192, 576, 192)
Params #:  249408
MACs:  12220992
 
[00:52:07] (INFO) Building library...
[00:52:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.372598544034091, 'median': 2.333251953125, 'mins': 2.2486572265625}
 
(7, 7, 1, 192, 592, 96)
Params #:  199504
MACs:  9775696
 
[00:52:27] (INFO) Building library...
[00:52:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.781375399502841, 'median': 1.749755859375, 'mins': 1.682373046875}
 
(7, 7, 1, 192, 592, 192)
Params #:  256336
MACs:  12560464
 
[00:52:45] (INFO) Building library...
[00:52:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5100241921164774, 'median': 2.506591796875, 'mins': 2.4073486328125}
 
(7, 7, 1, 192, 608, 96)
Params #:  204896
MACs:  10039904
 
[00:53:04] (INFO) Building library...
[00:53:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6789695046164772, 'median': 1.6710205078125, 'mins': 1.59912109375}
 
(7, 7, 1, 192, 608, 192)
Params #:  263264
MACs:  12899936
 
[00:53:22] (INFO) Building library...
[00:53:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5590309836647727, 'median': 2.5111083984375, 'mins': 2.40380859375}
 
(7, 7, 1, 192, 624, 96)
Params #:  210288
MACs:  10304112
 
[00:53:42] (INFO) Building library...
[00:53:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6356223366477274, 'median': 1.6273193359375, 'mins': 1.5662841796875}
 
(7, 7, 1, 192, 624, 192)
Params #:  270192
MACs:  13239408
 
[00:53:59] (INFO) Building library...
[00:53:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.430345569957386, 'median': 2.392333984375, 'mins': 2.2608642578125}
 
(7, 7, 1, 192, 640, 96)
Params #:  215680
MACs:  10568320
 
[00:54:18] (INFO) Building library...
[00:54:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6290638316761363, 'median': 1.619140625, 'mins': 1.5555419921875}
 
(7, 7, 1, 192, 640, 192)
Params #:  277120
MACs:  13578880
 
[00:54:36] (INFO) Building library...
[00:54:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5228593306107956, 'median': 2.4959716796875, 'mins': 2.3876953125}
 
(7, 7, 1, 192, 656, 96)
Params #:  221072
MACs:  10832528
 
[00:54:56] (INFO) Building library...
[00:54:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.668389337713068, 'median': 1.6573486328125, 'mins': 1.5859375}
 
(7, 7, 1, 192, 656, 192)
Params #:  284048
MACs:  13918352
 
[00:55:13] (INFO) Building library...
[00:55:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1593339399857956, 'median': 2.1251220703125, 'mins': 2.0235595703125}
 
(7, 7, 1, 192, 672, 96)
Params #:  226464
MACs:  11096736
 
[00:55:33] (INFO) Building library...
[00:55:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4971624200994318, 'median': 1.4757080078125, 'mins': 1.38037109375}
 
(7, 7, 1, 192, 672, 192)
Params #:  290976
MACs:  14257824
 
[00:55:51] (INFO) Building library...
[00:55:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.364434259588068, 'median': 2.3507080078125, 'mins': 2.2578125}
 
(7, 7, 1, 192, 688, 96)
Params #:  231856
MACs:  11360944
 
[00:56:10] (INFO) Building library...
[00:56:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.500420587713068, 'median': 1.4691162109375, 'mins': 1.384765625}
 
(7, 7, 1, 192, 688, 192)
Params #:  297904
MACs:  14597296
 
[00:56:28] (INFO) Building library...
[00:56:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5196455522017045, 'median': 2.4852294921875, 'mins': 2.390380859375}
 
(7, 7, 1, 192, 704, 96)
Params #:  237248
MACs:  11625152
 
[00:56:47] (INFO) Building library...
[00:56:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.68721923828125, 'median': 1.6644287109375, 'mins': 1.5792236328125}
 
(7, 7, 1, 192, 704, 192)
Params #:  304832
MACs:  14936768
 
[00:57:05] (INFO) Building library...
[00:57:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.361355868252841, 'median': 2.3408203125, 'mins': 2.2376708984375}
 
(7, 7, 1, 192, 720, 96)
Params #:  242640
MACs:  11889360
 
[00:57:25] (INFO) Building library...
[00:57:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6877485795454545, 'median': 1.65380859375, 'mins': 1.5921630859375}
 
(7, 7, 1, 192, 720, 192)
Params #:  311760
MACs:  15276240
 
[00:57:42] (INFO) Building library...
[00:57:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.425729092684659, 'median': 2.3934326171875, 'mins': 2.2755126953125}
 
(7, 7, 1, 192, 736, 96)
Params #:  248032
MACs:  12153568
 
[00:58:02] (INFO) Building library...
[00:58:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6311845259232955, 'median': 1.61279296875, 'mins': 1.5362548828125}
 
(7, 7, 1, 192, 736, 192)
Params #:  318688
MACs:  15615712
 
[00:58:20] (INFO) Building library...
[00:58:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5538041548295456, 'median': 2.5323486328125, 'mins': 2.445556640625}
 
(7, 7, 1, 192, 752, 96)
Params #:  253424
MACs:  12417776
 
[00:58:40] (INFO) Building library...
[00:58:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.854745205965909, 'median': 1.820556640625, 'mins': 1.7423095703125}
 
(7, 7, 1, 192, 752, 192)
Params #:  325616
MACs:  15955184
 
[00:58:57] (INFO) Building library...
[00:58:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5236150568181817, 'median': 2.51318359375, 'mins': 2.4180908203125}
 
(7, 7, 1, 192, 768, 96)
Params #:  258816
MACs:  12681984
 
[00:59:17] (INFO) Building library...
[00:59:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8786598899147726, 'median': 1.85888671875, 'mins': 1.723876953125}
 
(7, 7, 1, 192, 768, 192)
Params #:  332544
MACs:  16294656
 
[00:59:35] (INFO) Building library...
[00:59:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1528153852982954, 'median': 2.1427001953125, 'mins': 2.0679931640625}
 
(7, 7, 1, 192, 784, 96)
Params #:  264208
MACs:  12946192
 
[00:59:55] (INFO) Building library...
[00:59:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4870094992897727, 'median': 1.464599609375, 'mins': 1.3895263671875}
 
(7, 7, 1, 192, 784, 192)
Params #:  339472
MACs:  16634128
 
[01:00:12] (INFO) Building library...
[01:00:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4390025745738635, 'median': 2.4090576171875, 'mins': 2.2943115234375}
 
(7, 7, 1, 192, 800, 96)
Params #:  269600
MACs:  13210400
 
[01:00:31] (INFO) Building library...
[01:00:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.470511141690341, 'median': 1.4495849609375, 'mins': 1.376953125}
 
(7, 7, 1, 192, 800, 192)
Params #:  346400
MACs:  16973600
 
[01:00:49] (INFO) Building library...
[01:00:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4152587890625, 'median': 2.3702392578125, 'mins': 2.2791748046875}
 
(7, 7, 1, 192, 816, 96)
Params #:  274992
MACs:  13474608
 
[01:01:09] (INFO) Building library...
[01:01:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8187721946022728, 'median': 1.787109375, 'mins': 1.7020263671875}
 
(7, 7, 1, 192, 816, 192)
Params #:  353328
MACs:  17313072
 
[01:01:26] (INFO) Building library...
[01:01:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6586681019176135, 'median': 2.582275390625, 'mins': 2.4222412109375}
 
(7, 7, 1, 192, 832, 96)
Params #:  280384
MACs:  13738816
 
[01:01:46] (INFO) Building library...
[01:01:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8149458451704545, 'median': 1.8033447265625, 'mins': 1.723388671875}
 
(7, 7, 1, 192, 832, 192)
Params #:  360256
MACs:  17652544
 
[01:02:04] (INFO) Building library...
[01:02:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1246171431107954, 'median': 2.122802734375, 'mins': 2.0528564453125}
 
(7, 7, 1, 192, 848, 96)
Params #:  285776
MACs:  14003024
 
[01:02:23] (INFO) Building library...
[01:02:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6781316583806818, 'median': 1.645751953125, 'mins': 1.5616455078125}
 
(7, 7, 1, 192, 848, 192)
Params #:  367184
MACs:  17992016
 
[01:02:41] (INFO) Building library...
[01:02:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5526611328125, 'median': 2.520751953125, 'mins': 2.415283203125}
 
(7, 7, 1, 192, 864, 96)
Params #:  291168
MACs:  14267232
 
[01:03:00] (INFO) Building library...
[01:03:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8569546786221591, 'median': 1.8243408203125, 'mins': 1.73876953125}
 
(7, 7, 1, 192, 864, 192)
Params #:  374112
MACs:  18331488
 
[01:03:18] (INFO) Building library...
[01:03:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.146832830255682, 'median': 2.125244140625, 'mins': 2.0438232421875}
 
(7, 7, 1, 192, 880, 96)
Params #:  296560
MACs:  14531440
 
[01:03:38] (INFO) Building library...
[01:03:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7008267489346591, 'median': 1.6668701171875, 'mins': 1.5928955078125}
 
(7, 7, 1, 192, 880, 192)
Params #:  381040
MACs:  18670960
 
[01:03:55] (INFO) Building library...
[01:03:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5600630326704548, 'median': 2.5859375, 'mins': 2.2850341796875}
 
(7, 7, 1, 192, 896, 96)
Params #:  301952
MACs:  14795648
 
[01:04:15] (INFO) Building library...
[01:04:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.44395751953125, 'median': 1.4306640625, 'mins': 1.3734130859375}
 
(7, 7, 1, 192, 896, 192)
Params #:  387968
MACs:  19010432
 
[01:04:33] (INFO) Building library...
[01:04:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3419444691051137, 'median': 2.33349609375, 'mins': 2.2547607421875}
 
(7, 7, 1, 192, 912, 96)
Params #:  307344
MACs:  15059856
 
[01:04:53] (INFO) Building library...
[01:04:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4676591352982955, 'median': 1.44384765625, 'mins': 1.373046875}
 
(7, 7, 1, 192, 912, 192)
Params #:  394896
MACs:  19349904
 
[01:05:10] (INFO) Building library...
[01:05:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3430142489346593, 'median': 2.335693359375, 'mins': 2.2569580078125}
 
(7, 7, 1, 192, 928, 96)
Params #:  312736
MACs:  15324064
 
[01:05:30] (INFO) Building library...
[01:05:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6373801491477273, 'median': 1.6221923828125, 'mins': 1.5584716796875}
 
(7, 7, 1, 192, 928, 192)
Params #:  401824
MACs:  19689376
 
[01:05:48] (INFO) Building library...
[01:05:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.383072176846591, 'median': 2.3519287109375, 'mins': 2.2642822265625}
 
(7, 7, 1, 192, 944, 96)
Params #:  318128
MACs:  15588272
 
[01:06:07] (INFO) Building library...
[01:06:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7961647727272727, 'median': 1.7860107421875, 'mins': 1.70263671875}
 
(7, 7, 1, 192, 944, 192)
Params #:  408752
MACs:  20028848
 
[01:06:25] (INFO) Building library...
[01:06:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.365069025213068, 'median': 2.3284912109375, 'mins': 2.25}
 
(7, 7, 1, 192, 960, 96)
Params #:  323520
MACs:  15852480
 
[01:06:44] (INFO) Building library...
[01:06:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8554021661931819, 'median': 1.82763671875, 'mins': 1.7449951171875}
 
(7, 7, 1, 192, 960, 192)
Params #:  415680
MACs:  20368320
 
[01:07:02] (INFO) Building library...
[01:07:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.119264914772727, 'median': 2.10986328125, 'mins': 2.0440673828125}
 
(7, 7, 1, 192, 976, 96)
Params #:  328912
MACs:  16116688
 
[01:07:22] (INFO) Building library...
[01:07:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.930044833096591, 'median': 1.792724609375, 'mins': 1.6937255859375}
 
(7, 7, 1, 192, 976, 192)
Params #:  422608
MACs:  20707792
 
[01:07:39] (INFO) Building library...
[01:07:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4254771839488636, 'median': 2.390625, 'mins': 2.2808837890625}
 
(7, 7, 1, 192, 992, 96)
Params #:  334304
MACs:  16380896
 
[01:07:59] (INFO) Building library...
[01:07:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6707097833806819, 'median': 1.662109375, 'mins': 1.594482421875}
 
(7, 7, 1, 192, 992, 192)
Params #:  429536
MACs:  21047264
 
[01:08:17] (INFO) Building library...
[01:08:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.778824129971591, 'median': 2.76416015625, 'mins': 2.6727294921875}
 
(7, 7, 1, 192, 1008, 96)
Params #:  339696
MACs:  16645104
 
[01:08:37] (INFO) Building library...
[01:08:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6744850852272728, 'median': 1.650390625, 'mins': 1.56884765625}
 
(7, 7, 1, 192, 1008, 192)
Params #:  436464
MACs:  21386736
 
[01:08:54] (INFO) Building library...
[01:08:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6185047496448863, 'median': 2.598388671875, 'mins': 2.4931640625}
 
(7, 7, 1, 192, 1024, 96)
Params #:  345088
MACs:  16909312
 
[01:09:14] (INFO) Building library...
[01:09:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6388494318181819, 'median': 1.6326904296875, 'mins': 1.5706787109375}
 
(7, 7, 1, 192, 1024, 192)
Params #:  443392
MACs:  21726208
 
[01:09:32] (INFO) Building library...
[01:09:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.462709738991477, 'median': 2.4169921875, 'mins': 2.3155517578125}
 
(7, 7, 1, 192, 1040, 96)
Params #:  350480
MACs:  17173520
 
[01:09:52] (INFO) Building library...
[01:09:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6589066938920454, 'median': 1.6282958984375, 'mins': 1.544189453125}
 
(7, 7, 1, 192, 1040, 192)
Params #:  450320
MACs:  22065680
 
[01:10:09] (INFO) Building library...
[01:10:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.541623757102273, 'median': 2.4942626953125, 'mins': 2.401611328125}
 
(7, 7, 1, 192, 1056, 96)
Params #:  355872
MACs:  17437728
 
[01:10:28] (INFO) Building library...
[01:10:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9522272283380682, 'median': 1.822509765625, 'mins': 1.69921875}
 
(7, 7, 1, 192, 1056, 192)
Params #:  457248
MACs:  22405152
 
[01:10:47] (INFO) Building library...
[01:10:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.376089754971591, 'median': 2.3338623046875, 'mins': 2.241455078125}
 
(7, 7, 1, 192, 1072, 96)
Params #:  361264
MACs:  17701936
 
[01:11:07] (INFO) Building library...
[01:11:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7842484907670455, 'median': 1.767578125, 'mins': 1.6968994140625}
 
(7, 7, 1, 192, 1072, 192)
Params #:  464176
MACs:  22744624
 
[01:11:24] (INFO) Building library...
[01:11:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.691561612215909, 'median': 2.555419921875, 'mins': 2.4615478515625}
 
(7, 7, 1, 192, 1088, 96)
Params #:  366656
MACs:  17966144
 
[01:11:43] (INFO) Building library...
[01:11:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8318403764204545, 'median': 1.8095703125, 'mins': 1.71728515625}
 
(7, 7, 1, 192, 1088, 192)
Params #:  471104
MACs:  23084096
 
[01:12:01] (INFO) Building library...
[01:12:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.441695889559659, 'median': 2.434326171875, 'mins': 2.35888671875}
 
(7, 7, 1, 192, 1104, 96)
Params #:  372048
MACs:  18230352
 
[01:12:21] (INFO) Building library...
[01:12:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8095692027698864, 'median': 1.7672119140625, 'mins': 1.68994140625}
 
(7, 7, 1, 192, 1104, 192)
Params #:  478032
MACs:  23423568
 
[01:12:39] (INFO) Building library...
[01:12:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.849459561434659, 'median': 2.8448486328125, 'mins': 2.6005859375}
 
(7, 7, 1, 192, 1120, 96)
Params #:  377440
MACs:  18494560
 
[01:12:58] (INFO) Building library...
[01:12:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8015314275568182, 'median': 1.79345703125, 'mins': 1.7197265625}
 
(7, 7, 1, 192, 1120, 192)
Params #:  484960
MACs:  23763040
 
[01:13:16] (INFO) Building library...
[01:13:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.351773348721591, 'median': 2.3349609375, 'mins': 2.2564697265625}
 
(7, 7, 1, 192, 1136, 96)
Params #:  382832
MACs:  18758768
 
[01:13:36] (INFO) Building library...
[01:13:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.859683504971591, 'median': 1.82763671875, 'mins': 1.75}
 
(7, 7, 1, 192, 1136, 192)
Params #:  491888
MACs:  24102512
 
[01:13:53] (INFO) Building library...
[01:13:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.488189142400568, 'median': 2.480224609375, 'mins': 2.37890625}
 
(7, 7, 1, 192, 1152, 96)
Params #:  388224
MACs:  19022976
 
[01:14:13] (INFO) Building library...
[01:14:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7638205788352272, 'median': 1.75634765625, 'mins': 1.6905517578125}
 
(7, 7, 1, 192, 1152, 192)
Params #:  498816
MACs:  24441984
 
[01:14:31] (INFO) Building library...
[01:14:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.551918723366477, 'median': 2.5164794921875, 'mins': 2.4267578125}
 
(7, 7, 1, 192, 1168, 96)
Params #:  393616
MACs:  19287184
 
[01:14:51] (INFO) Building library...
[01:14:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.77659912109375, 'median': 1.7528076171875, 'mins': 1.66845703125}
 
(7, 7, 1, 192, 1168, 192)
Params #:  505744
MACs:  24781456
 
[01:15:08] (INFO) Building library...
[01:15:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.577099609375, 'median': 2.5130615234375, 'mins': 2.4110107421875}
 
(7, 7, 1, 192, 1184, 96)
Params #:  399008
MACs:  19551392
 
[01:15:27] (INFO) Building library...
[01:15:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8078191583806817, 'median': 1.7818603515625, 'mins': 1.69091796875}
 
(7, 7, 1, 192, 1184, 192)
Params #:  512672
MACs:  25120928
 
[01:15:45] (INFO) Building library...
[01:15:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.303326970880682, 'median': 2.2979736328125, 'mins': 2.2158203125}
 
(7, 7, 1, 192, 1200, 96)
Params #:  404400
MACs:  19815600
 
[01:16:05] (INFO) Building library...
[01:16:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8136163884943182, 'median': 1.7962646484375, 'mins': 1.70751953125}
 
(7, 7, 1, 192, 1200, 192)
Params #:  519600
MACs:  25460400
 
[01:16:23] (INFO) Building library...
[01:16:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3520041725852274, 'median': 2.3499755859375, 'mins': 2.23974609375}
 
(7, 7, 1, 192, 1216, 96)
Params #:  409792
MACs:  20079808
 
[01:16:42] (INFO) Building library...
[01:16:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7789639559659092, 'median': 1.7724609375, 'mins': 1.7039794921875}
 
(7, 7, 1, 192, 1216, 192)
Params #:  526528
MACs:  25799872
 
[01:17:00] (INFO) Building library...
[01:17:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5233109907670452, 'median': 2.49462890625, 'mins': 2.3875732421875}
 
(7, 7, 1, 192, 1232, 96)
Params #:  415184
MACs:  20344016
 
[01:17:20] (INFO) Building library...
[01:17:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8158369584517045, 'median': 1.79345703125, 'mins': 1.698974609375}
 
(7, 7, 1, 192, 1232, 192)
Params #:  533456
MACs:  26139344
 
[01:17:37] (INFO) Building library...
[01:17:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5129250266335226, 'median': 2.497314453125, 'mins': 2.415283203125}
 
(7, 7, 1, 192, 1248, 96)
Params #:  420576
MACs:  20608224
 
[01:17:57] (INFO) Building library...
[01:17:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8238913796164773, 'median': 1.80078125, 'mins': 1.74267578125}
 
(7, 7, 1, 192, 1248, 192)
Params #:  540384
MACs:  26478816
 
[01:18:15] (INFO) Building library...
[01:18:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3553466796875, 'median': 2.3211669921875, 'mins': 2.236572265625}
 
(7, 7, 1, 192, 1264, 96)
Params #:  425968
MACs:  20872432
 
[01:18:35] (INFO) Building library...
[01:18:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.812079412286932, 'median': 1.7791748046875, 'mins': 1.6954345703125}
 
(7, 7, 1, 192, 1264, 192)
Params #:  547312
MACs:  26818288
 
[01:18:52] (INFO) Building library...
[01:18:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.369540127840909, 'median': 2.3409423828125, 'mins': 2.2523193359375}
 
(7, 7, 1, 192, 1280, 96)
Params #:  431360
MACs:  21136640
 
[01:19:12] (INFO) Building library...
[01:19:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8080743963068182, 'median': 1.788330078125, 'mins': 1.7108154296875}
 
(7, 7, 1, 192, 1280, 192)
Params #:  554240
MACs:  27157760
 
[01:19:30] (INFO) Building library...
[01:19:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5132590553977274, 'median': 2.4813232421875, 'mins': 2.3763427734375}
 
(7, 7, 1, 192, 1296, 96)
Params #:  436752
MACs:  21400848
 
[01:19:50] (INFO) Building library...
[01:19:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7571910511363635, 'median': 1.722412109375, 'mins': 1.6595458984375}
 
(7, 7, 1, 192, 1296, 192)
Params #:  561168
MACs:  27497232
 
[01:20:07] (INFO) Building library...
[01:20:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.562775213068182, 'median': 2.52294921875, 'mins': 2.431884765625}
 
(7, 7, 1, 192, 1312, 96)
Params #:  442144
MACs:  21665056
 
[01:20:27] (INFO) Building library...
[01:20:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.84609375, 'median': 1.822021484375, 'mins': 1.7430419921875}
 
(7, 7, 1, 192, 1312, 192)
Params #:  568096
MACs:  27836704
 
[01:20:45] (INFO) Building library...
[01:20:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.823029119318182, 'median': 2.785888671875, 'mins': 2.650634765625}
 
(7, 7, 1, 192, 1328, 96)
Params #:  447536
MACs:  21929264
 
[01:21:05] (INFO) Building library...
[01:21:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6944779829545455, 'median': 1.6630859375, 'mins': 1.5858154296875}
 
(7, 7, 1, 192, 1328, 192)
Params #:  575024
MACs:  28176176
 
[01:21:23] (INFO) Building library...
[01:21:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5352461381392044, 'median': 2.4814453125, 'mins': 2.3956298828125}
 
(7, 7, 1, 192, 1344, 96)
Params #:  452928
MACs:  22193472
 
[01:21:42] (INFO) Building library...
[01:21:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.169442471590909, 'median': 2.070556640625, 'mins': 1.9190673828125}
 
(7, 7, 1, 192, 1344, 192)
Params #:  581952
MACs:  28515648
 
[01:22:01] (INFO) Building library...
[01:22:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 3.2375499378551136, 'median': 3.087890625, 'mins': 2.65478515625}
 
(7, 7, 1, 192, 1360, 96)
Params #:  458320
MACs:  22457680
 
[01:22:23] (INFO) Building library...
[01:22:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.152176180752841, 'median': 2.0811767578125, 'mins': 1.8907470703125}
 
(7, 7, 1, 192, 1360, 192)
Params #:  588880
MACs:  28855120
 
[01:22:42] (INFO) Building library...
[01:22:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 3.065145596590909, 'median': 2.85791015625, 'mins': 2.638916015625}
 
(7, 7, 1, 192, 1376, 96)
Params #:  463712
MACs:  22721888
 
[01:23:02] (INFO) Building library...
[01:23:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8580888227982955, 'median': 1.825927734375, 'mins': 1.725341796875}
 
(7, 7, 1, 192, 1376, 192)
Params #:  595808
MACs:  29194592
 
[01:23:21] (INFO) Building library...
[01:23:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4132945667613637, 'median': 2.37109375, 'mins': 2.2811279296875}
 
(7, 7, 1, 192, 1392, 96)
Params #:  469104
MACs:  22986096
 
[01:23:41] (INFO) Building library...
[01:23:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7611317027698863, 'median': 1.7568359375, 'mins': 1.684814453125}
 
(7, 7, 1, 192, 1392, 192)
Params #:  602736
MACs:  29534064
 
[01:23:58] (INFO) Building library...
[01:23:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.548459694602273, 'median': 2.5078125, 'mins': 2.4027099609375}
 
(7, 7, 1, 192, 1408, 96)
Params #:  474496
MACs:  23250304
 
[01:24:18] (INFO) Building library...
[01:24:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8271240234375, 'median': 1.8045654296875, 'mins': 1.6964111328125}
 
(7, 7, 1, 192, 1408, 192)
Params #:  609664
MACs:  29873536
 
[01:24:36] (INFO) Building library...
[01:24:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.580489834872159, 'median': 2.532958984375, 'mins': 2.4405517578125}
 
(7, 7, 1, 192, 1424, 96)
Params #:  479888
MACs:  23514512
 
[01:24:56] (INFO) Building library...
[01:24:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7625865589488636, 'median': 1.7542724609375, 'mins': 1.685302734375}
 
(7, 7, 1, 192, 1424, 192)
Params #:  616592
MACs:  30213008
 
[01:25:13] (INFO) Building library...
[01:25:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3756813742897727, 'median': 2.35693359375, 'mins': 2.273193359375}
 
(7, 7, 1, 192, 1440, 96)
Params #:  485280
MACs:  23778720
 
[01:25:33] (INFO) Building library...
[01:25:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8753828568892046, 'median': 1.8519287109375, 'mins': 1.75732421875}
 
(7, 7, 1, 192, 1440, 192)
Params #:  623520
MACs:  30552480
 
[01:25:51] (INFO) Building library...
[01:25:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5352727716619317, 'median': 2.4969482421875, 'mins': 2.401611328125}
 
(7, 7, 1, 192, 1456, 96)
Params #:  490672
MACs:  24042928
 
[01:26:11] (INFO) Building library...
[01:26:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8288330078125, 'median': 1.80078125, 'mins': 1.71484375}
 
(7, 7, 1, 192, 1456, 192)
Params #:  630448
MACs:  30891952
 
[01:26:29] (INFO) Building library...
[01:26:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4625077681107954, 'median': 2.455078125, 'mins': 2.3779296875}
 
(7, 7, 1, 192, 1472, 96)
Params #:  496064
MACs:  24307136
 
[01:26:49] (INFO) Building library...
[01:26:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.820263671875, 'median': 1.7913818359375, 'mins': 1.7044677734375}
 
(7, 7, 1, 192, 1472, 192)
Params #:  637376
MACs:  31231424
 
[01:27:07] (INFO) Building library...
[01:27:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5390514026988638, 'median': 2.5040283203125, 'mins': 2.411865234375}
 
(7, 7, 1, 192, 1488, 96)
Params #:  501456
MACs:  24571344
 
[01:27:27] (INFO) Building library...
[01:27:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6271129261363637, 'median': 1.6185302734375, 'mins': 1.5537109375}
 
(7, 7, 1, 192, 1488, 192)
Params #:  644304
MACs:  31570896
 
[01:27:44] (INFO) Building library...
[01:27:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5333362926136362, 'median': 2.49560546875, 'mins': 2.39990234375}
 
(7, 7, 1, 192, 1504, 96)
Params #:  506848
MACs:  24835552
 
[01:28:04] (INFO) Building library...
[01:28:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7907492897727273, 'median': 1.7684326171875, 'mins': 1.6695556640625}
 
(7, 7, 1, 192, 1504, 192)
Params #:  651232
MACs:  31910368
 
[01:28:22] (INFO) Building library...
[01:28:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.51727294921875, 'median': 2.4840087890625, 'mins': 2.3876953125}
 
(7, 7, 1, 192, 1520, 96)
Params #:  512240
MACs:  25099760
 
[01:28:42] (INFO) Building library...
[01:28:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8059614701704545, 'median': 1.79931640625, 'mins': 1.7283935546875}
 
(7, 7, 1, 192, 1520, 192)
Params #:  658160
MACs:  32249840
 
[01:29:00] (INFO) Building library...
[01:29:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5723466352982953, 'median': 2.538818359375, 'mins': 2.4488525390625}
 
(7, 7, 1, 192, 1536, 96)
Params #:  517632
MACs:  25363968
 
[01:29:19] (INFO) Building library...
[01:29:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8206099076704545, 'median': 1.7899169921875, 'mins': 1.6951904296875}
 
(7, 7, 1, 192, 1536, 192)
Params #:  665088
MACs:  32589312
 
[01:29:37] (INFO) Building library...
[01:29:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.622438742897727, 'median': 2.572998046875, 'mins': 2.476806640625}
 
(7, 7, 1, 192, 1552, 96)
Params #:  523024
MACs:  25628176
 
[01:29:57] (INFO) Building library...
[01:29:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9483132102272727, 'median': 1.9339599609375, 'mins': 1.8636474609375}
 
(7, 7, 1, 192, 1552, 192)
Params #:  672016
MACs:  32928784
 
[01:30:14] (INFO) Building library...
[01:30:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7389537464488636, 'median': 2.640869140625, 'mins': 2.46337890625}
 
(7, 7, 1, 192, 1568, 96)
Params #:  528416
MACs:  25892384
 
[01:30:34] (INFO) Building library...
[01:30:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8608875621448864, 'median': 1.8284912109375, 'mins': 1.7410888671875}
 
(7, 7, 1, 192, 1568, 192)
Params #:  678944
MACs:  33268256
 
[01:30:52] (INFO) Building library...
[01:30:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5432683771306817, 'median': 2.5050048828125, 'mins': 2.40087890625}
 
(7, 7, 1, 192, 1584, 96)
Params #:  533808
MACs:  26156592
 
[01:31:12] (INFO) Building library...
[01:31:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8508311878551136, 'median': 1.837646484375, 'mins': 1.760498046875}
 
(7, 7, 1, 192, 1584, 192)
Params #:  685872
MACs:  33607728
 
[01:31:30] (INFO) Building library...
[01:31:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.37130126953125, 'median': 2.33642578125, 'mins': 2.2366943359375}
 
(7, 7, 1, 192, 1600, 96)
Params #:  539200
MACs:  26420800
 
[01:31:49] (INFO) Building library...
[01:31:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8695068359375, 'median': 1.8428955078125, 'mins': 1.7623291015625}
 
(7, 7, 1, 192, 1600, 192)
Params #:  692800
MACs:  33947200
 
[01:32:07] (INFO) Building library...
[01:32:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.559530362215909, 'median': 2.523193359375, 'mins': 2.4195556640625}
 
(7, 7, 1, 192, 1616, 96)
Params #:  544592
MACs:  26685008
 
[01:32:27] (INFO) Building library...
[01:32:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6095281427556818, 'median': 1.6007080078125, 'mins': 1.5220947265625}
 
(7, 7, 1, 192, 1616, 192)
Params #:  699728
MACs:  34286672
 
[01:32:45] (INFO) Building library...
[01:32:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.542459383877841, 'median': 2.514892578125, 'mins': 2.4107666015625}
 
(7, 7, 1, 192, 1632, 96)
Params #:  549984
MACs:  26949216
 
[01:33:04] (INFO) Building library...
[01:33:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.937618741122159, 'median': 1.900146484375, 'mins': 1.8135986328125}
 
(7, 7, 1, 192, 1632, 192)
Params #:  706656
MACs:  34626144
 
[01:33:22] (INFO) Building library...
[01:33:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.530599698153409, 'median': 2.495361328125, 'mins': 2.3927001953125}
 
(7, 7, 1, 192, 1648, 96)
Params #:  555376
MACs:  27213424
 
[01:33:42] (INFO) Building library...
[01:33:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6287364612926136, 'median': 1.6192626953125, 'mins': 1.553955078125}
 
(7, 7, 1, 192, 1648, 192)
Params #:  713584
MACs:  34965616
 
[01:34:00] (INFO) Building library...
[01:34:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.545880681818182, 'median': 2.5047607421875, 'mins': 2.4197998046875}
 
(7, 7, 1, 192, 1664, 96)
Params #:  560768
MACs:  27477632
 
[01:34:20] (INFO) Building library...
[01:34:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6467185280539773, 'median': 1.6343994140625, 'mins': 1.566650390625}
 
(7, 7, 1, 192, 1664, 192)
Params #:  720512
MACs:  35305088
 
[01:34:38] (INFO) Building library...
[01:34:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5426824396306817, 'median': 2.5311279296875, 'mins': 2.4542236328125}
 
(7, 7, 1, 192, 1680, 96)
Params #:  566160
MACs:  27741840
 
[01:34:58] (INFO) Building library...
[01:34:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6920820756392045, 'median': 1.65234375, 'mins': 1.5635986328125}
 
(7, 7, 1, 192, 1680, 192)
Params #:  727440
MACs:  35644560
 
[01:35:15] (INFO) Building library...
[01:35:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5138294566761363, 'median': 2.4732666015625, 'mins': 2.38037109375}
 
(7, 7, 1, 192, 1696, 96)
Params #:  571552
MACs:  28006048
 
[01:35:35] (INFO) Building library...
[01:35:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6805608575994317, 'median': 1.654541015625, 'mins': 1.57373046875}
 
(7, 7, 1, 192, 1696, 192)
Params #:  734368
MACs:  35984032
 
[01:35:53] (INFO) Building library...
[01:35:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.642044344815341, 'median': 2.5933837890625, 'mins': 2.4892578125}
 
(7, 7, 1, 192, 1712, 96)
Params #:  576944
MACs:  28270256
 
[01:36:13] (INFO) Building library...
[01:36:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6200450550426135, 'median': 1.60986328125, 'mins': 1.530517578125}
 
(7, 7, 1, 192, 1712, 192)
Params #:  741296
MACs:  36323504
 
[01:36:31] (INFO) Building library...
[01:36:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5498646129261364, 'median': 2.5113525390625, 'mins': 2.4129638671875}
 
(7, 7, 1, 192, 16, 160)
Params #:  6416
MACs:  314384
 
[01:36:50] (INFO) Building library...
[01:36:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.577263849431818, 'median': 1.54443359375, 'mins': 1.4801025390625}
 
(7, 7, 1, 192, 16, 320)
Params #:  8976
MACs:  439824
 
[01:37:07] (INFO) Building library...
[01:37:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4544022993607955, 'median': 1.447509765625, 'mins': 1.38330078125}
 
(7, 7, 1, 192, 32, 160)
Params #:  12832
MACs:  628768
 
[01:37:25] (INFO) Building library...
[01:37:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5261130593039773, 'median': 1.5086669921875, 'mins': 1.42578125}
 
(7, 7, 1, 192, 32, 320)
Params #:  17952
MACs:  879648
 
[01:37:42] (INFO) Building library...
[01:37:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4734375, 'median': 1.46533203125, 'mins': 1.4068603515625}
 
(7, 7, 1, 192, 48, 160)
Params #:  19248
MACs:  943152
 
[01:38:00] (INFO) Building library...
[01:38:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4876986416903408, 'median': 1.47216796875, 'mins': 1.40576171875}
 
(7, 7, 1, 192, 48, 320)
Params #:  26928
MACs:  1319472
 
[01:38:17] (INFO) Building library...
[01:38:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5470925071022728, 'median': 1.5091552734375, 'mins': 1.4461669921875}
 
(7, 7, 1, 192, 64, 160)
Params #:  25664
MACs:  1257536
 
[01:38:34] (INFO) Building library...
[01:38:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6299416281960226, 'median': 1.607177734375, 'mins': 1.5281982421875}
 
(7, 7, 1, 192, 64, 320)
Params #:  35904
MACs:  1759296
 
[01:38:51] (INFO) Building library...
[01:38:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6280506480823864, 'median': 1.6031494140625, 'mins': 1.531494140625}
 
(7, 7, 1, 192, 80, 160)
Params #:  32080
MACs:  1571920
 
[01:39:09] (INFO) Building library...
[01:39:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5143221768465909, 'median': 1.483642578125, 'mins': 1.406982421875}
 
(7, 7, 1, 192, 80, 320)
Params #:  44880
MACs:  2199120
 
[01:39:25] (INFO) Building library...
[01:39:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7429276899857955, 'median': 1.638671875, 'mins': 1.55712890625}
 
(7, 7, 1, 192, 96, 160)
Params #:  38496
MACs:  1886304
 
[01:39:43] (INFO) Building library...
[01:39:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4789905894886364, 'median': 1.467529296875, 'mins': 1.379638671875}
 
(7, 7, 1, 192, 96, 320)
Params #:  53856
MACs:  2638944
 
[01:40:00] (INFO) Building library...
[01:40:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6378162730823864, 'median': 1.629638671875, 'mins': 1.5625}
 
(7, 7, 1, 192, 112, 160)
Params #:  44912
MACs:  2200688
 
[01:40:18] (INFO) Building library...
[01:40:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5653675426136364, 'median': 1.5379638671875, 'mins': 1.450439453125}
 
(7, 7, 1, 192, 112, 320)
Params #:  62832
MACs:  3078768
 
[01:40:35] (INFO) Building library...
[01:40:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.733649236505682, 'median': 1.7012939453125, 'mins': 1.5826416015625}
 
(7, 7, 1, 192, 128, 160)
Params #:  51328
MACs:  2515072
 
[01:40:53] (INFO) Building library...
[01:40:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7258733575994318, 'median': 1.6868896484375, 'mins': 1.6029052734375}
 
(7, 7, 1, 192, 128, 320)
Params #:  71808
MACs:  3518592
 
[01:41:10] (INFO) Building library...
[01:41:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4526844371448864, 'median': 1.4461669921875, 'mins': 1.3880615234375}
 
(7, 7, 1, 192, 144, 160)
Params #:  57744
MACs:  2829456
 
[01:41:28] (INFO) Building library...
[01:41:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4970447887073863, 'median': 1.48974609375, 'mins': 1.4266357421875}
 
(7, 7, 1, 192, 144, 320)
Params #:  80784
MACs:  3958416
 
[01:41:45] (INFO) Building library...
[01:41:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5817305131392045, 'median': 1.5576171875, 'mins': 1.470458984375}
 
(7, 7, 1, 192, 160, 160)
Params #:  64160
MACs:  3143840
 
[01:42:02] (INFO) Building library...
[01:42:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5743008700284091, 'median': 1.5472412109375, 'mins': 1.48291015625}
 
(7, 7, 1, 192, 160, 320)
Params #:  89760
MACs:  4398240
 
[01:42:20] (INFO) Building library...
[01:42:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.693545809659091, 'median': 1.51025390625, 'mins': 1.4185791015625}
 
(7, 7, 1, 192, 176, 160)
Params #:  70576
MACs:  3458224
 
[01:42:38] (INFO) Building library...
[01:42:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5433294122869319, 'median': 1.50927734375, 'mins': 1.44287109375}
 
(7, 7, 1, 192, 176, 320)
Params #:  98736
MACs:  4838064
 
[01:42:55] (INFO) Building library...
[01:42:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5438975941051136, 'median': 1.5164794921875, 'mins': 1.44921875}
 
(7, 7, 1, 192, 208, 160)
Params #:  83408
MACs:  4086992
 
[01:43:12] (INFO) Building library...
[01:43:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5287342418323864, 'median': 1.5096435546875, 'mins': 1.4442138671875}
 
(7, 7, 1, 192, 208, 320)
Params #:  116688
MACs:  5717712
 
[01:43:29] (INFO) Building library...
[01:43:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5485373757102272, 'median': 1.506591796875, 'mins': 1.4423828125}
 
(7, 7, 1, 192, 224, 160)
Params #:  89824
MACs:  4401376
 
[01:43:46] (INFO) Building library...
[01:43:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5764626242897728, 'median': 1.5521240234375, 'mins': 1.463134765625}
 
(7, 7, 1, 192, 224, 320)
Params #:  125664
MACs:  6157536
 
[01:44:03] (INFO) Building library...
[01:44:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5346457741477273, 'median': 1.5294189453125, 'mins': 1.4559326171875}
 
(7, 7, 1, 192, 240, 160)
Params #:  96240
MACs:  4715760
 
[01:44:21] (INFO) Building library...
[01:44:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5064774946732955, 'median': 1.483642578125, 'mins': 1.4053955078125}
 
(7, 7, 1, 192, 240, 320)
Params #:  134640
MACs:  6597360
 
[01:44:38] (INFO) Building library...
[01:44:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5563753995028409, 'median': 1.5220947265625, 'mins': 1.46337890625}
 
(7, 7, 1, 192, 256, 160)
Params #:  102656
MACs:  5030144
 
[01:44:55] (INFO) Building library...
[01:44:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5097922585227272, 'median': 1.5003662109375, 'mins': 1.4307861328125}
 
(7, 7, 1, 192, 256, 320)
Params #:  143616
MACs:  7037184
 
[01:45:13] (INFO) Building library...
[01:45:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6822376598011364, 'median': 1.6568603515625, 'mins': 1.5760498046875}
 
(7, 7, 1, 192, 272, 160)
Params #:  109072
MACs:  5344528
 
[01:45:31] (INFO) Building library...
[01:45:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.507022372159091, 'median': 1.485595703125, 'mins': 1.40771484375}
 
(7, 7, 1, 192, 272, 320)
Params #:  152592
MACs:  7477008
 
[01:45:48] (INFO) Building library...
[01:45:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.769729891690341, 'median': 1.6201171875, 'mins': 1.4591064453125}
 
(7, 7, 1, 192, 288, 160)
Params #:  115488
MACs:  5658912
 
[01:46:05] (INFO) Building library...
[01:46:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6060313831676136, 'median': 1.573974609375, 'mins': 1.48974609375}
 
(7, 7, 1, 192, 288, 320)
Params #:  161568
MACs:  7916832
 
[01:46:23] (INFO) Building library...
[01:46:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5383966619318181, 'median': 1.5098876953125, 'mins': 1.41552734375}
 
(7, 7, 1, 192, 304, 160)
Params #:  121904
MACs:  5973296
 
[01:46:40] (INFO) Building library...
[01:46:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5493219549005681, 'median': 1.5284423828125, 'mins': 1.4412841796875}
 
(7, 7, 1, 192, 304, 320)
Params #:  170544
MACs:  8356656
 
[01:46:58] (INFO) Building library...
[01:46:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5794344815340908, 'median': 1.56005859375, 'mins': 1.428955078125}
 
(7, 7, 1, 192, 320, 160)
Params #:  128320
MACs:  6287680
 
[01:47:15] (INFO) Building library...
[01:47:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7741776899857955, 'median': 1.7420654296875, 'mins': 1.644287109375}
 
(7, 7, 1, 192, 320, 320)
Params #:  179520
MACs:  8796480
 
[01:47:33] (INFO) Building library...
[01:47:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.490725985440341, 'median': 1.47607421875, 'mins': 1.4163818359375}
 
(7, 7, 1, 192, 336, 160)
Params #:  134736
MACs:  6602064
 
[01:47:50] (INFO) Building library...
[01:47:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4547929243607955, 'median': 1.4453125, 'mins': 1.3873291015625}
 
(7, 7, 1, 192, 336, 320)
Params #:  188496
MACs:  9236304
 
[01:48:07] (INFO) Building library...
[01:48:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5807284268465909, 'median': 1.5511474609375, 'mins': 1.4691162109375}
 
(7, 7, 1, 192, 352, 160)
Params #:  141152
MACs:  6916448
 
[01:48:25] (INFO) Building library...
[01:48:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6902510209517045, 'median': 1.6844482421875, 'mins': 1.62353515625}
 
(7, 7, 1, 192, 352, 320)
Params #:  197472
MACs:  9676128
 
[01:48:42] (INFO) Building library...
[01:48:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7408746892755682, 'median': 1.7091064453125, 'mins': 1.6092529296875}
 
(7, 7, 1, 192, 368, 160)
Params #:  147568
MACs:  7230832
 
[01:49:00] (INFO) Building library...
[01:49:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6981789328835226, 'median': 1.677734375, 'mins': 1.6123046875}
 
(7, 7, 1, 192, 368, 320)
Params #:  206448
MACs:  10115952
 
[01:49:17] (INFO) Building library...
[01:49:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5066938920454545, 'median': 1.4898681640625, 'mins': 1.4151611328125}
 
(7, 7, 1, 192, 384, 160)
Params #:  153984
MACs:  7545216
 
[01:49:35] (INFO) Building library...
[01:49:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9039994673295455, 'median': 1.8756103515625, 'mins': 1.7935791015625}
 
(7, 7, 1, 192, 384, 320)
Params #:  215424
MACs:  10555776
 
[01:49:52] (INFO) Building library...
[01:49:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.665108975497159, 'median': 1.6337890625, 'mins': 1.55419921875}
 
(7, 7, 1, 192, 400, 160)
Params #:  160400
MACs:  7859600
 
[01:50:10] (INFO) Building library...
[01:50:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7461825284090908, 'median': 1.7188720703125, 'mins': 1.63330078125}
 
(7, 7, 1, 192, 400, 320)
Params #:  224400
MACs:  10995600
 
[01:50:27] (INFO) Building library...
[01:50:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7864457563920455, 'median': 1.7762451171875, 'mins': 1.700439453125}
 
(7, 7, 1, 192, 416, 160)
Params #:  166816
MACs:  8173984
 
[01:50:44] (INFO) Building library...
[01:50:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4457896839488635, 'median': 1.435302734375, 'mins': 1.36669921875}
 
(7, 7, 1, 192, 416, 320)
Params #:  233376
MACs:  11435424
 
[01:51:02] (INFO) Building library...
[01:51:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6789561878551136, 'median': 1.6492919921875, 'mins': 1.5703125}
 
(7, 7, 1, 192, 432, 160)
Params #:  173232
MACs:  8488368
 
[01:51:20] (INFO) Building library...
[01:51:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5477494673295455, 'median': 1.520263671875, 'mins': 1.435302734375}
 
(7, 7, 1, 192, 432, 320)
Params #:  242352
MACs:  11875248
 
[01:51:37] (INFO) Building library...
[01:51:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6482122247869317, 'median': 1.642578125, 'mins': 1.5771484375}
 
(7, 7, 1, 192, 448, 160)
Params #:  179648
MACs:  8802752
 
[01:51:54] (INFO) Building library...
[01:51:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7834661310369317, 'median': 1.744873046875, 'mins': 1.6673583984375}
 
(7, 7, 1, 192, 448, 320)
Params #:  251328
MACs:  12315072
 
[01:52:12] (INFO) Building library...
[01:52:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8860329367897728, 'median': 1.83447265625, 'mins': 1.746826171875}
 
(7, 7, 1, 192, 464, 160)
Params #:  186064
MACs:  9117136
 
[01:52:30] (INFO) Building library...
[01:52:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5330388849431817, 'median': 1.513916015625, 'mins': 1.459228515625}
 
(7, 7, 1, 192, 464, 320)
Params #:  260304
MACs:  12754896
 
[01:52:47] (INFO) Building library...
[01:52:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7131902521306819, 'median': 1.679443359375, 'mins': 1.603759765625}
 
(7, 7, 1, 192, 480, 160)
Params #:  192480
MACs:  9431520
 
[01:53:05] (INFO) Building library...
[01:53:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5046653053977272, 'median': 1.4788818359375, 'mins': 1.391845703125}
 
(7, 7, 1, 192, 480, 320)
Params #:  269280
MACs:  13194720
 
[01:53:23] (INFO) Building library...
[01:53:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.688216885653409, 'median': 1.6572265625, 'mins': 1.5841064453125}
 
(7, 7, 1, 192, 496, 160)
Params #:  198896
MACs:  9745904
 
[01:53:41] (INFO) Building library...
[01:53:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7421253551136364, 'median': 1.708984375, 'mins': 1.63330078125}
 
(7, 7, 1, 192, 496, 320)
Params #:  278256
MACs:  13634544
 
[01:53:58] (INFO) Building library...
[01:53:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.786209383877841, 'median': 1.774658203125, 'mins': 1.70361328125}
 
(7, 7, 1, 192, 512, 160)
Params #:  205312
MACs:  10060288
 
[01:54:16] (INFO) Building library...
[01:54:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.53319091796875, 'median': 1.5283203125, 'mins': 1.46728515625}
 
(7, 7, 1, 192, 512, 320)
Params #:  287232
MACs:  14074368
 
[01:54:34] (INFO) Building library...
[01:54:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7502308238636364, 'median': 1.707763671875, 'mins': 1.609130859375}
 
(7, 7, 1, 192, 528, 160)
Params #:  211728
MACs:  10374672
 
[01:54:52] (INFO) Building library...
[01:54:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7419056285511363, 'median': 1.7205810546875, 'mins': 1.6444091796875}
 
(7, 7, 1, 192, 528, 320)
Params #:  296208
MACs:  14514192
 
[01:55:09] (INFO) Building library...
[01:55:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7888039328835228, 'median': 1.7789306640625, 'mins': 1.7088623046875}
 
(7, 7, 1, 192, 544, 160)
Params #:  218144
MACs:  10689056
 
[01:55:26] (INFO) Building library...
[01:55:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6153275923295454, 'median': 1.5938720703125, 'mins': 1.5096435546875}
 
(7, 7, 1, 192, 544, 320)
Params #:  305184
MACs:  14954016
 
[01:55:44] (INFO) Building library...
[01:55:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7054421164772726, 'median': 1.6873779296875, 'mins': 1.6041259765625}
 
(7, 7, 1, 192, 560, 160)
Params #:  224560
MACs:  11003440
 
[01:56:02] (INFO) Building library...
[01:56:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.78218994140625, 'median': 1.7178955078125, 'mins': 1.6297607421875}
 
(7, 7, 1, 192, 560, 320)
Params #:  314160
MACs:  15393840
 
[01:56:19] (INFO) Building library...
[01:56:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6331409801136363, 'median': 1.6199951171875, 'mins': 1.5679931640625}
 
(7, 7, 1, 192, 576, 160)
Params #:  230976
MACs:  11317824
 
[01:56:37] (INFO) Building library...
[01:56:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5288274591619317, 'median': 1.488525390625, 'mins': 1.3031005859375}
 
(7, 7, 1, 192, 576, 320)
Params #:  323136
MACs:  15833664
 
[01:56:55] (INFO) Building library...
[01:56:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.796438876065341, 'median': 1.780029296875, 'mins': 1.71923828125}
 
(7, 7, 1, 192, 592, 160)
Params #:  237392
MACs:  11632208
 
[01:57:13] (INFO) Building library...
[01:57:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5483076615767046, 'median': 1.5296630859375, 'mins': 1.4486083984375}
 
(7, 7, 1, 192, 592, 320)
Params #:  332112
MACs:  16273488
 
[01:57:30] (INFO) Building library...
[01:57:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8853249289772727, 'median': 1.7900390625, 'mins': 1.704345703125}
 
(7, 7, 1, 192, 608, 160)
Params #:  243808
MACs:  11946592
 
[01:57:47] (INFO) Building library...
[01:57:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7554609818892046, 'median': 1.7255859375, 'mins': 1.65625}
 
(7, 7, 1, 192, 608, 320)
Params #:  341088
MACs:  16713312
 
[01:58:05] (INFO) Building library...
[01:58:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.512283602627841, 'median': 1.4910888671875, 'mins': 1.416015625}
 
(7, 7, 1, 192, 624, 160)
Params #:  250224
MACs:  12260976
 
[01:58:23] (INFO) Building library...
[01:58:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6531327681107955, 'median': 1.6353759765625, 'mins': 1.5643310546875}
 
(7, 7, 1, 192, 624, 320)
Params #:  350064
MACs:  17153136
 
[01:58:40] (INFO) Building library...
[01:58:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8727283824573864, 'median': 1.8304443359375, 'mins': 1.7410888671875}
 
(7, 7, 1, 192, 640, 160)
Params #:  256640
MACs:  12575360
 
[01:58:58] (INFO) Building library...
[01:58:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6446910511363637, 'median': 1.6365966796875, 'mins': 1.5604248046875}
 
(7, 7, 1, 192, 640, 320)
Params #:  359040
MACs:  17592960
 
[01:59:16] (INFO) Building library...
[01:59:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7687799627130683, 'median': 1.757080078125, 'mins': 1.6834716796875}
 
(7, 7, 1, 192, 656, 160)
Params #:  263056
MACs:  12889744
 
[01:59:34] (INFO) Building library...
[01:59:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7234419389204545, 'median': 1.7025146484375, 'mins': 1.610595703125}
 
(7, 7, 1, 192, 656, 320)
Params #:  368016
MACs:  18032784
 
[01:59:51] (INFO) Building library...
[01:59:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8225408380681818, 'median': 1.8074951171875, 'mins': 1.74267578125}
 
(7, 7, 1, 192, 672, 160)
Params #:  269472
MACs:  13204128
 
[02:00:09] (INFO) Building library...
[02:00:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8910877574573863, 'median': 1.8734130859375, 'mins': 1.80517578125}
 
(7, 7, 1, 192, 672, 320)
Params #:  376992
MACs:  18472608
 
[02:00:27] (INFO) Building library...
[02:00:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6510142933238636, 'median': 1.6402587890625, 'mins': 1.5733642578125}
 
(7, 7, 1, 192, 688, 160)
Params #:  275888
MACs:  13518512
 
[02:00:45] (INFO) Building library...
[02:00:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7842795632102273, 'median': 1.7515869140625, 'mins': 1.6473388671875}
 
(7, 7, 1, 192, 688, 320)
Params #:  385968
MACs:  18912432
 
[02:01:02] (INFO) Building library...
[02:01:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7969304865056819, 'median': 1.7884521484375, 'mins': 1.7225341796875}
 
(7, 7, 1, 192, 704, 160)
Params #:  282304
MACs:  13832896
 
[02:01:20] (INFO) Building library...
[02:01:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6640569513494319, 'median': 1.64990234375, 'mins': 1.5831298828125}
 
(7, 7, 1, 192, 704, 320)
Params #:  394944
MACs:  19352256
 
[02:01:38] (INFO) Building library...
[02:01:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.75733642578125, 'median': 1.75390625, 'mins': 1.6849365234375}
 
(7, 7, 1, 192, 720, 160)
Params #:  288720
MACs:  14147280
 
[02:01:56] (INFO) Building library...
[02:01:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6840431906960227, 'median': 1.6529541015625, 'mins': 1.570068359375}
 
(7, 7, 1, 192, 720, 320)
Params #:  403920
MACs:  19792080
 
[02:02:13] (INFO) Building library...
[02:02:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6412109375, 'median': 1.6162109375, 'mins': 1.5494384765625}
 
(7, 7, 1, 192, 736, 160)
Params #:  295136
MACs:  14461664
 
[02:02:30] (INFO) Building library...
[02:02:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6865400834517046, 'median': 1.6588134765625, 'mins': 1.5655517578125}
 
(7, 7, 1, 192, 736, 320)
Params #:  412896
MACs:  20231904
 
[02:02:48] (INFO) Building library...
[02:02:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6289528586647728, 'median': 1.615966796875, 'mins': 1.5487060546875}
 
(7, 7, 1, 192, 752, 160)
Params #:  301552
MACs:  14776048
 
[02:03:06] (INFO) Building library...
[02:03:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9092351740056819, 'median': 1.734619140625, 'mins': 1.62890625}
 
(7, 7, 1, 192, 752, 320)
Params #:  421872
MACs:  20671728
 
[02:03:23] (INFO) Building library...
[02:03:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.78057861328125, 'median': 1.775634765625, 'mins': 1.6962890625}
 
(7, 7, 1, 192, 768, 160)
Params #:  307968
MACs:  15090432
 
[02:03:41] (INFO) Building library...
[02:03:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7328125, 'median': 1.7171630859375, 'mins': 1.6414794921875}
 
(7, 7, 1, 192, 768, 320)
Params #:  430848
MACs:  21111552
 
[02:03:59] (INFO) Building library...
[02:03:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6780650745738637, 'median': 1.656494140625, 'mins': 1.5819091796875}
 
(7, 7, 1, 192, 784, 160)
Params #:  314384
MACs:  15404816
 
[02:04:16] (INFO) Building library...
[02:04:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.462472256747159, 'median': 1.4466552734375, 'mins': 1.37548828125}
 
(7, 7, 1, 192, 784, 320)
Params #:  439824
MACs:  21551376
 
[02:04:34] (INFO) Building library...
[02:04:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.691659268465909, 'median': 1.6614990234375, 'mins': 1.5767822265625}
 
(7, 7, 1, 192, 800, 160)
Params #:  320800
MACs:  15719200
 
[02:04:51] (INFO) Building library...
[02:04:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7675370649857955, 'median': 1.7593994140625, 'mins': 1.6927490234375}
 
(7, 7, 1, 192, 800, 320)
Params #:  448800
MACs:  21991200
 
[02:05:09] (INFO) Building library...
[02:05:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7655506480823864, 'median': 1.759033203125, 'mins': 1.6990966796875}
 
(7, 7, 1, 192, 816, 160)
Params #:  327216
MACs:  16033584
 
[02:05:27] (INFO) Building library...
[02:05:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8457619406960226, 'median': 1.818115234375, 'mins': 1.73388671875}
 
(7, 7, 1, 192, 816, 320)
Params #:  457776
MACs:  22431024
 
[02:05:44] (INFO) Building library...
[02:05:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7885486949573863, 'median': 1.77587890625, 'mins': 1.7083740234375}
 
(7, 7, 1, 192, 832, 160)
Params #:  333632
MACs:  16347968
 
[02:06:02] (INFO) Building library...
[02:06:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8220381303267046, 'median': 1.789306640625, 'mins': 1.7122802734375}
 
(7, 7, 1, 192, 832, 320)
Params #:  466752
MACs:  22870848
 
[02:06:20] (INFO) Building library...
[02:06:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8506780450994318, 'median': 1.8106689453125, 'mins': 1.7410888671875}
 
(7, 7, 1, 192, 848, 160)
Params #:  340048
MACs:  16662352
 
[02:06:38] (INFO) Building library...
[02:06:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.86458740234375, 'median': 1.83447265625, 'mins': 1.738525390625}
 
(7, 7, 1, 192, 848, 320)
Params #:  475728
MACs:  23310672
 
[02:06:55] (INFO) Building library...
[02:06:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6830755060369318, 'median': 1.653564453125, 'mins': 1.5643310546875}
 
(7, 7, 1, 192, 864, 160)
Params #:  346464
MACs:  16976736
 
[02:07:13] (INFO) Building library...
[02:07:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9632646040482955, 'median': 1.81005859375, 'mins': 1.7127685546875}
 
(7, 7, 1, 192, 864, 320)
Params #:  484704
MACs:  23750496
 
[02:07:31] (INFO) Building library...
[02:07:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6494406960227272, 'median': 1.614501953125, 'mins': 1.5489501953125}
 
(7, 7, 1, 192, 880, 160)
Params #:  352880
MACs:  17291120
 
[02:07:49] (INFO) Building library...
[02:07:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7350841175426137, 'median': 1.7073974609375, 'mins': 1.6119384765625}
 
(7, 7, 1, 192, 880, 320)
Params #:  493680
MACs:  24190320
 
[02:08:06] (INFO) Building library...
[02:08:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6764548561789774, 'median': 1.6661376953125, 'mins': 1.5970458984375}
 
(7, 7, 1, 192, 896, 160)
Params #:  359296
MACs:  17605504
 
[02:08:24] (INFO) Building library...
[02:08:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5126054243607954, 'median': 1.4769287109375, 'mins': 1.4151611328125}
 
(7, 7, 1, 192, 896, 320)
Params #:  502656
MACs:  24630144
 
[02:08:42] (INFO) Building library...
[02:08:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.802490234375, 'median': 1.7720947265625, 'mins': 1.692138671875}
 
(7, 7, 1, 192, 912, 160)
Params #:  365712
MACs:  17919888
 
[02:09:00] (INFO) Building library...
[02:09:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7680952592329546, 'median': 1.76025390625, 'mins': 1.69287109375}
 
(7, 7, 1, 192, 912, 320)
Params #:  511632
MACs:  25069968
 
[02:09:17] (INFO) Building library...
[02:09:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.73885498046875, 'median': 1.70751953125, 'mins': 1.63720703125}
 
(7, 7, 1, 192, 928, 160)
Params #:  372128
MACs:  18234272
 
[02:09:35] (INFO) Building library...
[02:09:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4967451615767045, 'median': 1.475830078125, 'mins': 1.4036865234375}
 
(7, 7, 1, 192, 928, 320)
Params #:  520608
MACs:  25509792
 
[02:09:53] (INFO) Building library...
[02:09:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8381269975142045, 'median': 1.8109130859375, 'mins': 1.7322998046875}
 
(7, 7, 1, 192, 944, 160)
Params #:  378544
MACs:  18548656
 
[02:10:11] (INFO) Building library...
[02:10:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6130293412642045, 'median': 1.60986328125, 'mins': 1.534423828125}
 
(7, 7, 1, 192, 944, 320)
Params #:  529584
MACs:  25949616
 
[02:10:28] (INFO) Building library...
[02:10:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8651322798295455, 'median': 1.8372802734375, 'mins': 1.762939453125}
 
(7, 7, 1, 192, 960, 160)
Params #:  384960
MACs:  18863040
 
[02:10:46] (INFO) Building library...
[02:10:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.35325927734375, 'median': 1.3299560546875, 'mins': 1.2672119140625}
 
(7, 7, 1, 192, 960, 320)
Params #:  538560
MACs:  26389440
 
[02:11:04] (INFO) Building library...
[02:11:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.493643465909091, 'median': 1.4730224609375, 'mins': 1.40185546875}
 
(7, 7, 1, 192, 976, 160)
Params #:  391376
MACs:  19177424
 
[02:11:22] (INFO) Building library...
[02:11:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7129039417613636, 'median': 1.692626953125, 'mins': 1.6016845703125}
 
(7, 7, 1, 192, 976, 320)
Params #:  547536
MACs:  26829264
 
[02:11:39] (INFO) Building library...
[02:11:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7499700372869318, 'median': 1.71484375, 'mins': 1.61962890625}
 
(7, 7, 1, 192, 992, 160)
Params #:  397792
MACs:  19491808
 
[02:11:57] (INFO) Building library...
[02:11:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7300237482244318, 'median': 1.7020263671875, 'mins': 1.6107177734375}
 
(7, 7, 1, 192, 992, 320)
Params #:  556512
MACs:  27269088
 
[02:12:15] (INFO) Building library...
[02:12:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6887428977272727, 'median': 1.6605224609375, 'mins': 1.585693359375}
 
(7, 7, 1, 192, 1008, 160)
Params #:  404208
MACs:  19806192
 
[02:12:33] (INFO) Building library...
[02:12:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66533203125, 'median': 1.654296875, 'mins': 1.596923828125}
 
(7, 7, 1, 192, 1008, 320)
Params #:  565488
MACs:  27708912
 
[02:12:50] (INFO) Building library...
[02:12:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6764959161931818, 'median': 1.642822265625, 'mins': 1.5677490234375}
 
(7, 7, 1, 192, 1024, 160)
Params #:  410624
MACs:  20120576
 
[02:13:08] (INFO) Building library...
[02:13:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8026123046875, 'median': 1.784423828125, 'mins': 1.704833984375}
 
(7, 7, 1, 192, 1024, 320)
Params #:  574464
MACs:  28148736
 
[02:13:26] (INFO) Building library...
[02:13:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.718563565340909, 'median': 1.6912841796875, 'mins': 1.60986328125}
 
(7, 7, 1, 192, 1040, 160)
Params #:  417040
MACs:  20434960
 
[02:13:44] (INFO) Building library...
[02:13:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7345703125, 'median': 1.7047119140625, 'mins': 1.6142578125}
 
(7, 7, 1, 192, 1040, 320)
Params #:  583440
MACs:  28588560
 
[02:14:01] (INFO) Building library...
[02:14:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6833307439630683, 'median': 1.649658203125, 'mins': 1.5887451171875}
 
(7, 7, 1, 192, 1056, 160)
Params #:  423456
MACs:  20749344
 
[02:14:19] (INFO) Building library...
[02:14:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8362837357954545, 'median': 1.81689453125, 'mins': 1.71435546875}
 
(7, 7, 1, 192, 1056, 320)
Params #:  592416
MACs:  29028384
 
[02:14:37] (INFO) Building library...
[02:14:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6957674893465908, 'median': 1.66357421875, 'mins': 1.5849609375}
 
(7, 7, 1, 192, 1072, 160)
Params #:  429872
MACs:  21063728
 
[02:14:55] (INFO) Building library...
[02:14:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7009843306107955, 'median': 1.6712646484375, 'mins': 1.5968017578125}
 
(7, 7, 1, 192, 1072, 320)
Params #:  601392
MACs:  29468208
 
[02:15:12] (INFO) Building library...
[02:15:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6414561878551137, 'median': 1.614501953125, 'mins': 1.546875}
 
(7, 7, 1, 192, 1088, 160)
Params #:  436288
MACs:  21378112
 
[02:15:30] (INFO) Building library...
[02:15:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7751265092329545, 'median': 1.707275390625, 'mins': 1.5938720703125}
 
(7, 7, 1, 192, 1088, 320)
Params #:  610368
MACs:  29908032
 
[02:15:48] (INFO) Building library...
[02:15:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7851973100142045, 'median': 1.7586669921875, 'mins': 1.6790771484375}
 
(7, 7, 1, 192, 1104, 160)
Params #:  442704
MACs:  21692496
 
[02:16:06] (INFO) Building library...
[02:16:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7861838600852273, 'median': 1.7789306640625, 'mins': 1.7115478515625}
 
(7, 7, 1, 192, 1104, 320)
Params #:  619344
MACs:  30347856
 
[02:16:23] (INFO) Building library...
[02:16:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.665703790838068, 'median': 1.6441650390625, 'mins': 1.5582275390625}
 
(7, 7, 1, 192, 1120, 160)
Params #:  449120
MACs:  22006880
 
[02:16:41] (INFO) Building library...
[02:16:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8541947798295455, 'median': 1.6812744140625, 'mins': 1.583984375}
 
(7, 7, 1, 192, 1120, 320)
Params #:  628320
MACs:  30787680
 
[02:16:59] (INFO) Building library...
[02:16:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8277643377130681, 'median': 1.8016357421875, 'mins': 1.721435546875}
 
(7, 7, 1, 192, 1136, 160)
Params #:  455536
MACs:  22321264
 
[02:17:17] (INFO) Building library...
[02:17:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8341785777698865, 'median': 1.8372802734375, 'mins': 1.6632080078125}
 
(7, 7, 1, 192, 1136, 320)
Params #:  637296
MACs:  31227504
 
[02:17:34] (INFO) Building library...
[02:17:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8698930220170455, 'median': 1.836669921875, 'mins': 1.7518310546875}
 
(7, 7, 1, 192, 1152, 160)
Params #:  461952
MACs:  22635648
 
[02:17:52] (INFO) Building library...
[02:17:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6817515980113635, 'median': 1.6484375, 'mins': 1.560546875}
 
(7, 7, 1, 192, 1152, 320)
Params #:  646272
MACs:  31667328
 
[02:18:10] (INFO) Building library...
[02:18:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.01900634765625, 'median': 1.98388671875, 'mins': 1.8681640625}
 
(7, 7, 1, 192, 1168, 160)
Params #:  468368
MACs:  22950032
 
[02:18:28] (INFO) Building library...
[02:18:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6954123757102273, 'median': 1.6292724609375, 'mins': 1.5426025390625}
 
(7, 7, 1, 192, 1168, 320)
Params #:  655248
MACs:  32107152
 
[02:18:45] (INFO) Building library...
[02:18:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.778683194247159, 'median': 1.7021484375, 'mins': 1.5938720703125}
 
(7, 7, 1, 192, 1184, 160)
Params #:  474784
MACs:  23264416
 
[02:19:03] (INFO) Building library...
[02:19:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8358875621448865, 'median': 1.8076171875, 'mins': 1.6326904296875}
 
(7, 7, 1, 192, 1184, 320)
Params #:  664224
MACs:  32546976
 
[02:19:21] (INFO) Building library...
[02:19:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.816641512784091, 'median': 1.7872314453125, 'mins': 1.701904296875}
 
(7, 7, 1, 192, 1200, 160)
Params #:  481200
MACs:  23578800
 
[02:19:39] (INFO) Building library...
[02:19:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8081809303977272, 'median': 1.7752685546875, 'mins': 1.7069091796875}
 
(7, 7, 1, 192, 1200, 320)
Params #:  673200
MACs:  32986800
 
[02:19:56] (INFO) Building library...
[02:19:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8260475852272726, 'median': 1.8028564453125, 'mins': 1.7152099609375}
 
(7, 7, 1, 192, 1216, 160)
Params #:  487616
MACs:  23893184
 
[02:20:14] (INFO) Building library...
[02:20:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8430231267755681, 'median': 1.8076171875, 'mins': 1.72119140625}
 
(7, 7, 1, 192, 1216, 320)
Params #:  682176
MACs:  33426624
 
[02:20:32] (INFO) Building library...
[02:20:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.82755126953125, 'median': 1.82861328125, 'mins': 1.646240234375}
 
(7, 7, 1, 192, 1232, 160)
Params #:  494032
MACs:  24207568
 
[02:20:50] (INFO) Building library...
[02:20:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8100763494318182, 'median': 1.7974853515625, 'mins': 1.720703125}
 
(7, 7, 1, 192, 1232, 320)
Params #:  691152
MACs:  33866448
 
[02:21:08] (INFO) Building library...
[02:21:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7890036843039774, 'median': 1.775390625, 'mins': 1.72021484375}
 
(7, 7, 1, 192, 1248, 160)
Params #:  500448
MACs:  24521952
 
[02:21:25] (INFO) Building library...
[02:21:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6697676225142046, 'median': 1.6337890625, 'mins': 1.56005859375}
 
(7, 7, 1, 192, 1248, 320)
Params #:  700128
MACs:  34306272
 
[02:21:43] (INFO) Building library...
[02:21:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9258200905539773, 'median': 1.8621826171875, 'mins': 1.7181396484375}
 
(7, 7, 1, 192, 1264, 160)
Params #:  506864
MACs:  24836336
 
[02:22:01] (INFO) Building library...
[02:22:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6225985440340909, 'median': 1.61279296875, 'mins': 1.5301513671875}
 
(7, 7, 1, 192, 1264, 320)
Params #:  709104
MACs:  34746096
 
[02:22:19] (INFO) Building library...
[02:22:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7883866743607955, 'median': 1.7078857421875, 'mins': 1.5833740234375}
 
(7, 7, 1, 192, 1280, 160)
Params #:  513280
MACs:  25150720
 
[02:22:36] (INFO) Building library...
[02:22:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6413363370028409, 'median': 1.6231689453125, 'mins': 1.5589599609375}
 
(7, 7, 1, 192, 1280, 320)
Params #:  718080
MACs:  35185920
 
[02:22:54] (INFO) Building library...
[02:22:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.65184326171875, 'median': 1.6275634765625, 'mins': 1.553466796875}
 
(7, 7, 1, 192, 1296, 160)
Params #:  519696
MACs:  25465104
 
[02:23:13] (INFO) Building library...
[02:23:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7767444957386365, 'median': 1.7587890625, 'mins': 1.680419921875}
 
(7, 7, 1, 192, 1296, 320)
Params #:  727056
MACs:  35625744
 
[02:23:30] (INFO) Building library...
[02:23:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.806220037286932, 'median': 1.781494140625, 'mins': 1.709228515625}
 
(7, 7, 1, 192, 1312, 160)
Params #:  526112
MACs:  25779488
 
[02:23:48] (INFO) Building library...
[02:23:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6029796253551136, 'median': 1.595703125, 'mins': 1.5272216796875}
 
(7, 7, 1, 192, 1312, 320)
Params #:  736032
MACs:  36065568
 
[02:24:06] (INFO) Building library...
[02:24:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8451438210227273, 'median': 1.8157958984375, 'mins': 1.7257080078125}
 
(7, 7, 1, 192, 1328, 160)
Params #:  532528
MACs:  26093872
 
[02:24:24] (INFO) Building library...
[02:24:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.811774236505682, 'median': 1.8006591796875, 'mins': 1.736328125}
 
(7, 7, 1, 192, 1328, 320)
Params #:  745008
MACs:  36505392
 
[02:24:42] (INFO) Building library...
[02:24:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8107776988636364, 'median': 1.783447265625, 'mins': 1.702392578125}
 
(7, 7, 1, 192, 1344, 160)
Params #:  538944
MACs:  26408256
 
[02:24:59] (INFO) Building library...
[02:24:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.699087801846591, 'median': 1.674072265625, 'mins': 1.59326171875}
 
(7, 7, 1, 192, 1344, 320)
Params #:  753984
MACs:  36945216
 
[02:25:17] (INFO) Building library...
[02:25:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.687020596590909, 'median': 1.6607666015625, 'mins': 1.5838623046875}
 
(7, 7, 1, 192, 1360, 160)
Params #:  545360
MACs:  26722640
 
[02:25:35] (INFO) Building library...
[02:25:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7629527698863636, 'median': 1.762939453125, 'mins': 1.58544921875}
 
(7, 7, 1, 192, 1360, 320)
Params #:  762960
MACs:  37385040
 
[02:25:52] (INFO) Building library...
[02:25:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.837643155184659, 'median': 1.802978515625, 'mins': 1.730224609375}
 
(7, 7, 1, 192, 1376, 160)
Params #:  551776
MACs:  27037024
 
[02:26:10] (INFO) Building library...
[02:26:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7603926225142046, 'median': 1.746826171875, 'mins': 1.666748046875}
 
(7, 7, 1, 192, 1376, 320)
Params #:  771936
MACs:  37824864
 
[02:26:28] (INFO) Building library...
[02:26:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7720048384232954, 'median': 1.7625732421875, 'mins': 1.692626953125}
 
(7, 7, 1, 192, 1392, 160)
Params #:  558192
MACs:  27351408
 
[02:26:46] (INFO) Building library...
[02:26:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.74698486328125, 'median': 1.6622314453125, 'mins': 1.560791015625}
 
(7, 7, 1, 192, 1392, 320)
Params #:  780912
MACs:  38264688
 
[02:27:04] (INFO) Building library...
[02:27:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6517356178977274, 'median': 1.6141357421875, 'mins': 1.5384521484375}
 
(7, 7, 1, 192, 1408, 160)
Params #:  564608
MACs:  27665792
 
[02:27:21] (INFO) Building library...
[02:27:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.834964266690341, 'median': 1.796142578125, 'mins': 1.7275390625}
 
(7, 7, 1, 192, 1408, 320)
Params #:  789888
MACs:  38704512
 
[02:27:39] (INFO) Building library...
[02:27:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7529285777698864, 'median': 1.6611328125, 'mins': 1.568115234375}
 
(7, 7, 1, 192, 1424, 160)
Params #:  571024
MACs:  27980176
 
[02:27:58] (INFO) Building library...
[02:27:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7876875443892046, 'median': 1.763671875, 'mins': 1.6807861328125}
 
(7, 7, 1, 192, 1424, 320)
Params #:  798864
MACs:  39144336
 
[02:28:15] (INFO) Building library...
[02:28:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6919744318181817, 'median': 1.683349609375, 'mins': 1.6126708984375}
 
(7, 7, 1, 192, 1440, 160)
Params #:  577440
MACs:  28294560
 
[02:28:33] (INFO) Building library...
[02:28:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.621148126775568, 'median': 1.6126708984375, 'mins': 1.533447265625}
 
(7, 7, 1, 192, 1440, 320)
Params #:  807840
MACs:  39584160
 
[02:28:51] (INFO) Building library...
[02:28:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6325938831676137, 'median': 1.624755859375, 'mins': 1.548583984375}
 
(7, 7, 1, 192, 1456, 160)
Params #:  583856
MACs:  28608944
 
[02:29:09] (INFO) Building library...
[02:29:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6657115589488636, 'median': 1.634521484375, 'mins': 1.5614013671875}
 
(7, 7, 1, 192, 1456, 320)
Params #:  816816
MACs:  40023984
 
[02:29:26] (INFO) Building library...
[02:29:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8171275745738635, 'median': 1.7813720703125, 'mins': 1.683349609375}
 
(7, 7, 1, 192, 1472, 160)
Params #:  590272
MACs:  28923328
 
[02:29:44] (INFO) Building library...
[02:29:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7674371892755683, 'median': 1.7578125, 'mins': 1.6898193359375}
 
(7, 7, 1, 192, 1472, 320)
Params #:  825792
MACs:  40463808
 
[02:30:02] (INFO) Building library...
[02:30:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7914861505681818, 'median': 1.7724609375, 'mins': 1.681640625}
 
(7, 7, 1, 192, 1488, 160)
Params #:  596688
MACs:  29237712
 
[02:30:20] (INFO) Building library...
[02:30:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7943625710227273, 'median': 1.7613525390625, 'mins': 1.6873779296875}
 
(7, 7, 1, 192, 1488, 320)
Params #:  834768
MACs:  40903632
 
[02:30:38] (INFO) Building library...
[02:30:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0387562144886364, 'median': 1.991943359375, 'mins': 1.88037109375}
 
(7, 7, 1, 192, 1504, 160)
Params #:  603104
MACs:  29552096
 
[02:30:55] (INFO) Building library...
[02:30:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7600763494318181, 'median': 1.7423095703125, 'mins': 1.6656494140625}
 
(7, 7, 1, 192, 1504, 320)
Params #:  843744
MACs:  41343456
 
[02:31:13] (INFO) Building library...
[02:31:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.790328702059659, 'median': 1.777099609375, 'mins': 1.7115478515625}
 
(7, 7, 1, 192, 1520, 160)
Params #:  609520
MACs:  29866480
 
[02:31:31] (INFO) Building library...
[02:31:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7731367631392045, 'median': 1.740478515625, 'mins': 1.6678466796875}
 
(7, 7, 1, 192, 1520, 320)
Params #:  852720
MACs:  41783280
 
[02:31:49] (INFO) Building library...
[02:31:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8104192560369319, 'median': 1.7825927734375, 'mins': 1.7052001953125}
 
(7, 7, 1, 192, 1536, 160)
Params #:  615936
MACs:  30180864
 
[02:32:06] (INFO) Building library...
[02:32:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8701227361505681, 'median': 1.8333740234375, 'mins': 1.7506103515625}
 
(7, 7, 1, 192, 1536, 320)
Params #:  861696
MACs:  42223104
 
[02:32:24] (INFO) Building library...
[02:32:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7842507102272727, 'median': 1.761962890625, 'mins': 1.6865234375}
 
(7, 7, 1, 192, 1552, 160)
Params #:  622352
MACs:  30495248
 
[02:32:42] (INFO) Building library...
[02:32:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7249422940340908, 'median': 1.660400390625, 'mins': 1.5562744140625}
 
(7, 7, 1, 192, 1552, 320)
Params #:  870672
MACs:  42662928
 
[02:33:00] (INFO) Building library...
[02:33:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6986095081676136, 'median': 1.66015625, 'mins': 1.574951171875}
 
(7, 7, 1, 192, 1568, 160)
Params #:  628768
MACs:  30809632
 
[02:33:18] (INFO) Building library...
[02:33:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.898308771306818, 'median': 1.79345703125, 'mins': 1.701171875}
 
(7, 7, 1, 192, 1568, 320)
Params #:  879648
MACs:  43102752
 
[02:33:36] (INFO) Building library...
[02:33:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.811404696377841, 'median': 1.787841796875, 'mins': 1.7244873046875}
 
(7, 7, 1, 192, 1584, 160)
Params #:  635184
MACs:  31124016
 
[02:33:54] (INFO) Building library...
[02:33:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8487382368607954, 'median': 1.8272705078125, 'mins': 1.7421875}
 
(7, 7, 1, 192, 1584, 320)
Params #:  888624
MACs:  43542576
 
[02:34:11] (INFO) Building library...
[02:34:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7682084517045455, 'median': 1.7542724609375, 'mins': 1.6763916015625}
 
(7, 7, 1, 192, 1600, 160)
Params #:  641600
MACs:  31438400
 
[02:34:29] (INFO) Building library...
[02:34:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.792792302911932, 'median': 1.7830810546875, 'mins': 1.711669921875}
 
(7, 7, 1, 192, 1600, 320)
Params #:  897600
MACs:  43982400
 
[02:34:47] (INFO) Building library...
[02:34:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7575494939630683, 'median': 1.7520751953125, 'mins': 1.6834716796875}
 
(7, 7, 1, 192, 1616, 160)
Params #:  648016
MACs:  31752784
 
[02:35:05] (INFO) Building library...
[02:35:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7946056019176135, 'median': 1.7587890625, 'mins': 1.690185546875}
 
(7, 7, 1, 192, 1616, 320)
Params #:  906576
MACs:  44422224
 
[02:35:23] (INFO) Building library...
[02:35:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7909956498579545, 'median': 1.767578125, 'mins': 1.689697265625}
 
(7, 7, 1, 192, 1632, 160)
Params #:  654432
MACs:  32067168
 
[02:35:40] (INFO) Building library...
[02:35:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8224276455965909, 'median': 1.8052978515625, 'mins': 1.73779296875}
 
(7, 7, 1, 192, 1632, 320)
Params #:  915552
MACs:  44862048
 
[02:35:58] (INFO) Building library...
[02:35:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.81707763671875, 'median': 1.79296875, 'mins': 1.7120361328125}
 
(7, 7, 1, 192, 1648, 160)
Params #:  660848
MACs:  32381552
 
[02:36:16] (INFO) Building library...
[02:36:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8124589399857955, 'median': 1.803466796875, 'mins': 1.7386474609375}
 
(7, 7, 1, 192, 1648, 320)
Params #:  924528
MACs:  45301872
 
[02:36:34] (INFO) Building library...
[02:36:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8030717329545454, 'median': 1.7596435546875, 'mins': 1.6949462890625}
 
(7, 7, 1, 192, 1664, 160)
Params #:  667264
MACs:  32695936
 
[02:36:52] (INFO) Building library...
[02:36:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8442937677556819, 'median': 1.815185546875, 'mins': 1.7359619140625}
 
(7, 7, 1, 192, 1664, 320)
Params #:  933504
MACs:  45741696
 
[02:37:10] (INFO) Building library...
[02:37:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.78238525390625, 'median': 1.76025390625, 'mins': 1.69580078125}
 
(7, 7, 1, 192, 1680, 160)
Params #:  673680
MACs:  33010320
 
[02:37:28] (INFO) Building library...
[02:37:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.81285400390625, 'median': 1.7861328125, 'mins': 1.7060546875}
 
(7, 7, 1, 192, 1680, 320)
Params #:  942480
MACs:  46181520
 
[02:37:46] (INFO) Building library...
[02:37:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8573342063210228, 'median': 1.833984375, 'mins': 1.7464599609375}
 
(7, 7, 1, 192, 1696, 160)
Params #:  680096
MACs:  33324704
 
[02:38:04] (INFO) Building library...
[02:38:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8390025745738636, 'median': 1.8104248046875, 'mins': 1.7178955078125}
 
(7, 7, 1, 192, 1696, 320)
Params #:  951456
MACs:  46621344
 
[02:38:22] (INFO) Building library...
[02:38:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7901600230823864, 'median': 1.765625, 'mins': 1.6873779296875}
 
(7, 7, 1, 192, 1712, 160)
Params #:  686512
MACs:  33639088
 
[02:38:40] (INFO) Building library...
[02:38:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7772039240056818, 'median': 1.7691650390625, 'mins': 1.701416015625}
 
(7, 7, 1, 192, 1712, 320)
Params #:  960432
MACs:  47061168
 
[02:38:57] (INFO) Building library...
[02:38:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8354658647017046, 'median': 1.8052978515625, 'mins': 1.7216796875}
 
(56, 5, 2, 24, 16, 16)
Params #:  1040
MACs:  1718528
 
[02:39:15] (INFO) Building library...
[02:39:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5834972034801136, 'median': 1.567626953125, 'mins': 1.4940185546875}
 
(56, 5, 2, 24, 16, 32)
Params #:  1296
MACs:  1919232
 
[02:39:32] (INFO) Building library...
[02:39:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5504660866477273, 'median': 1.52880859375, 'mins': 1.466796875}
 
(56, 5, 2, 24, 32, 16)
Params #:  2080
MACs:  3437056
 
[02:39:49] (INFO) Building library...
[02:39:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8000588156960227, 'median': 1.7589111328125, 'mins': 1.598388671875}
 
(56, 5, 2, 24, 32, 32)
Params #:  2592
MACs:  3838464
 
[02:40:07] (INFO) Building library...
[02:40:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.808651455965909, 'median': 1.7646484375, 'mins': 1.6829833984375}
 
(56, 5, 2, 24, 48, 16)
Params #:  3120
MACs:  5155584
 
[02:40:24] (INFO) Building library...
[02:40:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6463578657670455, 'median': 1.62158203125, 'mins': 1.5400390625}
 
(56, 5, 2, 24, 48, 32)
Params #:  3888
MACs:  5757696
 
[02:40:42] (INFO) Building library...
[02:40:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6005992542613636, 'median': 1.5860595703125, 'mins': 1.50830078125}
 
(56, 5, 2, 24, 64, 16)
Params #:  4160
MACs:  6874112
 
[02:40:59] (INFO) Building library...
[02:40:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9271628639914773, 'median': 1.7747802734375, 'mins': 1.69189453125}
 
(56, 5, 2, 24, 64, 32)
Params #:  5184
MACs:  7676928
 
[02:41:17] (INFO) Building library...
[02:41:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6291237571022728, 'median': 1.6102294921875, 'mins': 1.537353515625}
 
(56, 5, 2, 24, 80, 16)
Params #:  5200
MACs:  8592640
 
[02:41:35] (INFO) Building library...
[02:41:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.005462091619318, 'median': 1.8902587890625, 'mins': 1.78955078125}
 
(56, 5, 2, 24, 80, 32)
Params #:  6480
MACs:  9596160
 
[02:41:53] (INFO) Building library...
[02:41:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8543301669034091, 'median': 1.8236083984375, 'mins': 1.7255859375}
 
(56, 5, 2, 24, 96, 16)
Params #:  6240
MACs:  10311168
 
[02:42:10] (INFO) Building library...
[02:42:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6813010475852272, 'median': 1.65087890625, 'mins': 1.5654296875}
 
(56, 5, 2, 24, 96, 32)
Params #:  7776
MACs:  11515392
 
[02:42:28] (INFO) Building library...
[02:42:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0366765802556817, 'median': 1.9005126953125, 'mins': 1.8052978515625}
 
(56, 5, 2, 24, 112, 16)
Params #:  7280
MACs:  12029696
 
[02:42:46] (INFO) Building library...
[02:42:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.82833251953125, 'median': 1.71923828125, 'mins': 1.612060546875}
 
(56, 5, 2, 24, 112, 32)
Params #:  9072
MACs:  13434624
 
[02:43:04] (INFO) Building library...
[02:43:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.010149591619318, 'median': 1.8955078125, 'mins': 1.798095703125}
 
(56, 5, 2, 24, 128, 16)
Params #:  8320
MACs:  13748224
 
[02:43:22] (INFO) Building library...
[02:43:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66715087890625, 'median': 1.638916015625, 'mins': 1.5557861328125}
 
(56, 5, 2, 24, 128, 32)
Params #:  10368
MACs:  15353856
 
[02:43:40] (INFO) Building library...
[02:43:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7971546519886363, 'median': 1.694091796875, 'mins': 1.6068115234375}
 
(56, 5, 2, 24, 144, 16)
Params #:  9360
MACs:  15466752
 
[02:43:58] (INFO) Building library...
[02:43:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5499212091619319, 'median': 1.526123046875, 'mins': 1.4356689453125}
 
(56, 5, 2, 24, 144, 32)
Params #:  11664
MACs:  17273088
 
[02:44:15] (INFO) Building library...
[02:44:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.2988469904119317, 'median': 1.2816162109375, 'mins': 1.2001953125}
 
(56, 5, 2, 24, 160, 16)
Params #:  10400
MACs:  17185280
 
[02:44:33] (INFO) Building library...
[02:44:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7972878196022728, 'median': 1.6964111328125, 'mins': 1.599853515625}
 
(56, 5, 2, 24, 160, 32)
Params #:  12960
MACs:  19192320
 
[02:44:51] (INFO) Building library...
[02:44:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9007546164772726, 'median': 1.8004150390625, 'mins': 1.701416015625}
 
(56, 5, 2, 24, 176, 16)
Params #:  11440
MACs:  18903808
 
[02:45:09] (INFO) Building library...
[02:45:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.041144353693182, 'median': 1.91796875, 'mins': 1.8245849609375}
 
(56, 5, 2, 24, 176, 32)
Params #:  14256
MACs:  21111552
 
[02:45:27] (INFO) Building library...
[02:45:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.12235107421875, 'median': 2.003173828125, 'mins': 1.9127197265625}
 
(56, 5, 2, 24, 192, 16)
Params #:  12480
MACs:  20622336
 
[02:45:45] (INFO) Building library...
[02:45:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9090021306818181, 'median': 1.8253173828125, 'mins': 1.72314453125}
 
(56, 5, 2, 24, 192, 32)
Params #:  15552
MACs:  23030784
 
[02:46:03] (INFO) Building library...
[02:46:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6324052290482955, 'median': 1.5506591796875, 'mins': 1.451904296875}
 
(56, 5, 2, 24, 208, 16)
Params #:  13520
MACs:  22340864
 
[02:46:21] (INFO) Building library...
[02:46:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.867074307528409, 'median': 1.7720947265625, 'mins': 1.6517333984375}
 
(56, 5, 2, 24, 208, 32)
Params #:  16848
MACs:  24950016
 
[02:46:39] (INFO) Building library...
[02:46:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9072065873579545, 'median': 1.8260498046875, 'mins': 1.7056884765625}
 
(28, 5, 1, 32, 16, 16)
Params #:  1168
MACs:  915712
 
[02:46:57] (INFO) Building library...
[02:46:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4885553533380682, 'median': 1.46875, 'mins': 1.3887939453125}
 
(28, 5, 1, 32, 16, 32)
Params #:  1424
MACs:  1116416
 
[02:47:14] (INFO) Building library...
[02:47:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1594271573153407, 'median': 2.1307373046875, 'mins': 2.04296875}
 
(28, 5, 1, 32, 32, 16)
Params #:  2336
MACs:  1831424
 
[02:47:33] (INFO) Building library...
[02:47:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.9502341530539773, 'median': 0.927734375, 'mins': 0.8643798828125}
 
(28, 5, 1, 32, 32, 32)
Params #:  2848
MACs:  2232832
 
[02:47:47] (INFO) Building library...
[02:47:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.517403897372159, 'median': 1.502197265625, 'mins': 1.395751953125}
 
(28, 5, 1, 32, 48, 16)
Params #:  3504
MACs:  2747136
 
[02:48:03] (INFO) Building library...
[02:48:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8045188210227272, 'median': 1.78173828125, 'mins': 1.6173095703125}
 
(28, 5, 1, 32, 48, 32)
Params #:  4272
MACs:  3349248
 
[02:48:20] (INFO) Building library...
[02:48:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2135198419744317, 'median': 2.18408203125, 'mins': 2.1046142578125}
 
(28, 5, 1, 32, 64, 16)
Params #:  4672
MACs:  3662848
 
[02:48:39] (INFO) Building library...
[02:48:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3562200372869317, 'median': 1.3416748046875, 'mins': 1.2708740234375}
 
(28, 5, 1, 32, 64, 32)
Params #:  5696
MACs:  4465664
 
[02:48:57] (INFO) Building library...
[02:48:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0120272549715907, 'median': 1.986083984375, 'mins': 1.914306640625}
 
(28, 5, 1, 32, 80, 16)
Params #:  5840
MACs:  4578560
 
[02:49:16] (INFO) Building library...
[02:49:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7111161665482955, 'median': 1.6815185546875, 'mins': 1.6036376953125}
 
(28, 5, 1, 32, 80, 32)
Params #:  7120
MACs:  5582080
 
[02:49:33] (INFO) Building library...
[02:49:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3756969105113637, 'median': 2.3634033203125, 'mins': 2.260498046875}
 
(28, 5, 1, 32, 96, 16)
Params #:  7008
MACs:  5494272
 
[02:49:53] (INFO) Building library...
[02:49:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5170765269886364, 'median': 1.490478515625, 'mins': 1.398681640625}
 
(28, 5, 1, 32, 96, 32)
Params #:  8544
MACs:  6698496
 
[02:50:10] (INFO) Building library...
[02:50:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3688709605823863, 'median': 2.3411865234375, 'mins': 2.240966796875}
 
(28, 5, 1, 32, 112, 16)
Params #:  8176
MACs:  6409984
 
[02:50:29] (INFO) Building library...
[02:50:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9328191583806817, 'median': 1.7672119140625, 'mins': 1.6773681640625}
 
(28, 5, 1, 32, 112, 32)
Params #:  9968
MACs:  7814912
 
[02:50:46] (INFO) Building library...
[02:50:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4260897549715907, 'median': 2.3836669921875, 'mins': 2.288330078125}
 
(28, 5, 1, 32, 128, 16)
Params #:  9344
MACs:  7325696
 
[02:51:06] (INFO) Building library...
[02:51:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7161465731534091, 'median': 1.7071533203125, 'mins': 1.641845703125}
 
(28, 5, 1, 32, 128, 32)
Params #:  11392
MACs:  8931328
 
[02:51:23] (INFO) Building library...
[02:51:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.422914817116477, 'median': 2.3914794921875, 'mins': 2.299072265625}
 
(28, 5, 1, 32, 144, 16)
Params #:  10512
MACs:  8241408
 
[02:51:42] (INFO) Building library...
[02:51:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7520319158380682, 'median': 1.718017578125, 'mins': 1.645751953125}
 
(28, 5, 1, 32, 144, 32)
Params #:  12816
MACs:  10047744
 
[02:51:59] (INFO) Building library...
[02:51:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.513196910511364, 'median': 2.484130859375, 'mins': 2.39111328125}
 
(28, 5, 1, 32, 160, 16)
Params #:  11680
MACs:  9157120
 
[02:52:19] (INFO) Building library...
[02:52:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7325328480113635, 'median': 1.7093505859375, 'mins': 1.6158447265625}
 
(28, 5, 1, 32, 160, 32)
Params #:  14240
MACs:  11164160
 
[02:52:36] (INFO) Building library...
[02:52:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5331953568892045, 'median': 2.4925537109375, 'mins': 2.3834228515625}
 
(28, 5, 1, 32, 176, 16)
Params #:  12848
MACs:  10072832
 
[02:52:55] (INFO) Building library...
[02:52:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.757193270596591, 'median': 1.7510986328125, 'mins': 1.6014404296875}
 
(28, 5, 1, 32, 176, 32)
Params #:  15664
MACs:  12280576
 
[02:53:13] (INFO) Building library...
[02:53:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4526256214488638, 'median': 2.45849609375, 'mins': 2.231201171875}
 
(28, 5, 1, 32, 192, 16)
Params #:  14016
MACs:  10988544
 
[02:53:32] (INFO) Building library...
[02:53:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4593916459517045, 'median': 1.449951171875, 'mins': 1.383544921875}
 
(28, 5, 1, 32, 192, 32)
Params #:  17088
MACs:  13396992
 
[02:53:50] (INFO) Building library...
[02:53:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.874637118252841, 'median': 1.8455810546875, 'mins': 1.7730712890625}
 
(28, 5, 1, 32, 208, 16)
Params #:  15184
MACs:  11904256
 
[02:54:09] (INFO) Building library...
[02:54:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.60438232421875, 'median': 1.594482421875, 'mins': 1.5130615234375}
 
(28, 5, 1, 32, 208, 32)
Params #:  18512
MACs:  14513408
 
[02:54:27] (INFO) Building library...
[02:54:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5290150035511365, 'median': 2.498046875, 'mins': 2.3890380859375}
 
(28, 5, 1, 32, 224, 16)
Params #:  16352
MACs:  12819968
 
[02:54:46] (INFO) Building library...
[02:54:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7433815696022728, 'median': 1.5777587890625, 'mins': 1.4833984375}
 
(28, 5, 1, 32, 224, 32)
Params #:  19936
MACs:  15629824
 
[02:55:03] (INFO) Building library...
[02:55:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.539594060724432, 'median': 2.48681640625, 'mins': 2.3958740234375}
 
(28, 5, 1, 32, 240, 16)
Params #:  17520
MACs:  13735680
 
[02:55:23] (INFO) Building library...
[02:55:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9857321999289772, 'median': 1.93212890625, 'mins': 1.8353271484375}
 
(28, 5, 1, 32, 240, 32)
Params #:  21360
MACs:  16746240
 
[02:55:40] (INFO) Building library...
[02:55:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.463357821377841, 'median': 2.42431640625, 'mins': 2.3321533203125}
 
(28, 5, 1, 32, 256, 16)
Params #:  18688
MACs:  14651392
 
[02:55:59] (INFO) Building library...
[02:55:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.722781649502841, 'median': 1.700439453125, 'mins': 1.604248046875}
 
(28, 5, 1, 32, 256, 32)
Params #:  22784
MACs:  17862656
 
[02:56:16] (INFO) Building library...
[02:56:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5824840198863637, 'median': 2.552490234375, 'mins': 2.4407958984375}
 
(28, 5, 1, 32, 272, 16)
Params #:  19856
MACs:  15567104
 
[02:56:36] (INFO) Building library...
[02:56:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7867054332386363, 'median': 1.7786865234375, 'mins': 1.7100830078125}
 
(28, 5, 1, 32, 272, 32)
Params #:  24208
MACs:  18979072
 
[02:56:53] (INFO) Building library...
[02:56:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5280439897017044, 'median': 2.4857177734375, 'mins': 2.3868408203125}
 
(28, 3, 1, 32, 32, 16)
Params #:  1824
MACs:  1430016
 
[02:57:12] (INFO) Building library...
[02:57:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.81744384765625, 'median': 0.8001708984375, 'mins': 0.74267578125}
 
(28, 3, 1, 32, 32, 32)
Params #:  2336
MACs:  1831424
 
[02:57:26] (INFO) Building library...
[02:57:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4961370294744318, 'median': 1.4805908203125, 'mins': 1.39453125}
 
(28, 5, 2, 32, 16, 32)
Params #:  1424
MACs:  580160
 
[02:57:42] (INFO) Building library...
[02:57:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.92960205078125, 'median': 1.8592529296875, 'mins': 1.5440673828125}
 
(28, 5, 2, 32, 16, 64)
Params #:  1936
MACs:  680512
 
[02:57:59] (INFO) Building library...
[02:57:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.498885830965909, 'median': 1.4869384765625, 'mins': 1.4251708984375}
 
(28, 5, 2, 32, 32, 32)
Params #:  2848
MACs:  1160320
 
[02:58:17] (INFO) Building library...
[02:58:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.8435136274857955, 'median': 0.8306884765625, 'mins': 0.771484375}
 
(28, 5, 2, 32, 32, 64)
Params #:  3872
MACs:  1361024
 
[02:58:31] (INFO) Building library...
[02:58:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 0.8480945933948864, 'median': 0.83837890625, 'mins': 0.7750244140625}
 
(28, 5, 2, 32, 48, 32)
Params #:  4272
MACs:  1740480
 
[02:58:45] (INFO) Building library...
[02:58:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.484244051846591, 'median': 1.458984375, 'mins': 1.3817138671875}
 
(28, 5, 2, 32, 48, 64)
Params #:  5808
MACs:  2041536
 
[02:59:02] (INFO) Building library...
[02:59:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5150901100852272, 'median': 1.49560546875, 'mins': 1.4281005859375}
 
(28, 5, 2, 32, 64, 32)
Params #:  5696
MACs:  2320640
 
[02:59:20] (INFO) Building library...
[02:59:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4781638405539772, 'median': 1.394287109375, 'mins': 1.3262939453125}
 
(28, 5, 2, 32, 64, 64)
Params #:  7744
MACs:  2722048
 
[02:59:38] (INFO) Building library...
[02:59:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3974409623579545, 'median': 1.3800048828125, 'mins': 1.30224609375}
 
(28, 5, 2, 32, 80, 32)
Params #:  7120
MACs:  2900800
 
[02:59:55] (INFO) Building library...
[02:59:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6383999911221592, 'median': 1.6136474609375, 'mins': 1.5167236328125}
 
(28, 5, 2, 32, 80, 64)
Params #:  9680
MACs:  3402560
 
[03:00:13] (INFO) Building library...
[03:00:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6393698952414772, 'median': 1.6314697265625, 'mins': 1.56787109375}
 
(28, 5, 2, 32, 96, 32)
Params #:  8544
MACs:  3480960
 
[03:00:30] (INFO) Building library...
[03:00:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6693736683238636, 'median': 1.529052734375, 'mins': 1.4100341796875}
 
(28, 5, 2, 32, 96, 64)
Params #:  11616
MACs:  4083072
 
[03:00:48] (INFO) Building library...
[03:00:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6676924272017046, 'median': 1.641357421875, 'mins': 1.5584716796875}
 
(28, 5, 2, 32, 112, 32)
Params #:  9968
MACs:  4061120
 
[03:01:05] (INFO) Building library...
[03:01:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.710107421875, 'median': 1.6806640625, 'mins': 1.5985107421875}
 
(28, 5, 2, 32, 112, 64)
Params #:  13552
MACs:  4763584
 
[03:01:23] (INFO) Building library...
[03:01:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4539007013494318, 'median': 1.44677734375, 'mins': 1.376708984375}
 
(28, 5, 2, 32, 128, 32)
Params #:  11392
MACs:  4641280
 
[03:01:40] (INFO) Building library...
[03:01:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5866099964488636, 'median': 1.5609130859375, 'mins': 1.488037109375}
 
(28, 5, 2, 32, 128, 64)
Params #:  15488
MACs:  5444096
 
[03:01:57] (INFO) Building library...
[03:01:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5135009765625, 'median': 1.489990234375, 'mins': 1.42578125}
 
(28, 5, 2, 32, 144, 32)
Params #:  12816
MACs:  5221440
 
[03:02:15] (INFO) Building library...
[03:02:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6623401988636364, 'median': 1.6412353515625, 'mins': 1.556640625}
 
(28, 5, 2, 32, 144, 64)
Params #:  17424
MACs:  6124608
 
[03:02:32] (INFO) Building library...
[03:02:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6933249733664772, 'median': 1.6644287109375, 'mins': 1.5848388671875}
 
(28, 5, 2, 32, 160, 32)
Params #:  14240
MACs:  5801600
 
[03:02:50] (INFO) Building library...
[03:02:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.750927734375, 'median': 1.7236328125, 'mins': 1.6517333984375}
 
(28, 5, 2, 32, 160, 64)
Params #:  19360
MACs:  6805120
 
[03:03:07] (INFO) Building library...
[03:03:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6804521040482954, 'median': 1.6544189453125, 'mins': 1.5791015625}
 
(28, 5, 2, 32, 176, 32)
Params #:  15664
MACs:  6381760
 
[03:03:24] (INFO) Building library...
[03:03:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5556141246448865, 'median': 1.5325927734375, 'mins': 1.44384765625}
 
(28, 5, 2, 32, 176, 64)
Params #:  21296
MACs:  7485632
 
[03:03:42] (INFO) Building library...
[03:03:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.53565673828125, 'median': 1.5283203125, 'mins': 1.4583740234375}
 
(28, 5, 2, 32, 192, 32)
Params #:  17088
MACs:  6961920
 
[03:03:59] (INFO) Building library...
[03:03:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.1535378196022728, 'median': 1.1461181640625, 'mins': 1.095947265625}
 
(28, 5, 2, 32, 192, 64)
Params #:  23232
MACs:  8166144
 
[03:04:17] (INFO) Building library...
[03:04:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.18623046875, 'median': 1.173095703125, 'mins': 1.11083984375}
 
(28, 5, 2, 32, 208, 32)
Params #:  18512
MACs:  7542080
 
[03:04:35] (INFO) Building library...
[03:04:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7527476917613636, 'median': 1.7147216796875, 'mins': 1.6279296875}
 
(28, 5, 2, 32, 208, 64)
Params #:  25168
MACs:  8846656
 
[03:04:53] (INFO) Building library...
[03:04:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.882864657315341, 'median': 1.85302734375, 'mins': 1.76171875}
 
(28, 5, 2, 32, 224, 32)
Params #:  19936
MACs:  8122240
 
[03:05:10] (INFO) Building library...
[03:05:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.516116610440341, 'median': 1.4962158203125, 'mins': 1.4208984375}
 
(28, 5, 2, 32, 224, 64)
Params #:  27104
MACs:  9527168
 
[03:05:28] (INFO) Building library...
[03:05:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7751786665482954, 'median': 1.74267578125, 'mins': 1.6507568359375}
 
(28, 5, 2, 32, 240, 32)
Params #:  21360
MACs:  8702400
 
[03:05:45] (INFO) Building library...
[03:05:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6494307084517046, 'median': 1.6082763671875, 'mins': 1.5230712890625}
 
(28, 5, 2, 32, 240, 64)
Params #:  29040
MACs:  10207680
 
[03:06:03] (INFO) Building library...
[03:06:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.923409756747159, 'median': 1.9320068359375, 'mins': 1.65771484375}
 
(28, 5, 2, 32, 256, 32)
Params #:  22784
MACs:  9282560
 
[03:06:20] (INFO) Building library...
[03:06:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7037187056107954, 'median': 1.669189453125, 'mins': 1.587646484375}
 
(28, 5, 2, 32, 256, 64)
Params #:  30976
MACs:  10888192
 
[03:06:38] (INFO) Building library...
[03:06:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8064786044034091, 'median': 1.775146484375, 'mins': 1.697998046875}
 
(28, 5, 2, 32, 272, 32)
Params #:  24208
MACs:  9862720
 
[03:06:55] (INFO) Building library...
[03:06:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8433205344460226, 'median': 1.7950439453125, 'mins': 1.70361328125}
 
(28, 5, 2, 32, 272, 64)
Params #:  32912
MACs:  11568704
 
[03:07:13] (INFO) Building library...
[03:07:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8229314630681819, 'median': 1.7926025390625, 'mins': 1.70849609375}
 
(14, 5, 1, 64, 16, 32)
Params #:  1936
MACs:  379456
 
[03:07:31] (INFO) Building library...
[03:07:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4827403675426136, 'median': 1.4576416015625, 'mins': 1.385986328125}
 
(14, 5, 1, 64, 16, 64)
Params #:  2448
MACs:  479808
 
[03:07:48] (INFO) Building library...
[03:07:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.471234685724432, 'median': 2.4146728515625, 'mins': 2.3221435546875}
 
(14, 5, 1, 64, 32, 32)
Params #:  3872
MACs:  758912
 
[03:08:07] (INFO) Building library...
[03:08:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.56639404296875, 'median': 1.5457763671875, 'mins': 1.468994140625}
 
(14, 5, 1, 64, 32, 64)
Params #:  4896
MACs:  959616
 
[03:08:24] (INFO) Building library...
[03:08:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.208883389559659, 'median': 2.1719970703125, 'mins': 2.0908203125}
 
(14, 5, 1, 64, 48, 32)
Params #:  5808
MACs:  1138368
 
[03:08:43] (INFO) Building library...
[03:08:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6290227716619319, 'median': 1.596435546875, 'mins': 1.516845703125}
 
(14, 5, 1, 64, 48, 64)
Params #:  7344
MACs:  1439424
 
[03:09:00] (INFO) Building library...
[03:09:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.215621670809659, 'median': 2.1888427734375, 'mins': 2.09521484375}
 
(14, 5, 1, 64, 80, 32)
Params #:  9680
MACs:  1897280
 
[03:09:19] (INFO) Building library...
[03:09:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4885331587357955, 'median': 1.478515625, 'mins': 1.415283203125}
 
(14, 5, 1, 64, 80, 64)
Params #:  12240
MACs:  2399040
 
[03:09:36] (INFO) Building library...
[03:09:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3312888405539773, 'median': 2.3116455078125, 'mins': 2.2169189453125}
 
(14, 5, 1, 64, 96, 32)
Params #:  11616
MACs:  2276736
 
[03:09:56] (INFO) Building library...
[03:09:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3662209250710227, 'median': 1.34716796875, 'mins': 1.27880859375}
 
(14, 5, 1, 64, 96, 64)
Params #:  14688
MACs:  2878848
 
[03:10:13] (INFO) Building library...
[03:10:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2958662553267044, 'median': 2.2911376953125, 'mins': 2.211669921875}
 
(14, 5, 1, 64, 112, 32)
Params #:  13552
MACs:  2656192
 
[03:10:32] (INFO) Building library...
[03:10:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5062888405539774, 'median': 1.48779296875, 'mins': 1.412841796875}
 
(14, 5, 1, 64, 112, 64)
Params #:  17136
MACs:  3358656
 
[03:10:50] (INFO) Building library...
[03:10:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3327903053977272, 'median': 2.2921142578125, 'mins': 2.2091064453125}
 
(14, 5, 1, 64, 128, 32)
Params #:  15488
MACs:  3035648
 
[03:11:09] (INFO) Building library...
[03:11:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3084494850852273, 'median': 1.303466796875, 'mins': 1.246337890625}
 
(14, 5, 1, 64, 128, 64)
Params #:  19584
MACs:  3838464
 
[03:11:28] (INFO) Building library...
[03:11:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.044516823508523, 'median': 2.0184326171875, 'mins': 1.9400634765625}
 
(14, 5, 1, 64, 144, 32)
Params #:  17424
MACs:  3415104
 
[03:11:50] (INFO) Building library...
[03:11:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.680472079190341, 'median': 1.6546630859375, 'mins': 1.568359375}
 
(14, 5, 1, 64, 144, 64)
Params #:  22032
MACs:  4318272
 
[03:12:07] (INFO) Building library...
[03:12:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3276256214488638, 'median': 2.2938232421875, 'mins': 2.207275390625}
 
(14, 5, 1, 64, 160, 32)
Params #:  19360
MACs:  3794560
 
[03:12:26] (INFO) Building library...
[03:12:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5354636452414774, 'median': 1.5152587890625, 'mins': 1.4560546875}
 
(14, 5, 1, 64, 160, 64)
Params #:  24480
MACs:  4798080
 
[03:12:43] (INFO) Building library...
[03:12:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3384743430397728, 'median': 2.327880859375, 'mins': 2.232177734375}
 
(14, 5, 1, 64, 176, 32)
Params #:  21296
MACs:  4174016
 
[03:13:02] (INFO) Building library...
[03:13:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6426158558238637, 'median': 1.6259765625, 'mins': 1.5445556640625}
 
(14, 5, 1, 64, 176, 64)
Params #:  26928
MACs:  5277888
 
[03:13:20] (INFO) Building library...
[03:13:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3394220525568183, 'median': 2.3116455078125, 'mins': 2.2249755859375}
 
(14, 5, 1, 64, 192, 32)
Params #:  23232
MACs:  4553472
 
[03:13:39] (INFO) Building library...
[03:13:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.457449618252841, 'median': 1.4361572265625, 'mins': 1.3665771484375}
 
(14, 5, 1, 64, 192, 64)
Params #:  29376
MACs:  5757696
 
[03:13:56] (INFO) Building library...
[03:13:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0451671253551136, 'median': 2.02685546875, 'mins': 1.916015625}
 
(14, 5, 1, 64, 208, 32)
Params #:  25168
MACs:  4932928
 
[03:14:16] (INFO) Building library...
[03:14:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7203191583806818, 'median': 1.690673828125, 'mins': 1.5665283203125}
 
(14, 5, 1, 64, 208, 64)
Params #:  31824
MACs:  6237504
 
[03:14:33] (INFO) Building library...
[03:14:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.204153719815341, 'median': 2.17041015625, 'mins': 2.089599609375}
 
(14, 5, 1, 64, 224, 32)
Params #:  27104
MACs:  5312384
 
[03:14:52] (INFO) Building library...
[03:14:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6677168412642045, 'median': 1.655517578125, 'mins': 1.5892333984375}
 
(14, 5, 1, 64, 224, 64)
Params #:  34272
MACs:  6717312
 
[03:15:09] (INFO) Building library...
[03:15:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1553144975142047, 'median': 2.143310546875, 'mins': 2.082275390625}
 
(14, 5, 1, 64, 240, 32)
Params #:  29040
MACs:  5691840
 
[03:15:28] (INFO) Building library...
[03:15:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6508511629971592, 'median': 1.6436767578125, 'mins': 1.5777587890625}
 
(14, 5, 1, 64, 240, 64)
Params #:  36720
MACs:  7197120
 
[03:15:46] (INFO) Building library...
[03:15:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.167702414772727, 'median': 2.1505126953125, 'mins': 2.0845947265625}
 
(14, 5, 1, 64, 256, 32)
Params #:  30976
MACs:  6071296
 
[03:16:05] (INFO) Building library...
[03:16:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8062400124289772, 'median': 1.6751708984375, 'mins': 1.5748291015625}
 
(14, 5, 1, 64, 256, 64)
Params #:  39168
MACs:  7676928
 
[03:16:22] (INFO) Building library...
[03:16:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.153760875355114, 'median': 2.1322021484375, 'mins': 2.058837890625}
 
(14, 5, 1, 64, 272, 32)
Params #:  32912
MACs:  6450752
 
[03:16:41] (INFO) Building library...
[03:16:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6248057972301135, 'median': 1.6175537109375, 'mins': 1.552001953125}
 
(14, 5, 1, 64, 272, 64)
Params #:  41616
MACs:  8156736
 
[03:16:59] (INFO) Building library...
[03:16:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1610273881392046, 'median': 2.1453857421875, 'mins': 2.0633544921875}
 
(14, 5, 1, 64, 288, 32)
Params #:  34848
MACs:  6830208
 
[03:17:18] (INFO) Building library...
[03:17:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7040460759943181, 'median': 1.6558837890625, 'mins': 1.569091796875}
 
(14, 5, 1, 64, 288, 64)
Params #:  44064
MACs:  8636544
 
[03:17:35] (INFO) Building library...
[03:17:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.32891845703125, 'median': 2.2989501953125, 'mins': 2.209228515625}
 
(14, 5, 1, 64, 304, 32)
Params #:  36784
MACs:  7209664
 
[03:17:54] (INFO) Building library...
[03:17:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6400013316761364, 'median': 1.509033203125, 'mins': 1.4156494140625}
 
(14, 5, 1, 64, 304, 64)
Params #:  46512
MACs:  9116352
 
[03:18:12] (INFO) Building library...
[03:18:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3037664240056817, 'median': 2.329345703125, 'mins': 2.0858154296875}
 
(14, 5, 1, 64, 320, 32)
Params #:  38720
MACs:  7589120
 
[03:18:31] (INFO) Building library...
[03:18:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7845181551846592, 'median': 1.6492919921875, 'mins': 1.5513916015625}
 
(14, 5, 1, 64, 320, 64)
Params #:  48960
MACs:  9596160
 
[03:18:48] (INFO) Building library...
[03:18:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2742686878551135, 'median': 2.201904296875, 'mins': 2.078857421875}
 
(14, 5, 1, 64, 336, 32)
Params #:  40656
MACs:  7968576
 
[03:19:08] (INFO) Building library...
[03:19:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7047807173295455, 'median': 1.6787109375, 'mins': 1.5953369140625}
 
(14, 5, 1, 64, 336, 64)
Params #:  51408
MACs:  10075968
 
[03:19:25] (INFO) Building library...
[03:19:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4709028764204546, 'median': 2.4212646484375, 'mins': 2.3172607421875}
 
(14, 5, 1, 64, 352, 32)
Params #:  42592
MACs:  8348032
 
[03:19:45] (INFO) Building library...
[03:19:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6429243607954545, 'median': 1.6278076171875, 'mins': 1.55224609375}
 
(14, 5, 1, 64, 352, 64)
Params #:  53856
MACs:  10555776
 
[03:20:02] (INFO) Building library...
[03:20:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4296530983664772, 'median': 2.361328125, 'mins': 2.2752685546875}
 
(14, 5, 1, 64, 368, 32)
Params #:  44528
MACs:  8727488
 
[03:20:21] (INFO) Building library...
[03:20:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.891678133877841, 'median': 1.9083251953125, 'mins': 1.6468505859375}
 
(14, 5, 1, 64, 368, 64)
Params #:  56304
MACs:  11035584
 
[03:20:39] (INFO) Building library...
[03:20:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5565241033380683, 'median': 2.5203857421875, 'mins': 2.4091796875}
 
(14, 5, 1, 64, 384, 32)
Params #:  46464
MACs:  9106944
 
[03:20:58] (INFO) Building library...
[03:20:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4053733132102273, 'median': 1.3984375, 'mins': 1.343017578125}
 
(14, 5, 1, 64, 384, 64)
Params #:  58752
MACs:  11515392
 
[03:21:16] (INFO) Building library...
[03:21:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.732796963778409, 'median': 1.7275390625, 'mins': 1.66796875}
 
(14, 5, 1, 64, 400, 32)
Params #:  48400
MACs:  9486400
 
[03:21:36] (INFO) Building library...
[03:21:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7993352716619317, 'median': 1.7967529296875, 'mins': 1.6258544921875}
 
(14, 5, 1, 64, 400, 64)
Params #:  61200
MACs:  11995200
 
[03:21:53] (INFO) Building library...
[03:21:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4167036576704546, 'median': 2.377197265625, 'mins': 2.2808837890625}
 
(14, 5, 1, 64, 416, 32)
Params #:  50336
MACs:  9865856
 
[03:22:12] (INFO) Building library...
[03:22:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6866621537642046, 'median': 1.6668701171875, 'mins': 1.5994873046875}
 
(14, 5, 1, 64, 416, 64)
Params #:  63648
MACs:  12475008
 
[03:22:30] (INFO) Building library...
[03:22:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3478593306107953, 'median': 2.3389892578125, 'mins': 2.2607421875}
 
(14, 5, 1, 64, 432, 32)
Params #:  52272
MACs:  10245312
 
[03:22:49] (INFO) Building library...
[03:22:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7157958984375, 'median': 1.6962890625, 'mins': 1.61083984375}
 
(14, 5, 1, 64, 432, 64)
Params #:  66096
MACs:  12954816
 
[03:23:06] (INFO) Building library...
[03:23:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.508069957386364, 'median': 2.459716796875, 'mins': 2.358642578125}
 
(14, 5, 1, 64, 448, 32)
Params #:  54208
MACs:  10624768
 
[03:23:25] (INFO) Building library...
[03:23:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6434803355823864, 'median': 1.6318359375, 'mins': 1.5660400390625}
 
(14, 5, 1, 64, 448, 64)
Params #:  68544
MACs:  13434624
 
[03:23:43] (INFO) Building library...
[03:23:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6481944691051136, 'median': 2.611328125, 'mins': 2.505126953125}
 
(14, 5, 1, 64, 464, 32)
Params #:  56144
MACs:  11004224
 
[03:24:02] (INFO) Building library...
[03:24:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6970592151988637, 'median': 1.6671142578125, 'mins': 1.59912109375}
 
(14, 5, 1, 64, 464, 64)
Params #:  70992
MACs:  13914432
 
[03:24:20] (INFO) Building library...
[03:24:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4064098011363635, 'median': 2.363037109375, 'mins': 2.2742919921875}
 
(14, 5, 1, 64, 480, 32)
Params #:  58080
MACs:  11383680
 
[03:24:39] (INFO) Building library...
[03:24:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5099309747869318, 'median': 1.495849609375, 'mins': 1.43115234375}
 
(14, 5, 1, 64, 480, 64)
Params #:  73440
MACs:  14394240
 
[03:24:56] (INFO) Building library...
[03:24:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.744100674715909, 'median': 2.6103515625, 'mins': 2.4683837890625}
 
(14, 5, 1, 64, 496, 32)
Params #:  60016
MACs:  11763136
 
[03:25:15] (INFO) Building library...
[03:25:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.727099609375, 'median': 1.7010498046875, 'mins': 1.608154296875}
 
(14, 5, 1, 64, 496, 64)
Params #:  75888
MACs:  14874048
 
[03:25:32] (INFO) Building library...
[03:25:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4562144886363635, 'median': 2.42236328125, 'mins': 2.3294677734375}
 
(14, 5, 1, 64, 512, 32)
Params #:  61952
MACs:  12142592
 
[03:25:52] (INFO) Building library...
[03:25:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5441872336647726, 'median': 1.5223388671875, 'mins': 1.465087890625}
 
(14, 5, 1, 64, 512, 64)
Params #:  78336
MACs:  15353856
 
[03:26:09] (INFO) Building library...
[03:26:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.221396706321023, 'median': 2.20458984375, 'mins': 2.1173095703125}
 
(14, 5, 1, 64, 528, 32)
Params #:  63888
MACs:  12522048
 
[03:26:28] (INFO) Building library...
[03:26:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7933393998579545, 'median': 1.771240234375, 'mins': 1.669921875}
 
(14, 5, 1, 64, 528, 64)
Params #:  80784
MACs:  15833664
 
[03:26:46] (INFO) Building library...
[03:26:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.50572509765625, 'median': 2.497314453125, 'mins': 2.4107666015625}
 
(14, 5, 1, 64, 544, 32)
Params #:  65824
MACs:  12901504
 
[03:27:05] (INFO) Building library...
[03:27:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5236439098011363, 'median': 1.500244140625, 'mins': 1.4320068359375}
 
(14, 5, 1, 64, 544, 64)
Params #:  83232
MACs:  16313472
 
[03:27:22] (INFO) Building library...
[03:27:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4877241654829545, 'median': 2.43994140625, 'mins': 2.33984375}
 
(14, 5, 1, 64, 560, 32)
Params #:  67760
MACs:  13280960
 
[03:27:41] (INFO) Building library...
[03:27:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8971968217329545, 'median': 1.7427978515625, 'mins': 1.6419677734375}
 
(14, 5, 1, 64, 560, 64)
Params #:  85680
MACs:  16793280
 
[03:27:59] (INFO) Building library...
[03:27:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.570615456321023, 'median': 2.5352783203125, 'mins': 2.44677734375}
 
(14, 5, 1, 64, 16, 56)
Params #:  2320
MACs:  454720
 
[03:28:18] (INFO) Building library...
[03:28:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4746815074573865, 'median': 1.450927734375, 'mins': 1.3778076171875}
 
(14, 5, 1, 64, 16, 112)
Params #:  3216
MACs:  630336
 
[03:28:35] (INFO) Building library...
[03:28:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6731756036931817, 'median': 1.646240234375, 'mins': 1.56591796875}
 
(14, 5, 1, 64, 32, 56)
Params #:  4640
MACs:  909440
 
[03:28:52] (INFO) Building library...
[03:28:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5186590021306818, 'median': 1.4990234375, 'mins': 1.416259765625}
 
(14, 5, 1, 64, 32, 112)
Params #:  6432
MACs:  1260672
 
[03:29:09] (INFO) Building library...
[03:29:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5545066139914774, 'median': 1.528076171875, 'mins': 1.447021484375}
 
(14, 5, 1, 64, 48, 56)
Params #:  6960
MACs:  1364160
 
[03:29:26] (INFO) Building library...
[03:29:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5708984375, 'median': 1.5467529296875, 'mins': 1.478515625}
 
(14, 5, 1, 64, 48, 112)
Params #:  9648
MACs:  1891008
 
[03:29:43] (INFO) Building library...
[03:29:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4994395862926135, 'median': 1.4808349609375, 'mins': 1.4163818359375}
 
(14, 5, 1, 64, 80, 56)
Params #:  11600
MACs:  2273600
 
[03:30:00] (INFO) Building library...
[03:30:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5190773703835228, 'median': 1.4925537109375, 'mins': 1.4136962890625}
 
(14, 5, 1, 64, 80, 112)
Params #:  16080
MACs:  3151680
 
[03:30:18] (INFO) Building library...
[03:30:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6725608132102272, 'median': 1.6495361328125, 'mins': 1.55859375}
 
(14, 5, 1, 64, 96, 56)
Params #:  13920
MACs:  2728320
 
[03:30:35] (INFO) Building library...
[03:30:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6341064453125, 'median': 1.604248046875, 'mins': 1.5322265625}
 
(14, 5, 1, 64, 96, 112)
Params #:  19296
MACs:  3782016
 
[03:30:52] (INFO) Building library...
[03:30:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5974143288352274, 'median': 1.5826416015625, 'mins': 1.5264892578125}
 
(14, 5, 1, 64, 112, 56)
Params #:  16240
MACs:  3183040
 
[03:31:09] (INFO) Building library...
[03:31:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5218761097301137, 'median': 1.49755859375, 'mins': 1.41796875}
 
(14, 5, 1, 64, 112, 112)
Params #:  22512
MACs:  4412352
 
[03:31:26] (INFO) Building library...
[03:31:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4964377663352273, 'median': 1.4788818359375, 'mins': 1.406005859375}
 
(14, 5, 1, 64, 128, 56)
Params #:  18560
MACs:  3637760
 
[03:31:43] (INFO) Building library...
[03:31:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3136086203835227, 'median': 1.3079833984375, 'mins': 1.2408447265625}
 
(14, 5, 1, 64, 128, 112)
Params #:  25728
MACs:  5042688
 
[03:32:03] (INFO) Building library...
[03:32:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.3211015181107955, 'median': 1.313720703125, 'mins': 1.2607421875}
 
(14, 5, 1, 64, 144, 56)
Params #:  20880
MACs:  4092480
 
[03:32:22] (INFO) Building library...
[03:32:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.685721102627841, 'median': 1.542724609375, 'mins': 1.421630859375}
 
(14, 5, 1, 64, 144, 112)
Params #:  28944
MACs:  5673024
 
[03:32:40] (INFO) Building library...
[03:32:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6067937677556818, 'median': 1.6021728515625, 'mins': 1.54052734375}
 
(14, 5, 1, 64, 160, 56)
Params #:  23200
MACs:  4547200
 
[03:32:57] (INFO) Building library...
[03:32:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6886896306818182, 'median': 1.6610107421875, 'mins': 1.5875244140625}
 
(14, 5, 1, 64, 160, 112)
Params #:  32160
MACs:  6303360
 
[03:33:15] (INFO) Building library...
[03:33:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6634909889914773, 'median': 1.638916015625, 'mins': 1.5709228515625}
 
(14, 5, 1, 64, 176, 56)
Params #:  25520
MACs:  5001920
 
[03:33:32] (INFO) Building library...
[03:33:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6330799449573863, 'median': 1.623046875, 'mins': 1.5699462890625}
 
(14, 5, 1, 64, 176, 112)
Params #:  35376
MACs:  6933696
 
[03:33:49] (INFO) Building library...
[03:33:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.61903076171875, 'median': 1.5968017578125, 'mins': 1.5286865234375}
 
(14, 5, 1, 64, 192, 56)
Params #:  27840
MACs:  5456640
 
[03:34:06] (INFO) Building library...
[03:34:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8341819069602272, 'median': 1.6593017578125, 'mins': 1.5672607421875}
 
(14, 5, 1, 64, 192, 112)
Params #:  38592
MACs:  7564032
 
[03:34:24] (INFO) Building library...
[03:34:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6160378196022727, 'median': 1.6004638671875, 'mins': 1.5224609375}
 
(14, 5, 1, 64, 208, 56)
Params #:  30160
MACs:  5911360
 
[03:34:41] (INFO) Building library...
[03:34:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.618270596590909, 'median': 1.5927734375, 'mins': 1.5120849609375}
 
(14, 5, 1, 64, 208, 112)
Params #:  41808
MACs:  8194368
 
[03:34:59] (INFO) Building library...
[03:34:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7712990500710226, 'median': 1.75048828125, 'mins': 1.6705322265625}
 
(14, 5, 1, 64, 224, 56)
Params #:  32480
MACs:  6366080
 
[03:35:16] (INFO) Building library...
[03:35:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.690560635653409, 'median': 1.6700439453125, 'mins': 1.5859375}
 
(14, 5, 1, 64, 224, 112)
Params #:  45024
MACs:  8824704
 
[03:35:33] (INFO) Building library...
[03:35:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6213001598011363, 'median': 1.603271484375, 'mins': 1.53173828125}
 
(14, 5, 1, 64, 240, 56)
Params #:  34800
MACs:  6820800
 
[03:35:50] (INFO) Building library...
[03:35:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6074240944602274, 'median': 1.60107421875, 'mins': 1.5430908203125}
 
(14, 5, 1, 64, 240, 112)
Params #:  48240
MACs:  9455040
 
[03:36:07] (INFO) Building library...
[03:36:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8071566495028408, 'median': 1.6771240234375, 'mins': 1.5625}
 
(14, 5, 1, 64, 256, 56)
Params #:  37120
MACs:  7275520
 
[03:36:25] (INFO) Building library...
[03:36:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.454769620028409, 'median': 1.4417724609375, 'mins': 1.364990234375}
 
(14, 5, 1, 64, 256, 112)
Params #:  51456
MACs:  10085376
 
[03:36:42] (INFO) Building library...
[03:36:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7639504172585228, 'median': 1.75634765625, 'mins': 1.67822265625}
 
(14, 5, 1, 64, 272, 56)
Params #:  39440
MACs:  7730240
 
[03:36:59] (INFO) Building library...
[03:36:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.677908602627841, 'median': 1.6552734375, 'mins': 1.5787353515625}
 
(14, 5, 1, 64, 272, 112)
Params #:  54672
MACs:  10715712
 
[03:37:17] (INFO) Building library...
[03:37:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7357688210227273, 'median': 1.653076171875, 'mins': 1.5426025390625}
 
(14, 5, 1, 64, 288, 56)
Params #:  41760
MACs:  8184960
 
[03:37:34] (INFO) Building library...
[03:37:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7398248845880682, 'median': 1.7078857421875, 'mins': 1.626220703125}
 
(14, 5, 1, 64, 288, 112)
Params #:  57888
MACs:  11346048
 
[03:37:52] (INFO) Building library...
[03:37:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7514448686079545, 'median': 1.6419677734375, 'mins': 1.556884765625}
 
(14, 5, 1, 64, 304, 56)
Params #:  44080
MACs:  8639680
 
[03:38:09] (INFO) Building library...
[03:38:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6778886274857954, 'median': 1.6588134765625, 'mins': 1.5919189453125}
 
(14, 5, 1, 64, 304, 112)
Params #:  61104
MACs:  11976384
 
[03:38:27] (INFO) Building library...
[03:38:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6333795720880682, 'median': 1.603759765625, 'mins': 1.5238037109375}
 
(14, 5, 1, 64, 320, 56)
Params #:  46400
MACs:  9094400
 
[03:38:44] (INFO) Building library...
[03:38:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6944136186079546, 'median': 1.6748046875, 'mins': 1.5985107421875}
 
(14, 5, 1, 64, 320, 112)
Params #:  64320
MACs:  12606720
 
[03:39:01] (INFO) Building library...
[03:39:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6768943093039772, 'median': 1.6434326171875, 'mins': 1.57080078125}
 
(14, 5, 1, 64, 336, 56)
Params #:  48720
MACs:  9549120
 
[03:39:18] (INFO) Building library...
[03:39:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6481989080255681, 'median': 1.6378173828125, 'mins': 1.570068359375}
 
(14, 5, 1, 64, 336, 112)
Params #:  67536
MACs:  13237056
 
[03:39:36] (INFO) Building library...
[03:39:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7147627397017045, 'median': 1.6903076171875, 'mins': 1.6072998046875}
 
(14, 5, 1, 64, 352, 56)
Params #:  51040
MACs:  10003840
 
[03:39:53] (INFO) Building library...
[03:39:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.64852294921875, 'median': 1.634521484375, 'mins': 1.5771484375}
 
(14, 5, 1, 64, 352, 112)
Params #:  70752
MACs:  13867392
 
[03:40:10] (INFO) Building library...
[03:40:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6098555131392045, 'median': 1.60205078125, 'mins': 1.533935546875}
 
(14, 5, 1, 64, 368, 56)
Params #:  53360
MACs:  10458560
 
[03:40:28] (INFO) Building library...
[03:40:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8944724343039774, 'median': 1.723388671875, 'mins': 1.626953125}
 
(14, 5, 1, 64, 368, 112)
Params #:  73968
MACs:  14497728
 
[03:40:45] (INFO) Building library...
[03:40:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6409024325284092, 'median': 1.6328125, 'mins': 1.572509765625}
 
(14, 5, 1, 64, 384, 56)
Params #:  55680
MACs:  10913280
 
[03:41:03] (INFO) Building library...
[03:41:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.353255948153409, 'median': 1.343505859375, 'mins': 1.289794921875}
 
(14, 5, 1, 64, 384, 112)
Params #:  77184
MACs:  15128064
 
[03:41:21] (INFO) Building library...
[03:41:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.302115145596591, 'median': 1.2940673828125, 'mins': 1.237548828125}
 
(14, 5, 1, 64, 400, 56)
Params #:  58000
MACs:  11368000
 
[03:41:39] (INFO) Building library...
[03:41:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7136363636363636, 'median': 1.68798828125, 'mins': 1.59814453125}
 
(14, 5, 1, 64, 400, 112)
Params #:  80400
MACs:  15758400
 
[03:41:56] (INFO) Building library...
[03:41:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.736624422940341, 'median': 1.6568603515625, 'mins': 1.573486328125}
 
(14, 5, 1, 64, 416, 56)
Params #:  60320
MACs:  11822720
 
[03:42:13] (INFO) Building library...
[03:42:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.549801358309659, 'median': 1.5360107421875, 'mins': 1.4647216796875}
 
(14, 5, 1, 64, 416, 112)
Params #:  83616
MACs:  16388736
 
[03:42:31] (INFO) Building library...
[03:42:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6563909357244317, 'median': 1.63623046875, 'mins': 1.55615234375}
 
(14, 5, 1, 64, 432, 56)
Params #:  62640
MACs:  12277440
 
[03:42:48] (INFO) Building library...
[03:42:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.66712646484375, 'median': 1.508544921875, 'mins': 1.414306640625}
 
(14, 5, 1, 64, 432, 112)
Params #:  86832
MACs:  17019072
 
[03:43:05] (INFO) Building library...
[03:43:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.749386319247159, 'median': 1.7437744140625, 'mins': 1.6771240234375}
 
(14, 5, 1, 64, 448, 56)
Params #:  64960
MACs:  12732160
 
[03:43:23] (INFO) Building library...
[03:43:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7715165571732954, 'median': 1.7421875, 'mins': 1.66455078125}
 
(14, 5, 1, 64, 448, 112)
Params #:  90048
MACs:  17649408
 
[03:43:40] (INFO) Building library...
[03:43:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8972401012073863, 'median': 1.75439453125, 'mins': 1.6712646484375}
 
(14, 5, 1, 64, 464, 56)
Params #:  67280
MACs:  13186880
 
[03:43:58] (INFO) Building library...
[03:43:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5334683504971591, 'median': 1.525390625, 'mins': 1.4652099609375}
 
(14, 5, 1, 64, 464, 112)
Params #:  93264
MACs:  18279744
 
[03:44:15] (INFO) Building library...
[03:44:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5332863547585227, 'median': 1.51416015625, 'mins': 1.4383544921875}
 
(14, 5, 1, 64, 480, 56)
Params #:  69600
MACs:  13641600
 
[03:44:32] (INFO) Building library...
[03:44:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.735830965909091, 'median': 1.71044921875, 'mins': 1.62548828125}
 
(14, 5, 1, 64, 480, 112)
Params #:  96480
MACs:  18910080
 
[03:44:49] (INFO) Building library...
[03:44:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7208274147727274, 'median': 1.6868896484375, 'mins': 1.6226806640625}
 
(14, 5, 1, 64, 496, 56)
Params #:  71920
MACs:  14096320
 
[03:45:06] (INFO) Building library...
[03:45:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7194635564630683, 'median': 1.708740234375, 'mins': 1.6422119140625}
 
(14, 5, 1, 64, 496, 112)
Params #:  99696
MACs:  19540416
 
[03:45:24] (INFO) Building library...
[03:45:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7347966974431819, 'median': 1.7119140625, 'mins': 1.6373291015625}
 
(14, 5, 1, 64, 512, 56)
Params #:  74240
MACs:  14551040
 
[03:45:41] (INFO) Building library...
[03:45:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.643356045809659, 'median': 1.605224609375, 'mins': 1.5380859375}
 
(14, 5, 1, 64, 512, 112)
Params #:  102912
MACs:  20170752
 
[03:45:58] (INFO) Building library...
[03:45:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7412420099431818, 'median': 1.7303466796875, 'mins': 1.6602783203125}
 
(14, 5, 1, 64, 528, 56)
Params #:  76560
MACs:  15005760
 
[03:46:16] (INFO) Building library...
[03:46:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8381880326704545, 'median': 1.81884765625, 'mins': 1.46240234375}
 
(14, 5, 1, 64, 528, 112)
Params #:  106128
MACs:  20801088
 
[03:46:33] (INFO) Building library...
[03:46:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5478293678977273, 'median': 1.5386962890625, 'mins': 1.4683837890625}
 
(14, 5, 1, 64, 544, 56)
Params #:  78880
MACs:  15460480
 
[03:46:50] (INFO) Building library...
[03:46:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7325272993607954, 'median': 1.72119140625, 'mins': 1.6539306640625}
 
(14, 5, 1, 64, 544, 112)
Params #:  109344
MACs:  21431424
 
[03:47:08] (INFO) Building library...
[03:47:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7491643732244317, 'median': 1.7257080078125, 'mins': 1.6448974609375}
 
(14, 5, 1, 64, 560, 56)
Params #:  81200
MACs:  15915200
 
[03:47:25] (INFO) Building library...
[03:47:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9367287375710227, 'median': 1.779052734375, 'mins': 1.700439453125}
 
(14, 5, 1, 64, 560, 112)
Params #:  112560
MACs:  22061760
 
[03:47:43] (INFO) Building library...
[03:47:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5403675426136363, 'median': 1.517578125, 'mins': 1.437255859375}
 
(14, 5, 1, 112, 16, 56)
Params #:  3088
MACs:  605248
 
[03:48:00] (INFO) Building library...
[03:48:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5073097922585228, 'median': 1.4859619140625, 'mins': 1.4019775390625}
 
(14, 5, 1, 112, 16, 112)
Params #:  3984
MACs:  780864
 
[03:48:17] (INFO) Building library...
[03:48:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2200827858664773, 'median': 2.18212890625, 'mins': 2.1075439453125}
 
(14, 5, 1, 112, 32, 56)
Params #:  6176
MACs:  1210496
 
[03:48:36] (INFO) Building library...
[03:48:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6029485529119318, 'median': 1.5682373046875, 'mins': 1.4256591796875}
 
(14, 5, 1, 112, 32, 112)
Params #:  7968
MACs:  1561728
 
[03:48:53] (INFO) Building library...
[03:48:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1847578568892048, 'median': 2.1519775390625, 'mins': 2.0733642578125}
 
(14, 5, 1, 112, 48, 56)
Params #:  9264
MACs:  1815744
 
[03:49:12] (INFO) Building library...
[03:49:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5500676935369317, 'median': 1.52099609375, 'mins': 1.4534912109375}
 
(14, 5, 1, 112, 48, 112)
Params #:  11952
MACs:  2342592
 
[03:49:30] (INFO) Building library...
[03:49:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1808826793323863, 'median': 2.16845703125, 'mins': 2.09716796875}
 
(14, 5, 1, 112, 64, 56)
Params #:  12352
MACs:  2420992
 
[03:49:49] (INFO) Building library...
[03:49:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5394886363636364, 'median': 1.5089111328125, 'mins': 1.4365234375}
 
(14, 5, 1, 112, 64, 112)
Params #:  15936
MACs:  3123456
 
[03:50:06] (INFO) Building library...
[03:50:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4250688032670453, 'median': 2.3853759765625, 'mins': 2.2877197265625}
 
(14, 5, 1, 112, 80, 56)
Params #:  15440
MACs:  3026240
 
[03:50:25] (INFO) Building library...
[03:50:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6260265003551135, 'median': 1.5650634765625, 'mins': 1.4906005859375}
 
(14, 5, 1, 112, 80, 112)
Params #:  19920
MACs:  3904320
 
[03:50:42] (INFO) Building library...
[03:50:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1508189808238636, 'median': 2.1429443359375, 'mins': 2.058837890625}
 
(14, 5, 1, 112, 96, 56)
Params #:  18528
MACs:  3631488
 
[03:51:02] (INFO) Building library...
[03:51:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5707597212357955, 'median': 1.5433349609375, 'mins': 1.4622802734375}
 
(14, 5, 1, 112, 96, 112)
Params #:  23904
MACs:  4685184
 
[03:51:19] (INFO) Building library...
[03:51:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1845647638494317, 'median': 2.1607666015625, 'mins': 2.0765380859375}
 
(14, 5, 1, 112, 128, 56)
Params #:  24704
MACs:  4841984
 
[03:51:38] (INFO) Building library...
[03:51:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5430308948863636, 'median': 1.5133056640625, 'mins': 1.4404296875}
 
(14, 5, 1, 112, 128, 112)
Params #:  31872
MACs:  6246912
 
[03:51:55] (INFO) Building library...
[03:51:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.202361505681818, 'median': 2.169189453125, 'mins': 2.092529296875}
 
(14, 5, 1, 112, 144, 56)
Params #:  27792
MACs:  5447232
 
[03:52:14] (INFO) Building library...
[03:52:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6595170454545454, 'median': 1.652099609375, 'mins': 1.5809326171875}
 
(14, 5, 1, 112, 144, 112)
Params #:  35856
MACs:  7027776
 
[03:52:32] (INFO) Building library...
[03:52:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4422463156960226, 'median': 2.4033203125, 'mins': 2.3126220703125}
 
(14, 5, 1, 112, 160, 56)
Params #:  30880
MACs:  6052480
 
[03:52:51] (INFO) Building library...
[03:52:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7126564719460227, 'median': 1.6881103515625, 'mins': 1.602294921875}
 
(14, 5, 1, 112, 160, 112)
Params #:  39840
MACs:  7808640
 
[03:53:09] (INFO) Building library...
[03:53:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4308971058238638, 'median': 2.398681640625, 'mins': 2.308349609375}
 
(14, 5, 1, 112, 176, 56)
Params #:  33968
MACs:  6657728
 
[03:53:28] (INFO) Building library...
[03:53:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7122636274857954, 'median': 1.6971435546875, 'mins': 1.6219482421875}
 
(14, 5, 1, 112, 176, 112)
Params #:  43824
MACs:  8589504
 
[03:53:45] (INFO) Building library...
[03:53:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.410674493963068, 'median': 2.37744140625, 'mins': 2.2755126953125}
 
(14, 5, 1, 112, 192, 56)
Params #:  37056
MACs:  7262976
 
[03:54:04] (INFO) Building library...
[03:54:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6669333718039774, 'median': 1.6558837890625, 'mins': 1.4482421875}
 
(14, 5, 1, 112, 192, 112)
Params #:  47808
MACs:  9370368
 
[03:54:21] (INFO) Building library...
[03:54:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.396157004616477, 'median': 2.3597412109375, 'mins': 2.282958984375}
 
(14, 5, 1, 112, 208, 56)
Params #:  40144
MACs:  7868224
 
[03:54:41] (INFO) Building library...
[03:54:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7715886896306818, 'median': 1.7447509765625, 'mins': 1.66455078125}
 
(14, 5, 1, 112, 208, 112)
Params #:  51792
MACs:  10151232
 
[03:54:58] (INFO) Building library...
[03:54:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.688433283025568, 'median': 2.6766357421875, 'mins': 2.413818359375}
 
(14, 5, 1, 112, 224, 56)
Params #:  43232
MACs:  8473472
 
[03:55:17] (INFO) Building library...
[03:55:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6967418323863637, 'median': 1.664306640625, 'mins': 1.6015625}
 
(14, 5, 1, 112, 224, 112)
Params #:  55776
MACs:  10932096
 
[03:55:35] (INFO) Building library...
[03:55:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1601884321732956, 'median': 2.1495361328125, 'mins': 2.0809326171875}
 
(14, 5, 1, 112, 240, 56)
Params #:  46320
MACs:  9078720
 
[03:55:54] (INFO) Building library...
[03:55:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7560990767045455, 'median': 1.712646484375, 'mins': 1.622314453125}
 
(14, 5, 1, 112, 240, 112)
Params #:  59760
MACs:  11712960
 
[03:56:12] (INFO) Building library...
[03:56:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3586625532670453, 'median': 2.3450927734375, 'mins': 2.26318359375}
 
(14, 5, 1, 112, 256, 56)
Params #:  49408
MACs:  9683968
 
[03:56:31] (INFO) Building library...
[03:56:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7247270063920455, 'median': 1.697021484375, 'mins': 1.6168212890625}
 
(14, 5, 1, 112, 256, 112)
Params #:  63744
MACs:  12493824
 
[03:56:49] (INFO) Building library...
[03:56:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.291226473721591, 'median': 2.2579345703125, 'mins': 2.166748046875}
 
(14, 5, 1, 112, 272, 56)
Params #:  52496
MACs:  10289216
 
[03:57:08] (INFO) Building library...
[03:57:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.52578125, 'median': 1.513916015625, 'mins': 1.44189453125}
 
(14, 5, 1, 112, 272, 112)
Params #:  67728
MACs:  13274688
 
[03:57:25] (INFO) Building library...
[03:57:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5886219371448864, 'median': 2.556640625, 'mins': 2.4747314453125}
 
(14, 5, 1, 112, 288, 56)
Params #:  55584
MACs:  10894464
 
[03:57:44] (INFO) Building library...
[03:57:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6062522194602273, 'median': 1.5479736328125, 'mins': 1.45556640625}
 
(14, 5, 1, 112, 288, 112)
Params #:  71712
MACs:  14055552
 
[03:58:02] (INFO) Building library...
[03:58:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.358202015269886, 'median': 2.34375, 'mins': 2.2540283203125}
 
(14, 5, 1, 112, 304, 56)
Params #:  58672
MACs:  11499712
 
[03:58:21] (INFO) Building library...
[03:58:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7637029474431818, 'median': 1.739013671875, 'mins': 1.634033203125}
 
(14, 5, 1, 112, 304, 112)
Params #:  75696
MACs:  14836416
 
[03:58:39] (INFO) Building library...
[03:58:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5621826171875, 'median': 2.56640625, 'mins': 2.3138427734375}
 
(14, 5, 1, 112, 320, 56)
Params #:  61760
MACs:  12104960
 
[03:58:58] (INFO) Building library...
[03:58:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4972733931107955, 'median': 1.4970703125, 'mins': 1.4212646484375}
 
(14, 5, 1, 112, 320, 112)
Params #:  79680
MACs:  15617280
 
[03:59:15] (INFO) Building library...
[03:59:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.654757412997159, 'median': 2.618408203125, 'mins': 2.5169677734375}
 
(14, 5, 1, 112, 336, 56)
Params #:  64848
MACs:  12710208
 
[03:59:35] (INFO) Building library...
[03:59:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.713080388849432, 'median': 1.6837158203125, 'mins': 1.604736328125}
 
(14, 5, 1, 112, 336, 112)
Params #:  83664
MACs:  16398144
 
[03:59:52] (INFO) Building library...
[03:59:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.570751953125, 'median': 2.52294921875, 'mins': 2.413818359375}
 
(14, 5, 1, 112, 352, 56)
Params #:  67936
MACs:  13315456
 
[04:00:11] (INFO) Building library...
[04:00:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6991632634943181, 'median': 1.6708984375, 'mins': 1.5858154296875}
 
(14, 5, 1, 112, 352, 112)
Params #:  87648
MACs:  17179008
 
[04:00:28] (INFO) Building library...
[04:00:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.563870516690341, 'median': 2.51513671875, 'mins': 2.42041015625}
 
(14, 5, 1, 112, 368, 56)
Params #:  71024
MACs:  13920704
 
[04:00:47] (INFO) Building library...
[04:00:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9164129083806818, 'median': 1.7449951171875, 'mins': 1.6810302734375}
 
(14, 5, 1, 112, 368, 112)
Params #:  91632
MACs:  17959872
 
[04:01:05] (INFO) Building library...
[04:01:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.585909756747159, 'median': 2.548583984375, 'mins': 2.4578857421875}
 
(14, 5, 1, 112, 384, 56)
Params #:  74112
MACs:  14525952
 
[04:01:25] (INFO) Building library...
[04:01:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.72420654296875, 'median': 1.69189453125, 'mins': 1.60791015625}
 
(14, 5, 1, 112, 384, 112)
Params #:  95616
MACs:  18740736
 
[04:01:42] (INFO) Building library...
[04:01:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4508167613636362, 'median': 2.4000244140625, 'mins': 2.3115234375}
 
(14, 5, 1, 112, 400, 56)
Params #:  77200
MACs:  15131200
 
[04:02:01] (INFO) Building library...
[04:02:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.620284756747159, 'median': 1.611328125, 'mins': 1.4434814453125}
 
(14, 5, 1, 112, 400, 112)
Params #:  99600
MACs:  19521600
 
[04:02:19] (INFO) Building library...
[04:02:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.7441173206676135, 'median': 2.6793212890625, 'mins': 2.581298828125}
 
(14, 5, 1, 112, 416, 56)
Params #:  80288
MACs:  15736448
 
[04:02:38] (INFO) Building library...
[04:02:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7472312233664773, 'median': 1.71875, 'mins': 1.636474609375}
 
(14, 5, 1, 112, 416, 112)
Params #:  103584
MACs:  20302464
 
[04:02:56] (INFO) Building library...
[04:02:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5546686345880683, 'median': 2.5247802734375, 'mins': 2.428466796875}
 
(14, 5, 1, 112, 432, 56)
Params #:  83376
MACs:  16341696
 
[04:03:15] (INFO) Building library...
[04:03:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7315962357954546, 'median': 1.70703125, 'mins': 1.6282958984375}
 
(14, 5, 1, 112, 432, 112)
Params #:  107568
MACs:  21083328
 
[04:03:33] (INFO) Building library...
[04:03:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.524569424715909, 'median': 2.4189453125, 'mins': 2.3275146484375}
 
(14, 5, 1, 112, 448, 56)
Params #:  86464
MACs:  16946944
 
[04:03:52] (INFO) Building library...
[04:03:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8294477982954545, 'median': 1.806640625, 'mins': 1.7105712890625}
 
(14, 5, 1, 112, 448, 112)
Params #:  111552
MACs:  21864192
 
[04:04:10] (INFO) Building library...
[04:04:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4644964044744317, 'median': 2.4232177734375, 'mins': 2.3392333984375}
 
(14, 5, 1, 112, 464, 56)
Params #:  89552
MACs:  17552192
 
[04:04:29] (INFO) Building library...
[04:04:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.766897860440341, 'median': 1.7376708984375, 'mins': 1.6627197265625}
 
(14, 5, 1, 112, 464, 112)
Params #:  115536
MACs:  22645056
 
[04:04:46] (INFO) Building library...
[04:04:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 3.0031505237926135, 'median': 2.9388427734375, 'mins': 2.847412109375}
 
(14, 5, 1, 112, 480, 56)
Params #:  92640
MACs:  18157440
 
[04:05:06] (INFO) Building library...
[04:05:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7584372780539772, 'median': 1.7099609375, 'mins': 1.639892578125}
 
(14, 5, 1, 112, 480, 112)
Params #:  119520
MACs:  23425920
 
[04:05:23] (INFO) Building library...
[04:05:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5035444779829548, 'median': 2.474853515625, 'mins': 2.3638916015625}
 
(14, 5, 1, 112, 496, 56)
Params #:  95728
MACs:  18762688
 
[04:05:43] (INFO) Building library...
[04:05:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7773748224431818, 'median': 1.7576904296875, 'mins': 1.666259765625}
 
(14, 5, 1, 112, 496, 112)
Params #:  123504
MACs:  24206784
 
[04:06:00] (INFO) Building library...
[04:06:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4470814098011364, 'median': 2.41162109375, 'mins': 2.3182373046875}
 
(14, 5, 1, 112, 512, 56)
Params #:  98816
MACs:  19367936
 
[04:06:19] (INFO) Building library...
[04:06:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.75164794921875, 'median': 1.7281494140625, 'mins': 1.634033203125}
 
(14, 5, 1, 112, 512, 112)
Params #:  127488
MACs:  24987648
 
[04:06:37] (INFO) Building library...
[04:06:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.705315607244318, 'median': 2.6484375, 'mins': 2.55224609375}
 
(14, 5, 1, 112, 528, 56)
Params #:  101904
MACs:  19973184
 
[04:06:56] (INFO) Building library...
[04:06:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8655007102272727, 'median': 1.759521484375, 'mins': 1.661376953125}
 
(14, 5, 1, 112, 528, 112)
Params #:  131472
MACs:  25768512
 
[04:07:14] (INFO) Building library...
[04:07:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6245649857954545, 'median': 2.5843505859375, 'mins': 2.478271484375}
 
(14, 5, 1, 112, 544, 56)
Params #:  104992
MACs:  20578432
 
[04:07:33] (INFO) Building library...
[04:07:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.76727294921875, 'median': 1.752685546875, 'mins': 1.6937255859375}
 
(14, 5, 1, 112, 544, 112)
Params #:  135456
MACs:  26549376
 
[04:07:50] (INFO) Building library...
[04:07:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3790327592329548, 'median': 2.4022216796875, 'mins': 2.1400146484375}
 
(14, 5, 1, 112, 560, 56)
Params #:  108080
MACs:  21183680
 
[04:08:10] (INFO) Building library...
[04:08:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7845447887073864, 'median': 1.7742919921875, 'mins': 1.7034912109375}
 
(14, 5, 1, 112, 560, 112)
Params #:  139440
MACs:  27330240
 
[04:08:27] (INFO) Building library...
[04:08:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2982588334517047, 'median': 2.2652587890625, 'mins': 2.184326171875}
 
(14, 5, 1, 112, 576, 56)
Params #:  111168
MACs:  21788928
 
[04:08:47] (INFO) Building library...
[04:08:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8980757279829545, 'median': 1.79248046875, 'mins': 1.6929931640625}
 
(14, 5, 1, 112, 576, 112)
Params #:  143424
MACs:  28111104
 
[04:09:05] (INFO) Building library...
[04:09:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6447432084517044, 'median': 2.615478515625, 'mins': 2.5010986328125}
 
(14, 5, 1, 112, 592, 56)
Params #:  114256
MACs:  22394176
 
[04:09:24] (INFO) Building library...
[04:09:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.73187255859375, 'median': 1.6002197265625, 'mins': 1.52490234375}
 
(14, 5, 1, 112, 592, 112)
Params #:  147408
MACs:  28891968
 
[04:09:42] (INFO) Building library...
[04:09:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6199806906960226, 'median': 2.57275390625, 'mins': 2.47314453125}
 
(14, 5, 1, 112, 608, 56)
Params #:  117344
MACs:  22999424
 
[04:10:01] (INFO) Building library...
[04:10:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6152632279829546, 'median': 1.5975341796875, 'mins': 1.530517578125}
 
(14, 5, 1, 112, 608, 112)
Params #:  151392
MACs:  29672832
 
[04:10:19] (INFO) Building library...
[04:10:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.705535333806818, 'median': 2.6719970703125, 'mins': 2.56494140625}
 
(14, 5, 1, 112, 624, 56)
Params #:  120432
MACs:  23604672
 
[04:10:38] (INFO) Building library...
[04:10:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.923141202059659, 'median': 1.9088134765625, 'mins': 1.7442626953125}
 
(14, 5, 1, 112, 624, 112)
Params #:  155376
MACs:  30453696
 
[04:10:56] (INFO) Building library...
[04:10:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5526566938920454, 'median': 2.503662109375, 'mins': 2.4100341796875}
 
(14, 5, 1, 112, 640, 56)
Params #:  123520
MACs:  24209920
 
[04:11:15] (INFO) Building library...
[04:11:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8308083274147726, 'median': 1.8040771484375, 'mins': 1.703125}
 
(14, 5, 1, 112, 640, 112)
Params #:  159360
MACs:  31234560
 
[04:11:33] (INFO) Building library...
[04:11:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.651674582741477, 'median': 2.528076171875, 'mins': 2.389404296875}
 
(14, 5, 1, 112, 656, 56)
Params #:  126608
MACs:  24815168
 
[04:11:52] (INFO) Building library...
[04:11:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9293612393465909, 'median': 1.9029541015625, 'mins': 1.7572021484375}
 
(14, 5, 1, 112, 656, 112)
Params #:  163344
MACs:  32015424
 
[04:12:10] (INFO) Building library...
[04:12:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5679432262073862, 'median': 2.5318603515625, 'mins': 2.42626953125}
 
(14, 5, 1, 112, 672, 56)
Params #:  129696
MACs:  25420416
 
[04:12:29] (INFO) Building library...
[04:12:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.017636940696023, 'median': 1.9796142578125, 'mins': 1.8466796875}
 
(14, 5, 1, 112, 672, 112)
Params #:  167328
MACs:  32796288
 
[04:12:47] (INFO) Building library...
[04:12:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.468307217684659, 'median': 2.4423828125, 'mins': 2.3497314453125}
 
(14, 5, 1, 112, 688, 56)
Params #:  132784
MACs:  26025664
 
[04:13:06] (INFO) Building library...
[04:13:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8240955699573864, 'median': 1.815673828125, 'mins': 1.7413330078125}
 
(14, 5, 1, 112, 688, 112)
Params #:  171312
MACs:  33577152
 
[04:13:24] (INFO) Building library...
[04:13:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.504742986505682, 'median': 2.4644775390625, 'mins': 2.3648681640625}
 
(14, 5, 1, 112, 704, 56)
Params #:  135872
MACs:  26630912
 
[04:13:43] (INFO) Building library...
[04:13:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7423439719460228, 'median': 1.71142578125, 'mins': 1.6463623046875}
 
(14, 5, 1, 112, 704, 112)
Params #:  175296
MACs:  34358016
 
[04:14:01] (INFO) Building library...
[04:14:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.573219992897727, 'median': 2.53271484375, 'mins': 2.4365234375}
 
(14, 5, 1, 112, 720, 56)
Params #:  138960
MACs:  27236160
 
[04:14:20] (INFO) Building library...
[04:14:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8504416725852273, 'median': 1.7679443359375, 'mins': 1.6859130859375}
 
(14, 5, 1, 112, 720, 112)
Params #:  179280
MACs:  35138880
 
[04:14:37] (INFO) Building library...
[04:14:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4666648171164773, 'median': 2.4573974609375, 'mins': 2.3607177734375}
 
(14, 5, 1, 112, 736, 56)
Params #:  142048
MACs:  27841408
 
[04:14:57] (INFO) Building library...
[04:14:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1628839666193183, 'median': 1.979248046875, 'mins': 1.8848876953125}
 
(14, 5, 1, 112, 736, 112)
Params #:  183264
MACs:  35919744
 
[04:15:15] (INFO) Building library...
[04:15:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5084738991477273, 'median': 2.4696044921875, 'mins': 2.3626708984375}
 
(14, 5, 1, 112, 752, 56)
Params #:  145136
MACs:  28446656
 
[04:15:34] (INFO) Building library...
[04:15:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9126997514204545, 'median': 1.7841796875, 'mins': 1.669189453125}
 
(14, 5, 1, 112, 752, 112)
Params #:  187248
MACs:  36700608
 
[04:15:52] (INFO) Building library...
[04:15:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4686390269886362, 'median': 2.4298095703125, 'mins': 2.3388671875}
 
(14, 5, 1, 112, 768, 56)
Params #:  148224
MACs:  29051904
 
[04:16:11] (INFO) Building library...
[04:16:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8269797585227272, 'median': 1.78173828125, 'mins': 1.6968994140625}
 
(14, 5, 1, 112, 768, 112)
Params #:  191232
MACs:  37481472
 
[04:16:29] (INFO) Building library...
[04:16:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.8343772194602272, 'median': 2.8199462890625, 'mins': 2.4752197265625}
 
(14, 5, 1, 112, 784, 56)
Params #:  151312
MACs:  29657152
 
[04:16:48] (INFO) Building library...
[04:16:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8709383877840908, 'median': 1.822021484375, 'mins': 1.70556640625}
 
(14, 5, 1, 112, 784, 112)
Params #:  195216
MACs:  38262336
 
[04:17:05] (INFO) Building library...
[04:17:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.50777587890625, 'median': 2.45849609375, 'mins': 2.3642578125}
 
(14, 5, 1, 112, 800, 56)
Params #:  154400
MACs:  30262400
 
[04:17:25] (INFO) Building library...
[04:17:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.93626708984375, 'median': 1.8504638671875, 'mins': 1.659423828125}
 
(14, 5, 1, 112, 800, 112)
Params #:  199200
MACs:  39043200
 
[04:17:42] (INFO) Building library...
[04:17:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.60826416015625, 'median': 2.48486328125, 'mins': 2.376708984375}
 
(14, 5, 1, 112, 816, 56)
Params #:  157488
MACs:  30867648
 
[04:18:02] (INFO) Building library...
[04:18:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.795525568181818, 'median': 1.768310546875, 'mins': 1.6800537109375}
 
(14, 5, 1, 112, 816, 112)
Params #:  203184
MACs:  39824064
 
[04:18:19] (INFO) Building library...
[04:18:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.475432794744318, 'median': 2.443603515625, 'mins': 2.3470458984375}
 
(14, 5, 1, 112, 832, 56)
Params #:  160576
MACs:  31472896
 
[04:18:39] (INFO) Building library...
[04:18:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.744195001775568, 'median': 1.7242431640625, 'mins': 1.647705078125}
 
(14, 5, 1, 112, 832, 112)
Params #:  207168
MACs:  40604928
 
[04:18:56] (INFO) Building library...
[04:18:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5434248490767044, 'median': 2.5076904296875, 'mins': 2.4139404296875}
 
(14, 5, 1, 112, 848, 56)
Params #:  163664
MACs:  32078144
 
[04:19:16] (INFO) Building library...
[04:19:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8216275301846592, 'median': 1.7965087890625, 'mins': 1.7025146484375}
 
(14, 5, 1, 112, 848, 112)
Params #:  211152
MACs:  41385792
 
[04:19:33] (INFO) Building library...
[04:19:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4976917613636362, 'median': 2.4619140625, 'mins': 2.3638916015625}
 
(14, 5, 1, 112, 864, 56)
Params #:  166752
MACs:  32683392
 
[04:19:53] (INFO) Building library...
[04:19:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7509865500710227, 'median': 1.7337646484375, 'mins': 1.6650390625}
 
(14, 5, 1, 112, 864, 112)
Params #:  215136
MACs:  42166656
 
[04:20:10] (INFO) Building library...
[04:20:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4753318093039773, 'median': 2.431640625, 'mins': 2.3470458984375}
 
(14, 5, 1, 112, 880, 56)
Params #:  169840
MACs:  33288640
 
[04:20:30] (INFO) Building library...
[04:20:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8165283203125, 'median': 1.77978515625, 'mins': 1.7095947265625}
 
(14, 5, 1, 112, 880, 112)
Params #:  219120
MACs:  42947520
 
[04:20:47] (INFO) Building library...
[04:20:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.487831809303977, 'median': 2.457763671875, 'mins': 2.3564453125}
 
(14, 5, 1, 112, 896, 56)
Params #:  172928
MACs:  33893888
 
[04:21:07] (INFO) Building library...
[04:21:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8296708540482955, 'median': 1.7958984375, 'mins': 1.706787109375}
 
(14, 5, 1, 112, 896, 112)
Params #:  223104
MACs:  43728384
 
[04:21:24] (INFO) Building library...
[04:21:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.49276123046875, 'median': 2.4613037109375, 'mins': 2.365234375}
 
(14, 5, 1, 112, 912, 56)
Params #:  176016
MACs:  34499136
 
[04:21:44] (INFO) Building library...
[04:21:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7928455699573864, 'median': 1.7647705078125, 'mins': 1.690673828125}
 
(14, 5, 1, 112, 912, 112)
Params #:  227088
MACs:  44509248
 
[04:22:01] (INFO) Building library...
[04:22:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.517584783380682, 'median': 2.4847412109375, 'mins': 2.380615234375}
 
(14, 5, 1, 112, 928, 56)
Params #:  179104
MACs:  35104384
 
[04:22:21] (INFO) Building library...
[04:22:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7613758433948863, 'median': 1.751708984375, 'mins': 1.6947021484375}
 
(14, 5, 1, 112, 928, 112)
Params #:  231072
MACs:  45290112
 
[04:22:38] (INFO) Building library...
[04:22:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.6057184392755683, 'median': 2.56201171875, 'mins': 2.455078125}
 
(14, 5, 1, 112, 944, 56)
Params #:  182192
MACs:  35709632
 
[04:22:58] (INFO) Building library...
[04:22:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7632335316051135, 'median': 1.733642578125, 'mins': 1.65576171875}
 
(14, 5, 1, 112, 944, 112)
Params #:  235056
MACs:  46070976
 
[04:23:16] (INFO) Building library...
[04:23:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.585630104758523, 'median': 2.54150390625, 'mins': 2.440185546875}
 
(14, 5, 1, 112, 960, 56)
Params #:  185280
MACs:  36314880
 
[04:23:35] (INFO) Building library...
[04:23:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9766379616477272, 'median': 1.9803466796875, 'mins': 1.720703125}
 
(14, 5, 1, 112, 960, 112)
Params #:  239040
MACs:  46851840
 
[04:23:53] (INFO) Building library...
[04:23:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5500743519176137, 'median': 2.53662109375, 'mins': 2.456787109375}
 
(14, 5, 1, 112, 976, 56)
Params #:  188368
MACs:  36920128
 
[04:24:12] (INFO) Building library...
[04:24:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7932051225142045, 'median': 1.758544921875, 'mins': 1.66552734375}
 
(14, 5, 1, 112, 976, 112)
Params #:  243024
MACs:  47632704
 
[04:24:30] (INFO) Building library...
[04:24:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.511947354403409, 'median': 2.4969482421875, 'mins': 2.4002685546875}
 
(14, 5, 1, 112, 992, 56)
Params #:  191456
MACs:  37525376
 
[04:24:49] (INFO) Building library...
[04:24:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7670709783380683, 'median': 1.7379150390625, 'mins': 1.6485595703125}
 
(14, 5, 1, 112, 992, 112)
Params #:  247008
MACs:  48413568
 
[04:25:07] (INFO) Building library...
[04:25:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.558168723366477, 'median': 2.5123291015625, 'mins': 2.43359375}
 
(14, 5, 2, 112, 16, 92)
Params #:  3664
MACs:  442960
 
[04:25:26] (INFO) Building library...
[04:25:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5249800248579546, 'median': 1.502685546875, 'mins': 1.4151611328125}
 
(14, 5, 2, 112, 16, 184)
Params #:  5136
MACs:  515088
 
[04:25:43] (INFO) Building library...
[04:25:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6842174183238636, 'median': 1.529296875, 'mins': 1.43798828125}
 
(14, 5, 2, 112, 32, 92)
Params #:  7328
MACs:  885920
 
[04:26:01] (INFO) Building library...
[04:26:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4916015625, 'median': 1.4656982421875, 'mins': 1.3878173828125}
 
(14, 5, 2, 112, 32, 184)
Params #:  10272
MACs:  1030176
 
[04:26:19] (INFO) Building library...
[04:26:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5453324751420454, 'median': 1.529052734375, 'mins': 1.4501953125}
 
(14, 5, 2, 112, 48, 92)
Params #:  10992
MACs:  1328880
 
[04:26:36] (INFO) Building library...
[04:26:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6971846147017045, 'median': 1.6285400390625, 'mins': 1.5506591796875}
 
(14, 5, 2, 112, 48, 184)
Params #:  15408
MACs:  1545264
 
[04:26:53] (INFO) Building library...
[04:26:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7040305397727273, 'median': 1.5853271484375, 'mins': 1.4659423828125}
 
(14, 5, 2, 112, 64, 92)
Params #:  14656
MACs:  1771840
 
[04:27:10] (INFO) Building library...
[04:27:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6825128728693182, 'median': 1.655029296875, 'mins': 1.5728759765625}
 
(14, 5, 2, 112, 64, 184)
Params #:  20544
MACs:  2060352
 
[04:27:28] (INFO) Building library...
[04:27:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5355035955255683, 'median': 1.525634765625, 'mins': 1.465087890625}
 
(14, 5, 2, 112, 80, 92)
Params #:  18320
MACs:  2214800
 
[04:27:45] (INFO) Building library...
[04:27:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5121293501420454, 'median': 1.5029296875, 'mins': 1.44091796875}
 
(14, 5, 2, 112, 80, 184)
Params #:  25680
MACs:  2575440
 
[04:28:02] (INFO) Building library...
[04:28:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5602982954545455, 'median': 1.5316162109375, 'mins': 1.452392578125}
 
(14, 5, 2, 112, 96, 92)
Params #:  21984
MACs:  2657760
 
[04:28:20] (INFO) Building library...
[04:28:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5476962002840908, 'median': 1.5286865234375, 'mins': 1.444091796875}
 
(14, 5, 2, 112, 96, 184)
Params #:  30816
MACs:  3090528
 
[04:28:37] (INFO) Building library...
[04:28:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5160322709517045, 'median': 1.5074462890625, 'mins': 1.43212890625}
 
(14, 5, 2, 112, 128, 92)
Params #:  29312
MACs:  3543680
 
[04:28:54] (INFO) Building library...
[04:28:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7095692027698863, 'median': 1.6871337890625, 'mins': 1.6063232421875}
 
(14, 5, 2, 112, 128, 184)
Params #:  41088
MACs:  4120704
 
[04:29:12] (INFO) Building library...
[04:29:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5195523348721591, 'median': 1.5096435546875, 'mins': 1.437255859375}
 
(14, 5, 2, 112, 144, 92)
Params #:  32976
MACs:  3986640
 
[04:29:29] (INFO) Building library...
[04:29:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7389470880681819, 'median': 1.7137451171875, 'mins': 1.6259765625}
 
(14, 5, 2, 112, 144, 184)
Params #:  46224
MACs:  4635792
 
[04:29:47] (INFO) Building library...
[04:29:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.576025390625, 'median': 1.541748046875, 'mins': 1.447509765625}
 
(14, 5, 2, 112, 160, 92)
Params #:  36640
MACs:  4429600
 
[04:30:04] (INFO) Building library...
[04:30:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7033447265625, 'median': 1.678955078125, 'mins': 1.6024169921875}
 
(14, 5, 2, 112, 160, 184)
Params #:  51360
MACs:  5150880
 
[04:30:21] (INFO) Building library...
[04:30:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7047529740767045, 'median': 1.7000732421875, 'mins': 1.63134765625}
 
(14, 5, 2, 112, 176, 92)
Params #:  40304
MACs:  4872560
 
[04:30:38] (INFO) Building library...
[04:30:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7229536576704545, 'median': 1.6971435546875, 'mins': 1.604736328125}
 
(14, 5, 2, 112, 176, 184)
Params #:  56496
MACs:  5665968
 
[04:30:56] (INFO) Building library...
[04:30:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7554598721590908, 'median': 1.697998046875, 'mins': 1.615478515625}
 
(14, 5, 2, 112, 192, 92)
Params #:  43968
MACs:  5315520
 
[04:31:13] (INFO) Building library...
[04:31:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4616876775568182, 'median': 1.4508056640625, 'mins': 1.380615234375}
 
(14, 5, 2, 112, 192, 184)
Params #:  61632
MACs:  6181056
 
[04:31:31] (INFO) Building library...
[04:31:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.53427734375, 'median': 1.4991455078125, 'mins': 1.4188232421875}
 
(14, 5, 2, 112, 208, 92)
Params #:  47632
MACs:  5758480
 
[04:31:48] (INFO) Building library...
[04:31:48] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.639095791903409, 'median': 1.6290283203125, 'mins': 1.573486328125}
 
(14, 5, 2, 112, 208, 184)
Params #:  66768
MACs:  6696144
 
[04:32:06] (INFO) Building library...
[04:32:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.722720614346591, 'median': 1.694091796875, 'mins': 1.5975341796875}
 
(14, 5, 2, 112, 224, 92)
Params #:  51296
MACs:  6201440
 
[04:32:23] (INFO) Building library...
[04:32:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7040993430397726, 'median': 1.6796875, 'mins': 1.609130859375}
 
(14, 5, 2, 112, 224, 184)
Params #:  71904
MACs:  7211232
 
[04:32:41] (INFO) Building library...
[04:32:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7127330433238637, 'median': 1.676025390625, 'mins': 1.60498046875}
 
(14, 5, 2, 112, 240, 92)
Params #:  54960
MACs:  6644400
 
[04:32:58] (INFO) Building library...
[04:32:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6819557883522727, 'median': 1.669921875, 'mins': 1.59765625}
 
(14, 5, 2, 112, 240, 184)
Params #:  77040
MACs:  7726320
 
[04:33:16] (INFO) Building library...
[04:33:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.54061279296875, 'median': 1.5189208984375, 'mins': 1.4476318359375}
 
(14, 5, 2, 112, 256, 92)
Params #:  58624
MACs:  7087360
 
[04:33:33] (INFO) Building library...
[04:33:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4636285955255681, 'median': 1.45703125, 'mins': 1.39599609375}
 
(14, 5, 2, 112, 256, 184)
Params #:  82176
MACs:  8241408
 
[04:33:50] (INFO) Building library...
[04:33:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5541093306107954, 'median': 1.5244140625, 'mins': 1.4534912109375}
 
(14, 5, 2, 112, 272, 92)
Params #:  62288
MACs:  7530320
 
[04:34:08] (INFO) Building library...
[04:34:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7398870294744317, 'median': 1.7147216796875, 'mins': 1.6424560546875}
 
(14, 5, 2, 112, 272, 184)
Params #:  87312
MACs:  8756496
 
[04:34:25] (INFO) Building library...
[04:34:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7224964488636363, 'median': 1.69189453125, 'mins': 1.6015625}
 
(14, 5, 2, 112, 288, 92)
Params #:  65952
MACs:  7973280
 
[04:34:43] (INFO) Building library...
[04:34:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6690995649857954, 'median': 1.6568603515625, 'mins': 1.5955810546875}
 
(14, 5, 2, 112, 288, 184)
Params #:  92448
MACs:  9271584
 
[04:35:00] (INFO) Building library...
[04:35:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.497012606534091, 'median': 1.481201171875, 'mins': 1.3988037109375}
 
(14, 5, 2, 112, 304, 92)
Params #:  69616
MACs:  8416240
 
[04:35:18] (INFO) Building library...
[04:35:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.848626154119318, 'median': 1.700439453125, 'mins': 1.591552734375}
 
(14, 5, 2, 112, 304, 184)
Params #:  97584
MACs:  9786672
 
[04:35:35] (INFO) Building library...
[04:35:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7164162375710228, 'median': 1.69384765625, 'mins': 1.59765625}
 
(14, 5, 2, 112, 320, 92)
Params #:  73280
MACs:  8859200
 
[04:35:53] (INFO) Building library...
[04:35:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6954556551846591, 'median': 1.6705322265625, 'mins': 1.5894775390625}
 
(14, 5, 2, 112, 320, 184)
Params #:  102720
MACs:  10301760
 
[04:36:10] (INFO) Building library...
[04:36:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6975330699573863, 'median': 1.6705322265625, 'mins': 1.583740234375}
 
(14, 5, 2, 112, 336, 92)
Params #:  76944
MACs:  9302160
 
[04:36:28] (INFO) Building library...
[04:36:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.730636319247159, 'median': 1.7017822265625, 'mins': 1.61083984375}
 
(14, 5, 2, 112, 336, 184)
Params #:  107856
MACs:  10816848
 
[04:36:45] (INFO) Building library...
[04:36:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7312056107954545, 'median': 1.7191162109375, 'mins': 1.6407470703125}
 
(14, 5, 2, 112, 352, 92)
Params #:  80608
MACs:  9745120
 
[04:37:03] (INFO) Building library...
[04:37:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4933760209517046, 'median': 1.4786376953125, 'mins': 1.3970947265625}
 
(14, 5, 2, 112, 352, 184)
Params #:  112992
MACs:  11331936
 
[04:37:20] (INFO) Building library...
[04:37:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6558693625710228, 'median': 1.649169921875, 'mins': 1.5732421875}
 
(14, 5, 2, 112, 368, 92)
Params #:  84272
MACs:  10188080
 
[04:37:38] (INFO) Building library...
[04:37:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4740578391335226, 'median': 1.468505859375, 'mins': 1.401611328125}
 
(14, 5, 2, 112, 368, 184)
Params #:  118128
MACs:  11847024
 
[04:37:55] (INFO) Building library...
[04:37:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6911998401988637, 'median': 1.665283203125, 'mins': 1.5816650390625}
 
(14, 5, 2, 112, 384, 92)
Params #:  87936
MACs:  10631040
 
[04:38:12] (INFO) Building library...
[04:38:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.685415926846591, 'median': 1.6759033203125, 'mins': 1.6124267578125}
 
(14, 5, 2, 112, 384, 184)
Params #:  123264
MACs:  12362112
 
[04:38:30] (INFO) Building library...
[04:38:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.893853204900568, 'median': 1.7353515625, 'mins': 1.627197265625}
 
(14, 5, 2, 112, 400, 92)
Params #:  91600
MACs:  11074000
 
[04:38:47] (INFO) Building library...
[04:38:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7577803178267046, 'median': 1.6983642578125, 'mins': 1.607421875}
 
(14, 5, 2, 112, 400, 184)
Params #:  128400
MACs:  12877200
 
[04:39:05] (INFO) Building library...
[04:39:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6465320933948864, 'median': 1.5010986328125, 'mins': 1.404052734375}
 
(14, 5, 2, 112, 416, 92)
Params #:  95264
MACs:  11516960
 
[04:39:23] (INFO) Building library...
[04:39:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7054820667613637, 'median': 1.5455322265625, 'mins': 1.4508056640625}
 
(14, 5, 2, 112, 416, 184)
Params #:  133536
MACs:  13392288
 
[04:39:40] (INFO) Building library...
[04:39:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.890478515625, 'median': 1.780029296875, 'mins': 1.6549072265625}
 
(14, 5, 2, 112, 432, 92)
Params #:  98928
MACs:  11959920
 
[04:39:58] (INFO) Building library...
[04:39:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4917158647017046, 'median': 1.482666015625, 'mins': 1.4227294921875}
 
(14, 5, 2, 112, 432, 184)
Params #:  138672
MACs:  13907376
 
[04:40:16] (INFO) Building library...
[04:40:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5029829545454545, 'median': 1.4849853515625, 'mins': 1.4012451171875}
 
(14, 5, 2, 112, 448, 92)
Params #:  102592
MACs:  12402880
 
[04:40:33] (INFO) Building library...
[04:40:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7951127485795455, 'median': 1.7835693359375, 'mins': 1.7076416015625}
 
(14, 5, 2, 112, 448, 184)
Params #:  143808
MACs:  14422464
 
[04:40:51] (INFO) Building library...
[04:40:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7662409002130681, 'median': 1.73681640625, 'mins': 1.665771484375}
 
(14, 5, 2, 112, 464, 92)
Params #:  106256
MACs:  12845840
 
[04:41:08] (INFO) Building library...
[04:41:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5352195046164774, 'median': 1.5087890625, 'mins': 1.4248046875}
 
(14, 5, 2, 112, 464, 184)
Params #:  148944
MACs:  14937552
 
[04:41:25] (INFO) Building library...
[04:41:25] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8419422496448863, 'median': 1.735595703125, 'mins': 1.6239013671875}
 
(14, 5, 2, 112, 480, 92)
Params #:  109920
MACs:  13288800
 
[04:41:43] (INFO) Building library...
[04:41:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5357288707386363, 'median': 1.51123046875, 'mins': 1.4405517578125}
 
(14, 5, 2, 112, 480, 184)
Params #:  154080
MACs:  15452640
 
[04:42:00] (INFO) Building library...
[04:42:00] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.765811434659091, 'median': 1.7530517578125, 'mins': 1.682373046875}
 
(14, 5, 2, 112, 496, 92)
Params #:  113584
MACs:  13731760
 
[04:42:18] (INFO) Building library...
[04:42:18] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.753857421875, 'median': 1.7244873046875, 'mins': 1.632080078125}
 
(14, 5, 2, 112, 496, 184)
Params #:  159216
MACs:  15967728
 
[04:42:35] (INFO) Building library...
[04:42:35] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7166115500710226, 'median': 1.6529541015625, 'mins': 1.562744140625}
 
(14, 5, 2, 112, 512, 92)
Params #:  117248
MACs:  14174720
 
[04:42:53] (INFO) Building library...
[04:42:53] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7472312233664773, 'median': 1.72265625, 'mins': 1.63623046875}
 
(14, 5, 2, 112, 512, 184)
Params #:  164352
MACs:  16482816
 
[04:43:10] (INFO) Building library...
[04:43:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6264459783380683, 'median': 1.6142578125, 'mins': 1.55126953125}
 
(14, 5, 2, 112, 528, 92)
Params #:  120912
MACs:  14617680
 
[04:43:28] (INFO) Building library...
[04:43:28] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8889814897017045, 'median': 1.717529296875, 'mins': 1.6116943359375}
 
(14, 5, 2, 112, 528, 184)
Params #:  169488
MACs:  16997904
 
[04:43:46] (INFO) Building library...
[04:43:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9526422674005681, 'median': 1.9061279296875, 'mins': 1.829833984375}
 
(14, 5, 2, 112, 544, 92)
Params #:  124576
MACs:  15060640
 
[04:44:03] (INFO) Building library...
[04:44:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6957441850142045, 'median': 1.55810546875, 'mins': 1.4527587890625}
 
(14, 5, 2, 112, 544, 184)
Params #:  174624
MACs:  17512992
 
[04:44:21] (INFO) Building library...
[04:44:21] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7979969371448863, 'median': 1.77001953125, 'mins': 1.6895751953125}
 
(14, 5, 2, 112, 560, 92)
Params #:  128240
MACs:  15503600
 
[04:44:38] (INFO) Building library...
[04:44:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8888216885653408, 'median': 1.86962890625, 'mins': 1.7904052734375}
 
(14, 5, 2, 112, 560, 184)
Params #:  179760
MACs:  18028080
 
[04:44:56] (INFO) Building library...
[04:44:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8531161221590908, 'median': 1.819091796875, 'mins': 1.73291015625}
 
(14, 5, 2, 112, 576, 92)
Params #:  131904
MACs:  15946560
 
[04:45:13] (INFO) Building library...
[04:45:13] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8310757723721591, 'median': 1.77880859375, 'mins': 1.662841796875}
 
(14, 5, 2, 112, 576, 184)
Params #:  184896
MACs:  18543168
 
[04:45:31] (INFO) Building library...
[04:45:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8120594371448864, 'median': 1.7813720703125, 'mins': 1.70068359375}
 
(14, 5, 2, 112, 592, 92)
Params #:  135568
MACs:  16389520
 
[04:45:49] (INFO) Building library...
[04:45:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7326371626420454, 'median': 1.6959228515625, 'mins': 1.614013671875}
 
(14, 5, 2, 112, 592, 184)
Params #:  190032
MACs:  19058256
 
[04:46:06] (INFO) Building library...
[04:46:06] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5798506303267046, 'median': 1.5733642578125, 'mins': 1.51220703125}
 
(14, 5, 2, 112, 608, 92)
Params #:  139232
MACs:  16832480
 
[04:46:23] (INFO) Building library...
[04:46:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5561268199573863, 'median': 1.52490234375, 'mins': 1.4483642578125}
 
(14, 5, 2, 112, 608, 184)
Params #:  195168
MACs:  19573344
 
[04:46:41] (INFO) Building library...
[04:46:41] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.73905029296875, 'median': 1.73583984375, 'mins': 1.5633544921875}
 
(14, 5, 2, 112, 624, 92)
Params #:  142896
MACs:  17275440
 
[04:46:58] (INFO) Building library...
[04:46:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8711681019176136, 'median': 1.731201171875, 'mins': 1.629638671875}
 
(14, 5, 2, 112, 624, 184)
Params #:  200304
MACs:  20088432
 
[04:47:16] (INFO) Building library...
[04:47:16] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8225108753551136, 'median': 1.7952880859375, 'mins': 1.7156982421875}
 
(14, 5, 2, 112, 640, 92)
Params #:  146560
MACs:  17718400
 
[04:47:33] (INFO) Building library...
[04:47:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8291736949573865, 'median': 1.822998046875, 'mins': 1.7537841796875}
 
(14, 5, 2, 112, 640, 184)
Params #:  205440
MACs:  20603520
 
[04:47:51] (INFO) Building library...
[04:47:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6147017045454546, 'median': 1.5736083984375, 'mins': 1.498046875}
 
(14, 5, 2, 112, 656, 92)
Params #:  150224
MACs:  18161360
 
[04:48:09] (INFO) Building library...
[04:48:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.726504794034091, 'median': 1.7103271484375, 'mins': 1.63671875}
 
(14, 5, 2, 112, 656, 184)
Params #:  210576
MACs:  21118608
 
[04:48:26] (INFO) Building library...
[04:48:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.779159268465909, 'median': 1.7545166015625, 'mins': 1.6641845703125}
 
(14, 5, 2, 112, 672, 92)
Params #:  153888
MACs:  18604320
 
[04:48:43] (INFO) Building library...
[04:48:43] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8538030450994318, 'median': 1.84326171875, 'mins': 1.7642822265625}
 
(14, 5, 2, 112, 672, 184)
Params #:  215712
MACs:  21633696
 
[04:49:01] (INFO) Building library...
[04:49:01] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0134987571022727, 'median': 1.859619140625, 'mins': 1.7548828125}
 
(14, 5, 2, 112, 688, 92)
Params #:  157552
MACs:  19047280
 
[04:49:19] (INFO) Building library...
[04:49:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.753653231534091, 'median': 1.7261962890625, 'mins': 1.642333984375}
 
(14, 5, 2, 112, 688, 184)
Params #:  220848
MACs:  22148784
 
[04:49:36] (INFO) Building library...
[04:49:36] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5494972922585226, 'median': 1.52783203125, 'mins': 1.4564208984375}
 
(14, 5, 2, 112, 704, 92)
Params #:  161216
MACs:  19490240
 
[04:49:54] (INFO) Building library...
[04:49:54] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7383378462357955, 'median': 1.709716796875, 'mins': 1.6173095703125}
 
(14, 5, 2, 112, 704, 184)
Params #:  225984
MACs:  22663872
 
[04:50:11] (INFO) Building library...
[04:50:11] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.624199884588068, 'median': 1.59423828125, 'mins': 1.527587890625}
 
(14, 5, 2, 112, 720, 92)
Params #:  164880
MACs:  19933200
 
[04:50:29] (INFO) Building library...
[04:50:29] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7570578835227273, 'median': 1.7288818359375, 'mins': 1.639404296875}
 
(14, 5, 2, 112, 720, 184)
Params #:  231120
MACs:  23178960
 
[04:50:46] (INFO) Building library...
[04:50:46] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7712812943892045, 'median': 1.7686767578125, 'mins': 1.6395263671875}
 
(14, 5, 2, 112, 736, 92)
Params #:  168544
MACs:  20376160
 
[04:51:04] (INFO) Building library...
[04:51:04] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8438398881392046, 'median': 1.6851806640625, 'mins': 1.5716552734375}
 
(14, 5, 2, 112, 736, 184)
Params #:  236256
MACs:  23694048
 
[04:51:22] (INFO) Building library...
[04:51:22] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.825912198153409, 'median': 1.781494140625, 'mins': 1.6781005859375}
 
(14, 5, 2, 112, 752, 92)
Params #:  172208
MACs:  20819120
 
[04:51:40] (INFO) Building library...
[04:51:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.72052001953125, 'median': 1.684326171875, 'mins': 1.613037109375}
 
(14, 5, 2, 112, 752, 184)
Params #:  241392
MACs:  24209136
 
[04:51:57] (INFO) Building library...
[04:51:57] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.667434969815341, 'median': 1.6480712890625, 'mins': 1.5694580078125}
 
(14, 5, 2, 112, 768, 92)
Params #:  175872
MACs:  21262080
 
[04:52:14] (INFO) Building library...
[04:52:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7526655717329545, 'median': 1.7259521484375, 'mins': 1.6405029296875}
 
(14, 5, 2, 112, 768, 184)
Params #:  246528
MACs:  24724224
 
[04:52:32] (INFO) Building library...
[04:52:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7971235795454545, 'median': 1.7587890625, 'mins': 1.667724609375}
 
(14, 5, 2, 112, 784, 92)
Params #:  179536
MACs:  21705040
 
[04:52:49] (INFO) Building library...
[04:52:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.744449129971591, 'median': 1.7069091796875, 'mins': 1.6119384765625}
 
(14, 5, 2, 112, 784, 184)
Params #:  251664
MACs:  25239312
 
[04:53:07] (INFO) Building library...
[04:53:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.708447265625, 'median': 1.6966552734375, 'mins': 1.630615234375}
 
(14, 5, 2, 112, 800, 92)
Params #:  183200
MACs:  22148000
 
[04:53:24] (INFO) Building library...
[04:53:24] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7433149857954546, 'median': 1.7176513671875, 'mins': 1.63525390625}
 
(14, 5, 2, 112, 800, 184)
Params #:  256800
MACs:  25754400
 
[04:53:42] (INFO) Building library...
[04:53:42] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.716800204190341, 'median': 1.69921875, 'mins': 1.632568359375}
 
(14, 5, 2, 112, 816, 92)
Params #:  186864
MACs:  22590960
 
[04:53:59] (INFO) Building library...
[04:53:59] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7210560191761364, 'median': 1.6954345703125, 'mins': 1.6094970703125}
 
(14, 5, 2, 112, 816, 184)
Params #:  261936
MACs:  26269488
 
[04:54:17] (INFO) Building library...
[04:54:17] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.0535877574573864, 'median': 2.029052734375, 'mins': 1.9058837890625}
 
(14, 5, 2, 112, 832, 92)
Params #:  190528
MACs:  23033920
 
[04:54:34] (INFO) Building library...
[04:54:34] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7983198686079545, 'median': 1.7625732421875, 'mins': 1.662353515625}
 
(14, 5, 2, 112, 832, 184)
Params #:  267072
MACs:  26784576
 
[04:54:52] (INFO) Building library...
[04:54:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7824873490767046, 'median': 1.7520751953125, 'mins': 1.65625}
 
(14, 5, 2, 112, 848, 92)
Params #:  194192
MACs:  23476880
 
[04:55:10] (INFO) Building library...
[04:55:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7442027698863636, 'median': 1.71826171875, 'mins': 1.6448974609375}
 
(14, 5, 2, 112, 848, 184)
Params #:  272208
MACs:  27299664
 
[04:55:27] (INFO) Building library...
[04:55:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.838818359375, 'median': 1.8076171875, 'mins': 1.7257080078125}
 
(14, 5, 2, 112, 864, 92)
Params #:  197856
MACs:  23919840
 
[04:55:45] (INFO) Building library...
[04:55:45] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8473821466619318, 'median': 1.83544921875, 'mins': 1.765380859375}
 
(14, 5, 2, 112, 864, 184)
Params #:  277344
MACs:  27814752
 
[04:56:02] (INFO) Building library...
[04:56:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6964133522727274, 'median': 1.6650390625, 'mins': 1.5865478515625}
 
(14, 5, 2, 112, 880, 92)
Params #:  201520
MACs:  24362800
 
[04:56:20] (INFO) Building library...
[04:56:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6989035866477273, 'median': 1.6903076171875, 'mins': 1.6195068359375}
 
(14, 5, 2, 112, 880, 184)
Params #:  282480
MACs:  28329840
 
[04:56:37] (INFO) Building library...
[04:56:37] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7369595614346591, 'median': 1.71044921875, 'mins': 1.6143798828125}
 
(14, 5, 2, 112, 896, 92)
Params #:  205184
MACs:  24805760
 
[04:56:55] (INFO) Building library...
[04:56:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.725, 'median': 1.7138671875, 'mins': 1.6436767578125}
 
(14, 5, 2, 112, 896, 184)
Params #:  287616
MACs:  28844928
 
[04:57:12] (INFO) Building library...
[04:57:12] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7876575816761364, 'median': 1.7464599609375, 'mins': 1.66357421875}
 
(14, 5, 2, 112, 912, 92)
Params #:  208848
MACs:  25248720
 
[04:57:30] (INFO) Building library...
[04:57:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7251786665482955, 'median': 1.7095947265625, 'mins': 1.6351318359375}
 
(14, 5, 2, 112, 912, 184)
Params #:  292752
MACs:  29360016
 
[04:57:47] (INFO) Building library...
[04:57:47] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7578857421875, 'median': 1.7315673828125, 'mins': 1.647216796875}
 
(14, 5, 2, 112, 928, 92)
Params #:  212512
MACs:  25691680
 
[04:58:05] (INFO) Building library...
[04:58:05] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.6951848810369319, 'median': 1.690185546875, 'mins': 1.6212158203125}
 
(14, 5, 2, 112, 928, 184)
Params #:  297888
MACs:  29875104
 
[04:58:23] (INFO) Building library...
[04:58:23] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7696799538352272, 'median': 1.7218017578125, 'mins': 1.6632080078125}
 
(14, 5, 2, 112, 944, 92)
Params #:  216176
MACs:  26134640
 
[04:58:40] (INFO) Building library...
[04:58:40] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.9340676047585228, 'median': 1.9095458984375, 'mins': 1.817626953125}
 
(14, 5, 2, 112, 944, 184)
Params #:  303024
MACs:  30390192
 
[04:58:58] (INFO) Building library...
[04:58:58] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8519575639204546, 'median': 1.8411865234375, 'mins': 1.767578125}
 
(14, 5, 2, 112, 960, 92)
Params #:  219840
MACs:  26577600
 
[04:59:15] (INFO) Building library...
[04:59:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.746634188565341, 'median': 1.7171630859375, 'mins': 1.6395263671875}
 
(14, 5, 2, 112, 960, 184)
Params #:  308160
MACs:  30905280
 
[04:59:33] (INFO) Building library...
[04:59:33] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.7818348277698863, 'median': 1.7381591796875, 'mins': 1.63232421875}
 
(14, 5, 2, 112, 976, 92)
Params #:  223504
MACs:  27020560
 
[04:59:51] (INFO) Building library...
[04:59:51] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2335526899857956, 'median': 2.1146240234375, 'mins': 1.9913330078125}
 
(14, 5, 2, 112, 976, 184)
Params #:  313296
MACs:  31420368
 
[05:00:09] (INFO) Building library...
[05:00:09] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.02005615234375, 'median': 1.866943359375, 'mins': 1.7691650390625}
 
(14, 5, 2, 112, 992, 92)
Params #:  227168
MACs:  27463520
 
[05:00:26] (INFO) Building library...
[05:00:26] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.780648526278409, 'median': 1.7564697265625, 'mins': 1.6612548828125}
 
(14, 5, 2, 112, 992, 184)
Params #:  318432
MACs:  31935456
 
[05:00:44] (INFO) Building library...
[05:00:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.8226595791903408, 'median': 1.83154296875, 'mins': 1.621337890625}
 
(7, 5, 1, 184, 16, 92)
Params #:  4816
MACs:  235984
 
[05:01:02] (INFO) Building library...
[05:01:02] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4694313742897727, 'median': 1.444580078125, 'mins': 1.3663330078125}
 
(7, 5, 1, 184, 16, 184)
Params #:  6288
MACs:  308112
 
[05:01:19] (INFO) Building library...
[05:01:19] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.167014382102273, 'median': 2.15966796875, 'mins': 2.09130859375}
 
(7, 5, 1, 184, 32, 92)
Params #:  9632
MACs:  471968
 
[05:01:38] (INFO) Building library...
[05:01:38] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.506087979403409, 'median': 1.4906005859375, 'mins': 1.4200439453125}
 
(7, 5, 1, 184, 32, 184)
Params #:  12576
MACs:  616224
 
[05:01:55] (INFO) Building library...
[05:01:55] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2280317826704548, 'median': 2.2039794921875, 'mins': 2.11865234375}
 
(7, 5, 1, 184, 48, 92)
Params #:  14448
MACs:  707952
 
[05:02:14] (INFO) Building library...
[05:02:14] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.4978105024857955, 'median': 1.472412109375, 'mins': 1.417236328125}
 
(7, 5, 1, 184, 48, 184)
Params #:  18864
MACs:  924336
 
[05:02:31] (INFO) Building library...
[05:02:31] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.4244151722301135, 'median': 2.392822265625, 'mins': 2.138916015625}
 
(7, 5, 1, 184, 64, 92)
Params #:  19264
MACs:  943936
 
[05:02:50] (INFO) Building library...
[05:02:50] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.612246981534091, 'median': 1.5994873046875, 'mins': 1.5430908203125}
 
(7, 5, 1, 184, 64, 184)
Params #:  25152
MACs:  1232448
 
[05:03:07] (INFO) Building library...
[05:03:07] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.337151544744318, 'median': 2.3187255859375, 'mins': 2.2362060546875}
 
(7, 5, 1, 184, 80, 92)
Params #:  24080
MACs:  1179920
 
[05:03:27] (INFO) Building library...
[05:03:27] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5294078480113635, 'median': 1.506103515625, 'mins': 1.435302734375}
 
(7, 5, 1, 184, 80, 184)
Params #:  31440
MACs:  1540560
 
[05:03:44] (INFO) Building library...
[05:03:44] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.2074007901278407, 'median': 2.18115234375, 'mins': 2.0966796875}
 
(7, 5, 1, 184, 96, 92)
Params #:  28896
MACs:  1415904
 
[05:04:03] (INFO) Building library...
[05:04:03] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5360573508522728, 'median': 1.50634765625, 'mins': 1.4356689453125}
 
(7, 5, 1, 184, 96, 184)
Params #:  37728
MACs:  1848672
 
[05:04:20] (INFO) Building library...
[05:04:20] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1941639293323862, 'median': 2.1669921875, 'mins': 2.09033203125}
 
(7, 5, 1, 184, 112, 92)
Params #:  33712
MACs:  1651888
 
[05:04:39] (INFO) Building library...
[05:04:39] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.5720259232954545, 'median': 1.541015625, 'mins': 1.4420166015625}
 
(7, 5, 1, 184, 112, 184)
Params #:  44016
MACs:  2156784
 
[05:04:56] (INFO) Building library...
[05:04:56] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.194557883522727, 'median': 2.1661376953125, 'mins': 2.0904541015625}
 
(7, 5, 1, 184, 128, 92)
Params #:  38528
MACs:  1887872
 
[05:05:15] (INFO) Building library...
[05:05:15] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.488162508877841, 'median': 1.4666748046875, 'mins': 1.4063720703125}
 
(7, 5, 1, 184, 128, 184)
Params #:  50304
MACs:  2464896
 
[05:05:32] (INFO) Building library...
[05:05:32] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.3834605823863635, 'median': 2.35302734375, 'mins': 2.2685546875}
 
(7, 5, 1, 184, 144, 92)
Params #:  43344
MACs:  2123856
 
[05:05:52] (INFO) Building library...
[05:05:52] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.062212579900568, 'median': 1.9971923828125, 'mins': 1.78955078125}
 
(7, 5, 1, 184, 144, 184)
Params #:  56592
MACs:  2773008
 
[05:06:10] (INFO) Building library...
[05:06:10] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.5018310546875, 'median': 2.4290771484375, 'mins': 2.22607421875}
 
(7, 5, 1, 184, 160, 92)
Params #:  48160
MACs:  2359840
 
[05:06:30] (INFO) Building library...
[05:06:30] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 1.927985174005682, 'median': 1.8480224609375, 'mins': 1.6455078125}
 
(7, 5, 1, 184, 160, 184)
Params #:  62880
MACs:  3081120
 
[05:06:49] (INFO) Building library...
[05:06:49] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
{'mean': 2.1667735706676137, 'median': 2.1553955078125, 'mins': 2.08740234375}
 
(7, 5, 1, 184, 176, 92)
Params #:  52976
MACs:  2595824
 
[05:07:08] (INFO) Building library...
[05:07:08] (DEBUG) Build target: cuda -arch=sm_72 -model=tx2 -libs=cudnn,cublas
{'use_implicit_batch': True, 'max_workspace_size': 1073741824, 'remove_no_mac_subgraphs': False, 'use_fp16': False, 'use_uint8': False, 'dla_core': 0, 'gpu_fallback': True, 'tensorrt_version': (8, 4, 1)}
